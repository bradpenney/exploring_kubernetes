{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Exploring Kubernetes - Developer to Platform Engineer Guide","text":""},{"location":"#exploring-kubernetes","title":"Exploring Kubernetes","text":"<p>From first deployment to production operations.</p> <p>A subsection of BradPenney.io, this site teaches practical Kubernetes skills through a progressive learning journey\u2014from application developer deploying your first app to platform engineer running production clusters.</p> <p>It emphasizes real-world scenarios, production safety, and the \"why\" behind every resource and command.</p>"},{"location":"#learning-path","title":"Learning Path","text":""},{"location":"#day-one-getting-started","title":"Day One: Getting Started","text":"<p>Everyone starts here: Learn what Kubernetes is and why it exists, then choose your path based on how you'll deploy applications.</p> <p>Shared foundation (read these first):</p> <ul> <li>Day One Overview - Your complete Day One roadmap</li> <li>What Is Kubernetes? - The problem Kubernetes solves and why companies adopt it</li> </ul> <p>Then choose your deployment path:</p> <ul> <li> <p> Path 1: From Scratch (kubectl)</p> <p>For: Learning Kubernetes fundamentals by writing YAML and using <code>kubectl</code> directly</p> <p>Your situation: You're learning Kubernetes from the ground up, or your team deploys with raw YAML manifests</p> <p>What you'll do: Write Deployment and Service YAML, apply with <code>kubectl</code>, understand Pods, ReplicaSets, and Services from first principles</p> <p>Articles:</p> <ul> <li>Getting <code>kubectl</code> Access</li> <li>Your First Deployment</li> <li>Essential <code>kubectl</code> Commands</li> <li>Understanding What Happened</li> </ul> </li> <li> <p> Path 2: Using Helm</p> <p>For: Working with Helm charts\u2014either from CI/CD pipelines or vendor distributions</p> <p>Your situation: Your CI/CD pipeline (Jenkins, GitLab CI, GitHub Actions, etc.) generates Helm charts, or you need to deploy vendor software like Prometheus/Grafana from Bitnami charts</p> <p>What you'll do: Deploy and customize Helm charts with <code>values.yaml</code>, use <code>helm</code> commands, understand what Helm creates under the hood</p> <p>Articles:</p> <ul> <li>Getting Helm Access</li> <li>Your First Helm Deployment</li> <li>Essential Helm Commands (coming soon)</li> <li>Understanding What Helm Created (coming soon)</li> </ul> </li> </ul> <p>Both Paths Converge at Level 1</p> <p>Whether you start From Scratch or Using Helm, both paths teach you to deploy applications and understand Kubernetes. They converge at Level 1 where everyone needs the same deep knowledge of Pods, Services, and Deployments.</p>"},{"location":"#continuing-your-journey","title":"Continuing Your Journey","text":"<ul> <li> <p> Level 1 - Core Primitives (Coming Soon)</p> <p>For: App developers getting comfortable</p> <p>Goal: Master the fundamental building blocks</p> <ul> <li>Pods Deep Dive</li> <li>Services: Connecting to Pods</li> <li>ConfigMaps and Secrets</li> <li>Namespaces</li> <li>Labels and Selectors</li> </ul> </li> <li> <p> Level 2 - Workload Management (Coming Soon)</p> <p>For: App developers deploying real applications</p> <p>Goal: Manage applications at scale</p> <ul> <li>Deployments Explained</li> <li>ReplicaSets Under the Hood</li> <li>StatefulSets</li> <li>DaemonSets</li> <li>Jobs and CronJobs</li> </ul> </li> <li> <p> Level 3 - Networking (Coming Soon)</p> <p>For: App developers + Platform engineers</p> <p>Goal: Connect services, expose applications</p> <ul> <li>Services Deep Dive</li> <li>Ingress Controllers</li> <li>Network Policies</li> <li>DNS and Service Discovery</li> <li>Troubleshooting Networking</li> </ul> </li> <li> <p> Level 4 - Storage and State (Coming Soon)</p> <p>For: Both personas, stateful applications</p> <p>Goal: Handle persistent data</p> <ul> <li>Understanding Volumes</li> <li>Persistent Volumes (PV)</li> <li>Persistent Volume Claims (PVC)</li> <li>StorageClasses</li> <li>Running Databases on Kubernetes</li> </ul> </li> <li> <p> Level 5 - Advanced Scheduling &amp; Security (Coming Soon)</p> <p>For: Platform engineers</p> <p>Goal: Production-ready operations</p> <ul> <li>Resource Requests and Limits</li> <li>Taints and Tolerations</li> <li>Node Affinity and Pod Affinity</li> <li>RBAC</li> <li>Security Best Practices</li> </ul> </li> <li> <p> Level 6 - Production Operations (Coming Soon)</p> <p>For: SREs and platform engineers</p> <p>Goal: Observe, maintain, scale</p> <ul> <li>Logging Architecture</li> <li>Monitoring and Metrics</li> <li>Health Checks and Probes</li> <li>Helm Package Manager</li> <li>Operators and Custom Resources</li> </ul> </li> </ul>"},{"location":"#philosophy","title":"Philosophy","text":"<ul> <li> <p> Production Safety</p> <p>Real-world scenarios you'll encounter with actual clusters. Learn when commands are dangerous and why. Safety-first approach throughout.</p> </li> <li> <p> Purpose-Driven Learning</p> <p>Understand the \"why,\" not just memorize YAML. Every resource type, every command\u2014you'll know why it exists and when to use it.</p> </li> <li> <p> Progressive Complexity</p> <p>From Day One deployment to production operations. Start as an intimidated app developer, become a confident platform engineer.</p> </li> <li> <p> Dual Personas</p> <p>Serving both application developers (Day One - Level 2) AND platform engineers (Level 3-6). The content meets you where you are.</p> </li> </ul>"},{"location":"#your-learning-journey","title":"Your Learning Journey","text":"<pre><code>flowchart TD\n    Start[Application Developer&lt;br/&gt;kubectl access&lt;br/&gt;Intimidated by terminal]\n\n    DayOne[Day One: First Deployment&lt;br/&gt;Essential Commands]\n    L12[Level 1-2: Core Skills&lt;br/&gt;Pods, Services, Deployments]\n    L34[Level 3-4: Operations&lt;br/&gt;Networking, Storage]\n    L56[Level 5-6: Production&lt;br/&gt;Security, Monitoring]\n\n    End[Platform Engineer&lt;br/&gt;Production clusters&lt;br/&gt;Confident &amp; capable]\n\n    Start --&gt; DayOne\n    DayOne --&gt; L12\n    L12 --&gt; L34\n    L34 --&gt; L56\n    L56 --&gt; End\n\n    style Start fill:#1a202c,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style DayOne fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style L12 fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style L34 fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style L56 fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style End fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre> Starting Point: Application DeveloperDestination: Platform Engineer <p>Day One (both paths) and Level 1-2 assume you're an application developer with:</p> <ul> <li>Access to a real development cluster (not minikube!)</li> <li>An application to deploy (or vendor software to install)</li> <li>Limited command-line experience</li> <li>No Kubernetes knowledge</li> <li>A team where nobody knows Kubernetes either (you're all figuring this out)</li> <li>A manager who said \"we're using Kubernetes now\" (but can't teach you)</li> </ul> <p>After the shared foundation, choose your path:</p> <ul> <li>From Scratch (kubectl) if you're writing YAML or learning fundamentals</li> <li>Using Helm if your CI/CD pipeline generates charts or you're deploying vendor products</li> </ul> <p>The reality: Your company adopted Kubernetes. Your teammates are learning alongside you. Nobody expects you to become an expert\u2014just functional enough to deploy your code and debug when things break.</p> <p>This site gets you unblocked. Not certified, not expert-level\u2014just competent enough to ship code.</p> <p>Level 3-6 shift to platform engineering concerns:</p> <ul> <li>Networking and security (Ingress, Network Policies, DNS)</li> <li>Storage and state management (PV, PVC, StorageClasses)</li> <li>Resource management and scheduling (Requests, Limits, Affinity)</li> <li>Production operations and observability (Logging, Monitoring, Helm)</li> </ul> <p>By Level 6, you'll be running production Kubernetes clusters with confidence.</p>"},{"location":"#what-makes-this-different","title":"What Makes This Different","text":"<p>This site takes a unique approach to teaching Kubernetes:</p> <ul> <li>Two entry points \u2014 Learn from scratch with YAML, or start with Helm charts (both valid!)</li> <li>Assumes real cluster access \u2014 You already have credentials, skip the local setup</li> <li>Starts with deployment \u2014 Get working first, understand architecture later</li> <li>Acknowledges the reality \u2014 Your team is figuring this out together, you're not alone</li> <li>Builds confidence gradually \u2014 Read-only commands first, then move to changes</li> <li>Progressive personas \u2014 Start as app developer (Day One - Level 2), grow into platform engineer (Level 3-6)</li> <li>Production safety throughout \u2014 Namespace awareness, command labels, safety-first approach</li> </ul> <p>Start with the shared foundation (Overview + What Is Kubernetes?), then:</p> <ul> <li>If you're writing YAML manifests or learning fundamentals \u2192 Day One: From Scratch (kubectl)</li> <li>If you're working with Helm charts from CI/CD pipelines or vendors \u2192 Day One: Using Helm</li> </ul> <p>If you're a platform engineer preparing to run production clusters, you'll still benefit from the progressive learning path, but can move faster through early levels.</p>"},{"location":"#connect","title":"Connect","text":"<ul> <li>Main site: bradpenney.io</li> <li>Source code: GitHub</li> <li>Related: Exploring Enterprise Linux - Progressive Linux learning</li> </ul>"},{"location":"day_one/overview/","title":"Day One: Getting Started with Kubernetes","text":"<p>Your First Day with Kubernetes</p> <p>You just got access to your company's dev cluster. You have an application to deploy. You're probably thinking: What is this? How do I use it? Will I break something?</p>"},{"location":"day_one/overview/#welcome-to-kubernetes","title":"Welcome to Kubernetes","text":"<p>This is your first time working with Kubernetes, and you're probably feeling one of two ways:</p> <ul> <li>Curious: \"Everyone talks about Kubernetes. Time to see what it's about!\"</li> <li>Nervous: \"I just got cluster access and need to deploy my app. No idea where to start...\"</li> </ul> <p>Both are valid. You're in the right place.</p> <p>This guide is for application developers who need to deploy their code to a Kubernetes cluster. Not infrastructure engineers (yet)\u2014just developers who need to ship their applications.</p> <p>What You'll Learn</p> <p>By the end of Day One, you'll know how to:</p> <ul> <li>Understand Kubernetes - What it is, why your company uses it, what problem it solves</li> <li>Connect to your cluster - Authentication, contexts, and namespaces</li> <li>Deploy your first application - From container image to running pods</li> <li>Use essential commands - The tools you'll use daily for deployment and troubleshooting</li> <li>Understand what you just did - What actually happens when you deploy</li> <li>Explore safely - Read-only vs. destructive operations, troubleshooting basics</li> </ul>"},{"location":"day_one/overview/#your-first-day-the-journey","title":"Your First Day: The Journey","text":"<p>Day One offers two paths depending on how your team works with Kubernetes:</p> <ul> <li>kubectl Path (From Scratch): Write YAML manifests manually and deploy with <code>kubectl apply</code> commands</li> <li>Helm Path (Package Manager): Use packaged charts and deploy with <code>helm install</code> commands</li> </ul> <p>Both paths follow a similar pattern and reach the same destination\u2014a deployed application. The difference is in the tools you use. Choose the journey that matches how your team deploys applications:</p> kubectl Journey (From Scratch)Helm Journey (Package Manager) <pre><code>flowchart TD\n    Start[You receive kubectl&lt;br/&gt;credentials]\n    Connect[Connect to cluster&lt;br/&gt;kubectl config]\n    Verify[Verify access&lt;br/&gt;kubectl get nodes]\n    Check[Check your namespace&lt;br/&gt;kubectl config view]\n    Deploy[Deploy your app&lt;br/&gt;kubectl apply -f app.yaml]\n    Watch[Watch deployment&lt;br/&gt;kubectl get pods]\n    Logs[Check logs&lt;br/&gt;kubectl logs pod-name]\n    Success[App running!&lt;br/&gt;Confidence built]\n\n    Start --&gt; Connect\n    Connect --&gt; Verify\n    Verify --&gt; Check\n    Check --&gt; Deploy\n    Deploy --&gt; Watch\n    Watch --&gt; Logs\n    Logs --&gt; Success\n\n    style Start fill:#1a202c,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Connect fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Verify fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Check fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Deploy fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Watch fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Logs fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Success fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre> <p>The kubectl path: Write YAML manifests by hand, deploy with <code>kubectl apply</code>, understand every building block from the ground up.</p> <pre><code>flowchart TD\n    Start[You receive kubectl&lt;br/&gt;credentials]\n    InstallHelm[Install Helm CLI&lt;br/&gt;brew install helm]\n    Connect[Connect to cluster&lt;br/&gt;helm list]\n    Repo[Add chart repository&lt;br/&gt;helm repo add]\n    Deploy[Deploy your app&lt;br/&gt;helm install my-app chart/]\n    Watch[Watch deployment&lt;br/&gt;helm status my-app]\n    Logs[Check logs&lt;br/&gt;kubectl logs pod-name]\n    Success[App running!&lt;br/&gt;Confidence built]\n\n    Start --&gt; InstallHelm\n    InstallHelm --&gt; Connect\n    Connect --&gt; Repo\n    Repo --&gt; Deploy\n    Deploy --&gt; Watch\n    Watch --&gt; Logs\n    Logs --&gt; Success\n\n    style Start fill:#1a202c,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style InstallHelm fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Connect fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Repo fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Deploy fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Watch fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Logs fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Success fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre> <p>The Helm path: Use packaged charts (from CI/CD or vendors), deploy with <code>helm install</code>, customize through values files rather than raw YAML.</p>"},{"location":"day_one/overview/#understanding-your-context","title":"Understanding Your Context","text":"<p>This guide is written for a specific situation\u2014one that's incredibly common but rarely addressed directly in Kubernetes tutorials.</p> <ul> <li> <p> Your Setup</p> <p>You have:</p> <ul> <li>A containerized application (Docker image in a registry)</li> <li>Cluster access configured (IT/ops gave you credentials)</li> <li>Access to a development Kubernetes cluster (real cluster, not minikube)</li> <li>A namespace you can deploy to</li> <li>Basic understanding that containers exist</li> <li>Either <code>kubectl</code> for manual YAML or Helm charts from your CI/CD pipeline</li> </ul> <p>You don't have:</p> <ul> <li>Deep Kubernetes knowledge (that's why you're here!)</li> <li>Infrastructure or ops responsibilities (yet)</li> <li>A cluster to manage (you're using an existing one)</li> <li>Command line confidence (we'll fix that)</li> </ul> <p>You are:</p> <ul> <li>An application developer</li> <li>Possibly intimidated by the terminal</li> <li>Responsible for getting your app running in dev/staging</li> <li>Learning Kubernetes because your company adopted it</li> </ul> </li> <li> <p> The Team Reality</p> <p>Let's be honest about what's probably happening at your company:</p> <ul> <li>Your manager said \"we're using Kubernetes now\" but doesn't know how it works either</li> <li>Your teammates are figuring this out alongside you - nobody on the team is a Kubernetes expert</li> <li>The platform/ops team gave you access but can't hand-hold through every deployment</li> <li>Nobody expects you to become a Kubernetes expert - just functional enough to deploy your app</li> <li>You're learning by necessity, not choice - this wasn't on your career roadmap, but here you are</li> </ul> <p>This guide gets you unblocked. Not certified, not expert-level\u2014just competent enough to ship code and debug when things break.</p> <p>You're not alone. Thousands of application developers are in the exact same situation: handed cluster credentials and told \"figure it out.\" This guide is for all of you.</p> </li> </ul>"},{"location":"day_one/overview/#common-developer-scenarios","title":"Common Developer Scenarios","text":"<p>Why are you really here? Probably one of these situations (examples show both paths):</p>  Deploy My App to Dev Check Logs When Bug Reported Update Environment Variable Roll Back Bad Deploy Scale Up for Load Testing <p>Your situation: QA needs to test your latest build. You have a Docker image. Now what?</p> kubectl PathHelm Path <p>What you need to learn:</p> <ul> <li>How to write a deployment YAML (or modify the existing one)</li> <li><code>kubectl apply -f deployment.yaml</code> - push your changes</li> <li><code>kubectl get pods</code> - verify it's running</li> <li><code>kubectl rollout status</code> - watch the deployment happen</li> </ul> <p>What you need to learn:</p> <ul> <li>How to customize your chart's <code>values.yaml</code> file</li> <li><code>helm install my-app ./chart</code> - deploy your application</li> <li><code>kubectl get pods</code> - verify it's running (same as kubectl path!)</li> <li><code>helm status my-app</code> - check deployment status</li> </ul> <p>Day One gets you here. By the end, you'll deploy confidently.</p> <p>Your situation: QA says \"it's broken in dev\" or \"I'm seeing weird errors.\" You need to investigate.</p> <p>What you need to learn (same for both paths):</p> <ul> <li><code>kubectl get pods</code> - find your pod name</li> <li><code>kubectl logs pod-name-xyz</code> - see what's failing</li> <li><code>kubectl logs -f pod-name-xyz</code> - follow logs live</li> <li><code>kubectl describe pod pod-name-xyz</code> - see why it crashed</li> </ul> <p>Both Paths Use kubectl for Debugging</p> <p>Even Helm users rely on <code>kubectl</code> commands for troubleshooting. Helm deploys resources, but you inspect and debug them with kubectl.</p> <p>Essential Commands teaches this. Debugging becomes routine.</p> <p>Your situation: Need to point to a different database, change an API key, or update a config value.</p> kubectl PathHelm Path <p>What you need to learn:</p> <ul> <li>ConfigMaps and Secrets (where config lives)</li> <li>How to update them with <code>kubectl apply</code></li> <li>How to restart your deployment to pick up changes</li> <li>How to verify the new value is being used</li> </ul> <p>What you need to learn:</p> <ul> <li>How values flow from <code>values.yaml</code> to ConfigMaps/Secrets</li> <li>Update <code>values.yaml</code> with new configuration</li> <li>Run <code>helm upgrade my-app ./chart</code> to apply changes</li> <li>How to verify the new value is being used</li> </ul> <p>Level 1: ConfigMaps and Secrets covers this. Configuration management made clear.</p> <p>Your situation: You deployed something and now dev is broken. You need to undo it. Fast.</p> kubectl PathHelm Path <p>What you need to learn:</p> <ul> <li><code>kubectl rollout undo deployment/yourapp</code> - instant rollback</li> <li><code>kubectl rollout history</code> - see what versions exist</li> <li>How to verify you're back to the working version</li> </ul> <p>What you need to learn:</p> <ul> <li><code>helm rollback my-app</code> - instant rollback to previous release</li> <li><code>helm history my-app</code> - see all release versions</li> <li>How to verify you're back to the working version</li> </ul> <p>Day One and Level 2 cover this. Rollbacks become your safety net.</p> <p>Your situation: Performance team wants to load test. You need more instances running.</p> kubectl PathHelm Path <p>What you need to learn:</p> <ul> <li><code>kubectl scale deployment/yourapp --replicas=10</code> - simple scaling</li> <li>How to verify all replicas are running</li> <li>How to scale back down when done</li> </ul> <p>What you need to learn:</p> <ul> <li>Update <code>replicas: 10</code> in your <code>values.yaml</code></li> <li>Run <code>helm upgrade my-app ./chart</code> to apply new scale</li> <li>How to verify all replicas are running</li> <li>Update values and upgrade again to scale back down</li> </ul> <p>Level 2: Deployments covers scaling. It's easier than you think.</p> <p>Sound familiar? These are the real-world tasks you'll master. Not theoretical Kubernetes architecture\u2014practical skills for shipping code.</p>"},{"location":"day_one/overview/#which-day-one-path-are-you-on","title":"Which Day One Path Are You On?","text":"<p>Not sure which path to choose? Use these criteria to decide:</p> <ul> <li> <p> The kubectl Path</p> <p>Choose this if:</p> <ul> <li>You're learning Kubernetes from the ground up</li> <li>You'll be writing your own YAML manifests</li> <li>Your team deploys with <code>kubectl apply</code></li> <li>You want to understand every building block</li> </ul> </li> <li> <p> The Helm Path</p> <p>Choose this if:</p> <ul> <li>Your CI/CD pipeline generates Helm charts for you</li> <li>You need to deploy vendor charts (Prometheus, Grafana, Redis, etc.)</li> <li>Your team standardizes on Helm for all deployments</li> <li>You were told \"just run <code>helm install</code>\"</li> </ul> </li> </ul>"},{"location":"day_one/overview/#the-articles","title":"The Articles","text":""},{"location":"day_one/overview/#shared-foundation","title":"Shared Foundation","text":"<ul> <li> What Is Kubernetes and Why? - Understand the problem Kubernetes solves.</li> </ul> kubectl Path (From Scratch)Helm Path (Package Manager) <ul> <li> <p> Getting kubectl Access   Connect to your company's cluster and verify access.</p> </li> <li> <p> Your First Deployment   Deploy a simple application from a YAML file.</p> </li> <li> <p> Essential kubectl Commands   The 10 commands you'll use every single day.</p> </li> <li> <p> Understanding What Happened   Learn about Pods, ReplicaSets, and how they fit together.</p> </li> </ul> <p>Helm Path Expanding</p> <p>The first two articles in the <code>Helm</code> path are now published! Both paths teach the same foundational Kubernetes concepts, but the <code>Helm</code> path focuses on using a package manager rather than writing raw YAML.</p> <ul> <li> Getting Helm Access</li> </ul> <p>Install the <code>helm</code> CLI and connect to your cluster using existing <code>kubectl</code> credentials.</p> <ul> <li> Your First Helm Deployment</li> </ul> <p>Deploy your first chart from a CI/CD pipeline or vendor repository.</p> <ul> <li> Essential Helm Commands (coming soon)</li> </ul> <p>Master <code>helm install</code>, <code>helm upgrade</code>, <code>helm rollback</code>, and release management.</p> <ul> <li> Understanding What Helm Created (coming soon)</li> </ul> <p>See how <code>Helm</code> translates values into Kubernetes resources under the hood.</p> <p>Day One Paths Available</p> <p>kubectl Path: Complete! All four articles published.</p> <p>Helm Path: Getting Started and First Deployment articles are now available. Additional articles coming soon.</p> <p>Start with What Is Kubernetes? then choose your path!</p>"},{"location":"day_one/overview/#the-philosophy","title":"The Philosophy","text":"<p>Throughout Day One, we emphasize safety and confidence. You're working on a shared development cluster:</p> <p>Read-Only Commands Are Safe</p> <p>These commands won't hurt anything\u2014explore freely:</p> <ul> <li><code>kubectl get</code> - See what's running</li> <li><code>kubectl describe</code> - Inspect resources</li> <li><code>kubectl logs</code> - Read application logs</li> <li><code>kubectl explain</code> - Get documentation</li> </ul> <p>Some Commands Need Care</p> <p>These commands modify or delete resources\u2014use with namespace awareness:</p> <ul> <li><code>kubectl delete</code> - Removes resources (yours, hopefully!)</li> <li><code>kubectl apply</code> - Creates/updates resources (stay in your namespace!)</li> <li><code>kubectl edit</code> - Modifies running resources</li> </ul> <p>We'll clearly label which is which throughout the articles.</p> <p>Your Namespace Is Your Sandbox</p> <p>Your company likely gave you a namespace (like <code>dev-yourname</code> or <code>team-frontend-dev</code>). Think of it as your personal workspace:</p> <ul> <li>You can create/delete freely in YOUR namespace</li> <li>Other namespaces are off-limits (unless you're explicitly given access)</li> <li>Production namespaces require extra caution (healthy respect for production!)</li> </ul> <p>We'll teach you to always check which namespace you're working in.</p>"},{"location":"day_one/overview/#our-teaching-approach","title":"Our Teaching Approach","text":"<p>Day One focuses on practical, immediate needs\u2014the skills you need to deploy your application today.</p> <ul> <li> <p> We Teach</p> <p>The \"Why\" \u2014 Why Kubernetes exists, why this command, why it matters</p> <p>Real Scenarios \u2014 \"You need to deploy your app to dev\" not abstract theory</p> <p>Finding Answers \u2014 How to help yourself when stuck (<code>kubectl explain</code>, <code>describe</code>, <code>logs</code>)</p> <p>Safety Habits \u2014 Namespace awareness, read-only vs destructive commands</p> </li> </ul>"},{"location":"day_one/overview/#whats-next","title":"What's Next?","text":"<p>Once you're comfortable deploying applications and troubleshooting them, you're ready for Level 1: Core Primitives. That's where you'll dive deeper into Pods, Services, ConfigMaps, and the fundamental building blocks of Kubernetes\u2014regardless of whether you deployed them with kubectl or Helm.</p> <p>But first, let's get you deploying applications. That's what Day One is all about.</p>"},{"location":"day_one/overview/#ready","title":"Ready?","text":"<p>Start with What Is Kubernetes and Why? to understand what you're working with and why it matters.</p> <p>Your journey from intimidated app developer to confident Kubernetes user starts here. Let's get you deploying.</p>"},{"location":"day_one/what_is_kubernetes/","title":"What Is Kubernetes and Why?","text":"<p>Part of Day One: Getting Started</p> <p>This is the first article in Day One: Getting Started. Start here if you're brand new to Kubernetes.</p> <p>You just found out your company uses Kubernetes. Or maybe you saw it on a job description. Or your manager said \"we're deploying to K8s now\" and you nodded along, hoping it would make sense eventually.</p> <p>Let's make it make sense.</p> <p>What You'll Learn</p> <p>By the end of this article, you'll understand:</p> <ul> <li>What Kubernetes is - Container orchestration at scale</li> <li>Why it exists - The problem it solves (managing thousands of containers)</li> <li>Why your company uses it - Real-world benefits (reliability, scaling, zero-downtime deploys)</li> <li>What problems it doesn't solve - Setting realistic expectations</li> <li>Whether you need to learn it - Spoiler: yes, if you're deploying to it!</li> </ul>"},{"location":"day_one/what_is_kubernetes/#the-container-orchestration-challenge","title":"The Container Orchestration Challenge","text":"<pre><code>flowchart TD\n    Dev[Development&lt;br/&gt;docker-compose up&lt;br/&gt;\u2705 Easy]\n    Prod[Production&lt;br/&gt;100 servers&lt;br/&gt;1000s of containers]\n    Manual[Manual Management&lt;br/&gt;\u274c Impossible]\n    K8s[Kubernetes&lt;br/&gt;\u2705 Automated]\n\n    Dev --&gt; Prod\n    Prod --&gt; Manual\n    Manual -.-&gt;|Solution| K8s\n\n    style Dev fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Prod fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Manual fill:#c53030,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style K8s fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre>"},{"location":"day_one/what_is_kubernetes/#the-problem-too-many-containers-not-enough-hands","title":"The Problem: Too Many Containers, Not Enough Hands","text":"<p>Imagine you've containerized your application. You have:</p> <ul> <li>A frontend container (React app)</li> <li>A backend container (API server)</li> <li>A cache container (Redis)</li> <li>Maybe a message queue (RabbitMQ)</li> </ul> <p>On your laptop: Docker Compose handles this beautifully. One <code>docker-compose up</code> and everything runs.</p> Don't have Docker Compose experience? That's okay! <p>Not everyone comes to Kubernetes from Docker Compose\u2014and that's perfectly fine.</p> <p>The key point: On a single computer, managing a few containers is easy. Your company probably already has a solution for this (Docker Compose, single-server Docker, or even just running processes directly).</p> <p>The problem Kubernetes solves isn't running containers on one machine\u2014it's managing hundreds or thousands of containers across dozens or hundreds of servers. That's where manual management becomes impossible.</p> <p>You can learn Kubernetes without Docker Compose experience. The concepts transfer: containers need to run, they need to talk to each other, they need to restart when they crash. Kubernetes does this at massive scale.</p> <p>In production: You have 50 frontend containers, 30 backend containers, 10 databases across 100 servers. Now you need to:</p> <ul> <li>Ensure containers are running (and restart them if they crash)</li> <li>Spread them across servers (load balancing)</li> <li>Connect them together (networking)</li> <li>Handle traffic spikes (scaling up/down)</li> <li>Deploy updates without downtime</li> <li>Monitor everything</li> </ul> <p>Doing this manually is impossible. This is why Kubernetes exists.</p>"},{"location":"day_one/what_is_kubernetes/#what-kubernetes-actually-is","title":"What Kubernetes Actually Is","text":"<p>Kubernetes (K8s) is a container orchestration platform. It's software that manages containers for you.</p> <p>Think of it like an operating system for a data center:</p> <ul> <li>Your laptop's OS manages processes, memory, and files</li> <li>Kubernetes manages containers, servers, and networking</li> </ul> <p>The key idea: You tell Kubernetes what you want (\"I want 3 copies of my API running\"), and Kubernetes makes it happen. If something breaks, Kubernetes fixes it automatically.</p> Wait, What's the Difference Between Container, Image, and Pod? <p>If you're new to containers, these terms can be confusing. Here's the relationship:</p> <p>Container Image (the blueprint):</p> <ul> <li>A packaged file containing your application code and all its dependencies</li> <li>Like a <code>.zip</code> file or installer\u2014it doesn't run by itself, it's just the package</li> <li>Stored in a registry (Docker Hub, AWS ECR, Google Artifact Registry, etc.)</li> <li>Example: <code>nginx:1.21</code> is an image\u2014the nginx web server version 1.21 packaged up</li> </ul> <p>Container (the running instance):</p> <ul> <li>A running instance of an image</li> <li>Like opening an application from an installer\u2014now it's actually executing</li> <li>Has its own filesystem, process space, and network</li> <li>Example: When you run <code>docker run nginx:1.21</code>, you start a container from the image</li> </ul> <p>Pod (Kubernetes wrapper):</p> <ul> <li>Kubernetes doesn't run containers directly\u2014it wraps them in Pods</li> <li>A Pod is the smallest unit Kubernetes manages</li> <li>Usually 1 container per Pod (but can have multiple containers that work together)</li> <li>Example: Your nginx container runs inside a Pod managed by Kubernetes</li> </ul> <p>The flow:</p> <pre><code>flowchart LR\n    Image[\"&lt;b&gt;Container Image&lt;/b&gt;&lt;br/&gt;nginx:1.21&lt;br/&gt;(stored in registry)\"]\n    Container[\"&lt;b&gt;Container&lt;/b&gt;&lt;br/&gt;Running nginx process&lt;br/&gt;(executing)\"]\n    Pod[\"&lt;b&gt;Pod&lt;/b&gt;&lt;br/&gt;Kubernetes wrapper&lt;br/&gt;(managed by K8s)\"]\n\n    Image --&gt;|\"docker run&lt;br/&gt;or kubectl\"| Container\n    Container --&gt;|\"wrapped inside\"| Pod\n\n    style Image fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Container fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Pod fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre> <p>TL;DR: Image = packaged app, Container = running app, Pod = Kubernetes' management unit for containers</p> What Does 'Telling Kubernetes What You Want' Look Like? <p>You interact with Kubernetes using the <code>kubectl</code> command-line tool.</p> <p>Here's what checking on your app looks like:</p> <pre><code>kubectl get pods\n# NAME                       READY   STATUS    RESTARTS   AGE\n# my-app-7c5ddbdf54-abc123   1/1     Running   0          2m\n# my-app-7c5ddbdf54-def456   1/1     Running   0          2m\n# my-app-7c5ddbdf54-ghi789   1/1     Running   0          2m\n</code></pre> <p>That's it. One command to see what's running. Three copies of your app, all healthy.</p> <p>Don't worry if this looks foreign. In the next article, you'll run your first <code>kubectl</code> commands, and they'll become second nature quickly.</p>"},{"location":"day_one/what_is_kubernetes/#the-shipping-container-analogy","title":"The Shipping Container Analogy","text":"<p>The name \"Kubernetes\" means \"helmsman\" (ship pilot) in Greek. The logo is a ship's wheel. This isn't random\u2014containers (Docker) are literally named after shipping containers.</p> <p>Before shipping containers (1950s):</p> <ul> <li>Every cargo was different (boxes, barrels, crates)</li> <li>Loading/unloading was manual and slow</li> <li>Each port handled things differently</li> </ul> <p>After shipping containers:</p> <ul> <li>Everything goes in standard 20' or 40' boxes</li> <li>Cranes can move them automatically</li> <li>Any port can handle any container</li> <li>Global trade exploded</li> </ul> <p>Kubernetes is the Port Authority:</p> <ul> <li>Docker containers are the standardized boxes</li> <li>Kubernetes is the crane system that moves them around</li> <li>It doesn't matter what's inside the container\u2014K8s handles it the same way</li> </ul>"},{"location":"day_one/what_is_kubernetes/#why-companies-adopt-kubernetes","title":"Why Companies Adopt Kubernetes","text":"<ul> <li> <p> Run Anywhere</p> <p>Why it matters: No vendor lock-in, move workloads freely</p> <p>Same Kubernetes runs on:</p> <ul> <li>AWS (EKS)</li> <li>Google Cloud (GKE)</li> <li>Azure (AKS)</li> <li>Your company's data center</li> <li>Your laptop (for development)</li> </ul> <p>Benefit: Switch cloud providers without rewriting deployment infrastructure.</p> </li> <li> <p> Self-Healing</p> <p>Why it matters: Fewer 3 AM pages, automatic recovery</p> <p>Kubernetes automatically handles failures:</p> <ul> <li>Container crashes \u2192 Kubernetes restarts it</li> <li>Server dies \u2192 Kubernetes moves containers to healthy servers</li> <li>Traffic spike \u2192 Kubernetes scales up automatically</li> </ul> <p>Benefit: Operations team sleeps better, applications stay running.</p> </li> <li> <p> Declarative Configuration</p> <p>Why it matters: Infrastructure as Code, everything version-controlled</p> <p>Traditional approach: Imperative scripts</p> <pre><code># Run these commands in this exact order...\ndocker run container-a\nsleep 5\ndocker run container-b\n# Hope nothing breaks!\n</code></pre> <p>Kubernetes approach: Declarative YAML</p> <pre><code># Describe desired state, Kubernetes figures out how\nspec:\n  replicas: 3  # I want 3 running\n  containers:\n    - name: my-app\n</code></pre> <p>Benefit: Git tracks changes, rollbacks are easy, no procedural scripts.</p> </li> <li> <p> Rolling Updates</p> <p>Why it matters: Deploy anytime, no maintenance windows</p> <p>Update your app from v1 to v2 without downtime:</p> <ol> <li>Kubernetes starts new v2 containers</li> <li>Waits for them to be healthy</li> <li>Gradually stops v1 containers</li> <li>If v2 fails, automatically rolls back to v1</li> </ol> <p>Benefit: Deploy during business hours, users never notice.</p> </li> </ul>"},{"location":"day_one/what_is_kubernetes/#what-kubernetes-isnt","title":"What Kubernetes Isn't","text":"<p>Kubernetes is NOT:</p> <ul> <li>\u274c A replacement for Docker (K8s uses Docker/containerd)</li> <li>\u274c A cloud provider (it runs ON clouds)</li> <li>\u274c Easy (it's powerful but complex)</li> <li>\u274c Required for small projects (might be overkill)</li> </ul> <p>Kubernetes IS:</p> <ul> <li>\u2705 An orchestrator for containers</li> <li>\u2705 Platform for running distributed systems</li> <li>\u2705 Industry standard for production deployments</li> <li>\u2705 Worth learning if you're shipping software at scale</li> </ul>"},{"location":"day_one/what_is_kubernetes/#the-trade-off","title":"The Trade-Off","text":"<p>Complexity vs. Capability</p> <p>Kubernetes adds complexity:</p> <ul> <li>New concepts to learn (Pods, Services, Deployments)</li> <li>YAML configuration everywhere</li> <li>More moving parts</li> </ul> <p>Kubernetes adds capability:</p> <ul> <li>Automatic scaling and healing</li> <li>Zero-downtime deployments</li> <li>Runs anywhere</li> <li>Battle-tested at Google/Cloud Native scale</li> </ul> <p>When it's worth it: Teams shipping multiple services, need high availability, or running at scale.</p> <p>When it's not: Single-server apps, hobby projects, teams without ops experience.</p>"},{"location":"day_one/what_is_kubernetes/#your-company-probably-uses-kubernetes-if","title":"Your Company Probably Uses Kubernetes If...","text":"<ul> <li> <p> Microservices Architecture</p> <p>You have 10+ independent services (not a monolith)</p> </li> <li> <p> High Availability Requirements</p> <p>Need 99.9%+ uptime, can't afford extended outages</p> </li> <li> <p> Frequent Deployments</p> <p>Deploy multiple times per day, need fast iteration</p> </li> <li> <p> Major Cloud Provider</p> <p>Running on AWS, GCP, or Azure (all offer managed K8s)</p> </li> <li> <p> Platform/DevOps Team</p> <p>Company has dedicated infrastructure team</p> </li> </ul> <p>If 2 or more apply: Kubernetes makes sense for your company.</p> <p>What this means for you: You don't need to learn how to install Kubernetes (that's the platform team's job). You need to learn how to use Kubernetes to deploy your applications.</p> <p>That's what Day One is about.</p>"},{"location":"day_one/what_is_kubernetes/#what-youll-actually-do-with-kubernetes","title":"What You'll Actually Do with Kubernetes","text":"<p>Remember the scenarios from the overview? Here's how Kubernetes addresses them:</p> <ul> <li>Deploy your app \u2192 <code>kubectl apply</code> pushes your changes to the cluster</li> <li>Check logs \u2192 <code>kubectl logs</code> shows what's happening inside containers</li> <li>Update config \u2192 ConfigMaps and Secrets manage environment variables</li> <li>Roll back \u2192 <code>kubectl rollout undo</code> instantly reverts bad deployments</li> <li>Scale \u2192 <code>kubectl scale</code> adjusts how many copies are running</li> </ul> <p>We'll cover each of these in Day One and Level 1-2.</p>"},{"location":"day_one/what_is_kubernetes/#reflection-questions","title":"Reflection Questions","text":"<p>These aren't hands-on exercises (we'll do that in the next article), but take a moment to think through these questions:</p> Exercise 1: Identify Your Scenario <p>Which of these describes your company?</p> <ul> <li>Monolithic application on a single server</li> <li>Microservices architecture (10+ services)</li> <li>High availability requirements (99.9%+ uptime)</li> <li>Multiple deployments per day</li> <li>Running on major cloud provider (AWS, GCP, Azure)</li> </ul> <p>How many apply? If you checked 2 or more, Kubernetes makes sense for your company.</p> Why This Matters <p>Understanding why your company adopted Kubernetes helps you appreciate the complexity trade-off. If you're running 50 microservices across 100 servers, the overhead of learning Kubernetes is worth it. If you have 1 app on 1 server, maybe not.</p> Exercise 2: Match the Problem to the Solution <p>We listed 6 orchestration challenges earlier. Can you match each problem to the Kubernetes feature that solves it?</p> <p>Problems:</p> <ol> <li>Containers crash and need to restart automatically</li> <li>Load needs to be distributed across many servers</li> <li>Services need to find and talk to each other</li> <li>Traffic spikes require spinning up more instances quickly</li> <li>Updates need to happen without taking the app offline</li> <li>Need to monitor health across hundreds of containers</li> </ol> <p>Kubernetes Features: Self-healing, Load balancing, Service discovery, Scaling, Rolling updates, Health checks</p> Answers <ol> <li>Self-healing - Kubernetes restarts crashed containers automatically</li> <li>Load balancing - Services distribute traffic evenly across pods</li> <li>Service discovery - Kubernetes DNS lets services find each other by name</li> <li>Scaling - Deployments can increase/decrease replicas on demand</li> <li>Rolling updates - Gradual replacement of old containers with new ones</li> <li>Health checks - Probes continuously monitor container health</li> </ol> <p>The pattern: Every <code>kubectl</code> command you learn (coming in the next articles) maps back to solving one of these problems. Kubernetes isn't abstract\u2014it's solving real operational challenges your team faces daily.</p> Exercise 3: What's Your Current Deploy Process? <p>Before Kubernetes (or right now, if you haven't deployed yet):</p> <p>How does your team currently deploy applications?</p> <ul> <li>SSH into servers and run commands?</li> <li>CI/CD pipeline that deploys to VMs?</li> <li>Docker Compose on a single server?</li> <li>Already using Kubernetes (but you don't understand it yet)?</li> </ul> <p>Write down 2-3 pain points with your current process.</p> Why This Exercise Matters <p>When you deploy your first application to Kubernetes (next article!), you'll compare:</p> <p>Before: Manual SSH, forgotten steps, downtime during deploys, \"works on my machine\" problems</p> <p>After: <code>kubectl apply -f deployment.yaml</code> and Kubernetes handles the rest</p> <p>Understanding your current pain points helps you appreciate what Kubernetes solves.</p>"},{"location":"day_one/what_is_kubernetes/#quick-recap","title":"Quick Recap","text":"Question Answer What is Kubernetes? Container orchestration platform Why does it exist? Managing containers at scale is impossible manually What problem does it solve? Automated deployment, scaling, healing, and updates Do I need to learn it? If your company uses it, yes!"},{"location":"day_one/what_is_kubernetes/#further-reading","title":"Further Reading","text":""},{"location":"day_one/what_is_kubernetes/#official-documentation","title":"Official Documentation","text":"<ul> <li>What is Kubernetes? - Official overview</li> <li>Kubernetes Components - Architecture overview</li> </ul>"},{"location":"day_one/what_is_kubernetes/#deep-dives","title":"Deep Dives","text":"<ul> <li>The Illustrated Children's Guide to Kubernetes - Visual story explaining K8s concepts</li> <li>The Kubernetes Origin Story - How Google's Borg became Kubernetes</li> <li>Borg: The Predecessor to Kubernetes - Official Kubernetes blog on Borg history</li> </ul>"},{"location":"day_one/what_is_kubernetes/#related-articles","title":"Related Articles","text":"<ul> <li>Day One: Getting Started - Complete learning path overview</li> </ul>"},{"location":"day_one/what_is_kubernetes/#whats-next","title":"What's Next?","text":"<p>You understand why Kubernetes exists. Now let's get you connected: Getting kubectl Access will show you how to connect to your company's cluster and verify you're ready to deploy.</p> <p>Remember: Every Kubernetes expert started by asking \"What even is this?\" You're on your way.</p>"},{"location":"day_one/helm/access/","title":"Getting Helm Access","text":"<p>Part of Day One: Getting Started</p> <p>This is the second article in the Helm Path for Day One: Getting Started. Make sure you've read What Is Kubernetes? first.</p> <p>Your company's platform team told you: \"We use <code>Helm</code> for everything. Just install the CLI, and you're good to go.\" </p> <p>Maybe you've seen <code>Helm</code> mentioned in a CI/CD pipeline, or maybe you need to install a third-party tool like <code>Prometheus</code> or <code>Grafana</code>. Either way, you're not writing Kubernetes YAML from scratch today\u2014you're using a package manager.</p> <p>Good news: <code>Helm</code> makes deploying complex applications much simpler. It handles the \"glue\" that connects multiple Kubernetes resources together, so you don't have to.</p> <p>Let's get you set up and connected.</p> <p>What You'll Learn</p> <p>By the end of this article, you'll be able to:</p> <ul> <li>Install the Helm CLI on your operating system</li> <li>Verify the relationship between <code>Helm</code> and <code>kubectl</code></li> <li>Connect Helm to your cluster using your existing credentials</li> <li>Add and search chart repositories to find the software you need</li> <li>Understand your namespace sandbox and how to explore safely</li> </ul>"},{"location":"day_one/helm/access/#helm-kubernetes-package-manager","title":"Helm: Kubernetes Package Manager","text":"<p><code>Helm</code> is a local command-line tool that talks to the Kubernetes API on your behalf. It doesn't run as a service in your cluster\u2014it's purely client-side, just like <code>kubectl</code>.</p> <p>The key relationship:</p> <ul> <li>Kubernetes is the orchestration platform</li> <li><code>kubectl</code> is the CLI for direct cluster operations</li> <li><code>Helm</code> is the package manager (like <code>apt</code>/<code>brew</code> for your cluster)</li> </ul> <p>Important: <code>Helm</code> uses the same <code>kubeconfig</code> credentials you set up in Getting kubectl Access. Everything you learned there about authentication, contexts, and namespaces applies to <code>Helm</code>\u2014it's using the exact same configuration.</p> <pre><code>graph TD\n    You[You&lt;br/&gt;Developer]\n    Helm[Helm CLI]\n    kubectl[kubectl&lt;br/&gt;Credentials]\n    cluster[Kubernetes Cluster]\n\n    You --&gt;|runs| Helm\n    Helm --&gt;|uses| kubectl\n    kubectl --&gt;|authenticates to| cluster\n\n    style You fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Helm fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style kubectl fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style cluster fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre>"},{"location":"day_one/helm/access/#prerequisites-kubectl-access","title":"Prerequisites: kubectl Access","text":"<p>Before you can use <code>Helm</code>, you must have working access to your Kubernetes cluster. If you haven't set up your cluster credentials yet, stop here and follow the Getting kubectl Access guide first.</p> <p>Why? <code>Helm</code> uses the same configuration file (<code>~/.kube/config</code>) as <code>kubectl</code>. If <code>kubectl get pods</code> doesn't work, <code>Helm</code> won't work either.</p>"},{"location":"day_one/helm/access/#before-you-install-understanding-helms-hidden-cost","title":"Before You Install: Understanding Helm's Hidden Cost","text":"<p><code>Helm</code> makes deployment easier, but it adds complexity to troubleshooting. Be honest with yourself about this tradeoff before committing to the <code>Helm</code> path.</p> What You GainWhat You Trade <ul> <li> <p>Simplified deployments - One command deploys complex applications with multiple Kubernetes resources. No need to manually <code>kubectl apply</code> each file individually.</p> </li> <li> <p>Package management - Reusable charts for common software (<code>Prometheus</code>, <code>nginx</code>, ingress controllers). Install <code>Prometheus</code> or <code>nginx</code> with a single <code>helm install</code> command.</p> </li> <li> <p>Configuration management - Change settings through <code>values.yaml</code> files instead of editing raw YAML manifests. Update replicas, image tags, or environment variables without touching templates.</p> </li> <li> <p>Version control and rollbacks - Easy rollbacks with <code>helm rollback</code>. Every deployment is tracked as a revision\u2014undo bad changes instantly.</p> </li> </ul> <ul> <li> <p>Troubleshooting complexity - When things break, you debug through multiple layers: Is the problem in your <code>values.yaml</code>? The chart's templates? The rendered Kubernetes YAML? Or the running resources? Each layer adds places where things can go wrong.</p> </li> <li> <p>Abstraction opacity - Templates hide what's actually being deployed until you inspect the rendered output with <code>helm get manifest</code>. You're trusting the chart author's template logic without seeing the actual YAML upfront.</p> </li> <li> <p>Steeper learning curve - You need to understand both <code>Helm</code>'s templating system (Go templates, values hierarchy, chart structure) AND Kubernetes resources. You're learning two complex systems simultaneously instead of one.</p> </li> </ul> <p>Helm's Hidden Cost</p> <p>In large, complex applications, <code>Helm</code>'s abstraction layers can make troubleshooting significantly harder. A misconfigured value might produce invalid YAML that's difficult to trace. An error during template rendering looks different from a Kubernetes deployment error. You'll need <code>kubectl</code> skills anyway for debugging\u2014<code>Helm</code> doesn't eliminate the need to understand Kubernetes fundamentals.</p> <p>Our recommendation: If your team already uses <code>Helm</code> (CI/CD generates charts, standardized on <code>Helm</code> deployments), learn it. If you're choosing for yourself and learning Kubernetes for the first time, start with the kubectl path to build foundational understanding, then add <code>Helm</code> later.</p> <p>The rest of this article assumes you're committed to the <code>Helm</code> path. We'll teach you both <code>Helm</code> AND the underlying Kubernetes concepts so you can troubleshoot effectively.</p>"},{"location":"day_one/helm/access/#installing-helm","title":"Installing Helm","text":"<p>The <code>helm</code> CLI is a single binary that you've got to install on your machine.</p> macOSLinuxWindows <p>Using Homebrew (Recommended): Install Helm with Homebrew<pre><code>brew install helm\n</code></pre></p> <p>Verify installation: Verify Helm version<pre><code>helm version\n# Should show: version.BuildInfo{Version:\"v3.x.x\"...}\n</code></pre></p> <p>Using the automated script: Install Helm with official script<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre></p> <p>Verify installation: Verify Helm version<pre><code>helm version\n</code></pre></p> PowerShell (Manual)ChocolateyScoop <p>Works on any Windows system without package managers.</p> <p>Open PowerShell and run:</p> Download and install Helm manually<pre><code># Download latest Helm release\nInvoke-WebRequest -Uri https://get.helm.sh/helm-v3.14.0-windows-amd64.zip -OutFile helm.zip\n\n# Extract the archive\nExpand-Archive -Path helm.zip -DestinationPath $env:TEMP\\helm\n\n# Create bin directory if it doesn't exist\nNew-Item -Path \"$env:USERPROFILE\\bin\" -ItemType Directory -Force\n\n# Move helm.exe to your bin directory\nMove-Item -Path \"$env:TEMP\\helm\\windows-amd64\\helm.exe\" -Destination \"$env:USERPROFILE\\bin\\helm.exe\" -Force\n\n# Add to PATH (current session)\n$env:PATH += \";$env:USERPROFILE\\bin\"\n\n# Add to PATH permanently\n[Environment]::SetEnvironmentVariable(\"Path\", $env:PATH + \";$env:USERPROFILE\\bin\", [EnvironmentVariableTarget]::User)\n\n# Clean up\nRemove-Item -Path helm.zip, $env:TEMP\\helm -Recurse -Force\n</code></pre> <p>Verify installation:</p> <p>Close and reopen PowerShell, then run:</p> Verify Helm version<pre><code>helm version\n# Should show: version.BuildInfo{Version:\"v3.x.x\"...}\n</code></pre> <p>If you have Chocolatey installed:</p> Install Helm with Chocolatey<pre><code>choco install kubernetes-helm\n</code></pre> <p>Verify installation: Verify Helm version<pre><code>helm version\n</code></pre></p> <p>If you have Scoop installed:</p> Install Helm with Scoop<pre><code>scoop install helm\n</code></pre> <p>Verify installation: Verify Helm version<pre><code>helm version\n</code></pre></p>"},{"location":"day_one/helm/access/#verifying-your-connection","title":"Verifying Your Connection","text":"<p>Once <code>Helm</code> is installed, you need to make sure it can talk to your cluster.</p>"},{"location":"day_one/helm/access/#step-1-check-your-context-and-namespace","title":"Step 1: Check your context and namespace","text":"<p><code>Helm</code> uses whichever cluster and namespace you have set as \"current\" in <code>kubectl</code>. If you need a refresher on contexts and namespaces, see Understanding Contexts.</p> Quick verification<pre><code># Verify your current cluster\nkubectl config current-context\n\n# Verify your current namespace\nkubectl config view --minify | grep namespace\n</code></pre> <p>If you're in the right cluster and namespace, proceed to Step 2.</p>"},{"location":"day_one/helm/access/#step-2-test-helms-reach","title":"Step 2: Test Helm's reach","text":"<p>Run this command to see if <code>Helm</code> can list \"releases\" (deployed applications) in your current namespace.</p> List releases<pre><code>helm list\n# NAME    NAMESPACE    REVISION    UPDATED    STATUS    CHART    APP VERSION\n# (It's okay if this list is empty! It means you're connected.)\n</code></pre> <p>If you see an error:</p> <ul> <li><code>Error: Kubernetes cluster unreachable</code>: Check your VPN or cluster credentials.</li> <li><code>Error: namespaces \"...\" is forbidden</code>: You don't have permissions in your current namespace.</li> </ul>"},{"location":"day_one/helm/access/#adding-chart-repositories","title":"Adding Chart Repositories","text":"<p><code>Helm</code> finds software in repositories. Think of these like the \"sources\" in <code>apt</code> or \"taps\" in <code>brew</code>.</p>"},{"location":"day_one/helm/access/#common-repositories","title":"Common Repositories","text":"<p>Most developers start by adding a few standard repositories for common software (web servers, monitoring tools, ingress controllers).</p> Add common repositories<pre><code># Bitnami is a gold standard for reliable, secure charts\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n# Prometheus/Grafana community charts\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n\n# Always update after adding to get the latest metadata\nhelm repo update\n</code></pre>"},{"location":"day_one/helm/access/#searching-for-software","title":"Searching for Software","text":"<p>Once you've added repositories, you can search for the software you need.</p> Search for Nginx<pre><code>helm search repo nginx\n# NAME             CHART VERSION    APP VERSION    DESCRIPTION\n# bitnami/nginx    13.2.23          1.25.3         NGINX Open Source is an...\n</code></pre>"},{"location":"day_one/helm/access/#safety-and-exploration","title":"Safety and Exploration","text":"<p>Even though you're using <code>Helm</code>, namespace security works exactly the same as <code>kubectl</code>.</p> <p>Remember: Your Namespace Is Your Sandbox</p> <p><code>Helm</code> respects the same namespace boundaries you learned about in Getting kubectl Access.</p> <p>Quick safety checklist before deploying:</p> <ul> <li>Verify your namespace (you did this in Step 1 above)</li> <li><code>Helm</code> releases are namespace-specific\u2014just like <code>kubectl</code> resources</li> <li>Always double-check before running <code>helm install</code> or <code>helm uninstall</code></li> </ul>"},{"location":"day_one/helm/access/#practice-exercises","title":"Practice Exercises","text":"Exercise 1: Verify Installation <p>Run <code>helm version</code> and identify your client version. Then, verify that <code>helm list</code> runs without errors.</p> Solution <p><pre><code>helm version\nhelm list\n</code></pre> If <code>helm list</code> returns a header but no data, you are successful! It means the CLI is talking to the Kubernetes API correctly.</p> Exercise 2: Add a Repository <p>Add the Bitnami repository and search for \"redis\".</p> Solution <p><pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\nhelm search repo redis\n</code></pre> You should see several options, including <code>bitnami/redis</code> (a popular caching layer for Kubernetes).</p>"},{"location":"day_one/helm/access/#quick-recap","title":"Quick Recap","text":"Goal Command Install Helm <code>brew install helm</code> (macOS) Check Connection <code>helm list</code> Add Repo <code>helm repo add &lt;name&gt; &lt;url&gt;</code> Find Software <code>helm search repo &lt;keyword&gt;</code> Update Repos <code>helm repo update</code>"},{"location":"day_one/helm/access/#further-reading","title":"Further Reading","text":""},{"location":"day_one/helm/access/#official-documentation","title":"Official Documentation","text":"<ul> <li>Installing Helm - Official installation guide for all platforms</li> <li>Helm Quickstart Guide - Get started with <code>Helm</code> in 5 minutes</li> <li>Using Helm - Core <code>Helm</code> concepts and workflows</li> <li>Helm Repositories - How chart repositories work</li> </ul>"},{"location":"day_one/helm/access/#deep-dives","title":"Deep Dives","text":"<ul> <li>What is a Helm Chart? Tutorial for Beginners - Comprehensive beginner-friendly guide</li> <li>Helm GitHub Repository - Source code, issues, and community discussions</li> </ul>"},{"location":"day_one/helm/access/#related-articles","title":"Related Articles","text":"<ul> <li>Getting kubectl Access - Required before <code>Helm</code> will work</li> <li>What Is Kubernetes? - Core concepts</li> </ul>"},{"location":"day_one/helm/access/#whats-next","title":"What's Next?","text":"<p>You have the tools, and you have the access. Now let's actually put something on the cluster.</p>"},{"location":"day_one/helm/access/#next-your-first-helm-deployment-deploy-your-first-chart-whether-its-a-third-party-tool-or-your-own-application-from-a-cicd-pipeline","title":"Next: Your First Helm Deployment - Deploy your first chart, whether it's a third-party tool or your own application from a CI/CD pipeline.","text":""},{"location":"day_one/helm/access/#practice-exercises_1","title":"Practice Exercises","text":"Exercise 1: Verify Installation <p>Run <code>helm version</code> and identify your client version. Then, verify that <code>helm list</code> runs without errors.</p> Solution <p><pre><code>helm version\nhelm list\n</code></pre> If <code>helm list</code> returns a header but no data, you are successful! It means the CLI is talking to the Kubernetes API correctly.</p> Exercise 2: Add a Repository <p>Add the Bitnami repository and search for \"redis\".</p> Solution <p><pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\nhelm search repo redis\n</code></pre> You should see several options, including <code>bitnami/redis</code> (a popular caching layer for Kubernetes).</p>"},{"location":"day_one/helm/access/#quick-recap_1","title":"Quick Recap","text":"Goal Command Install Helm <code>brew install helm</code> (macOS) Check Connection <code>helm list</code> Add Repo <code>helm repo add &lt;name&gt; &lt;url&gt;</code> Find Software <code>helm search repo &lt;keyword&gt;</code> Update Repos <code>helm repo update</code>"},{"location":"day_one/helm/access/#further-reading_1","title":"Further Reading","text":""},{"location":"day_one/helm/access/#official-documentation_1","title":"Official Documentation","text":"<ul> <li>Installing Helm - Official installation guide for all platforms</li> <li>Helm Quickstart Guide - Get started with Helm in 5 minutes</li> <li>Using Helm - Core Helm concepts and workflows</li> <li>Helm Repositories - How chart repositories work</li> </ul>"},{"location":"day_one/helm/access/#deep-dives_1","title":"Deep Dives","text":"<ul> <li>What is a Helm Chart? Tutorial for Beginners - Comprehensive beginner-friendly guide</li> <li>Helm GitHub Repository - Source code, issues, and community discussions</li> </ul>"},{"location":"day_one/helm/access/#related-articles_1","title":"Related Articles","text":"<ul> <li>Getting kubectl Access - Required before Helm will work</li> <li>What Is Kubernetes? - Core concepts</li> </ul>"},{"location":"day_one/helm/access/#whats-next_1","title":"What's Next?","text":"<p>You have the tools, and you have the access. Now let's actually put something on the cluster.</p> <p>Next: Your First Helm Deployment - Deploy your first chart, whether it's a third-party tool or your own application from a CI/CD pipeline.</p>"},{"location":"day_one/helm/first_deploy/","title":"Your First Helm Deployment","text":"<p>Part of Day One: Getting Started</p> <p>This is the third article in the Helm Path for Day One: Getting Started. Make sure you've completed Getting Helm Access first.</p> <p>You have <code>helm</code> installed. You have cluster access. Now comes the moment of truth: putting your first application onto the cluster.</p> <p>In the <code>Helm</code> world, we don't just \"deploy\" files; we manage Releases. A release is a specific instance of a chart running in your cluster. You can have three different \"releases\" of the same <code>nginx</code> chart running in the same namespace, each with its own name and configuration.</p> <p>This article covers the two ways you'll most likely encounter <code>Helm</code> in your daily work.</p> <p>What You'll Learn</p> <p>By the end of this article, you'll be able to:</p> <ul> <li>Choose between the two common scenarios: CI/CD charts vs. Vendor charts</li> <li>Customize a deployment using a <code>values.yaml</code> file</li> <li>Install your first release and verify it's running</li> <li>Update a running application with new configuration</li> <li>Clean up resources when you're finished</li> </ul> <pre><code>graph TD\n    Task[You need to deploy&lt;br/&gt;to the cluster]\n    Task --&gt; Q{What are&lt;br/&gt;you deploying?}\n    Q --&gt;|Your app| S1[CI/CD Pipeline Chart&lt;br/&gt;local directory or .tgz]\n    Q --&gt;|Third-party tool| S2[Vendor Repository&lt;br/&gt;bitnami/nginx, prometheus...]\n    S1 --&gt; V1[Edit values.yaml&lt;br/&gt;Set image tag and replicas]\n    S2 --&gt; V2[Export and edit&lt;br/&gt;values.yaml]\n    V1 --&gt; Install[helm install release-name chart]\n    V2 --&gt; Install\n    Install --&gt; Verify[Verify with helm list&lt;br/&gt;and kubectl get all]\n\n    style Task fill:#1a202c,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Q fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style S1 fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style S2 fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style V1 fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style V2 fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Install fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Verify fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre>"},{"location":"day_one/helm/first_deploy/#which-scenario-are-you-in","title":"Which Scenario Are You In?","text":"<p>Before we start, identify which situation matches your task today:</p> <p>Check Your Namespace First</p> <p><code>helm install</code> deploys into your current namespace. Before running any install command, confirm you're in the right place:</p> <pre><code>kubectl config view --minify | grep namespace\n# namespace: dev-user\n</code></pre> <p>If you need a refresher on namespaces and contexts, see Verifying Your Connection.</p>  Scenario 1: The CI/CD Chart Scenario 2: The Vendor Chart <p>Your situation: Your company's CI/CD pipeline (Jenkins, GitLab, GitHub Actions) has built your code and packaged it into a <code>Helm</code> chart. You need to deploy your app to a dev environment.</p> <p>What you have: A directory containing a <code>Chart.yaml</code> and a <code>values.yaml</code> file, or a <code>.tgz</code> package.</p> <p>Your situation: You need to install a standard piece of infrastructure\u2014like a web server (<code>nginx</code>), a cache (<code>Redis</code>), or a monitoring tool (<code>Prometheus</code>)\u2014using a chart provided by a vendor or the community.</p> <p>What you have: A chart name like <code>bitnami/nginx</code> or <code>prometheus-community/prometheus</code>.</p>"},{"location":"day_one/helm/first_deploy/#examine-valuesyaml","title":"Examine values.yaml","text":"<p>In <code>Helm</code>, you never edit the templates (the complex Kubernetes YAML). Instead, you edit <code>values.yaml</code>. This file is the \"settings menu\" for your application. Make your changes here \u2014 image tag, replica count, environment variables \u2014 then commit the file before deploying.</p> values.yaml (example)<pre><code>replicaCount: 1  # (1)!\n\nimage:\n  repository: my-company/my-app\n  tag: \"v1.0.1\"  # (2)!\n\nservice:\n  type: ClusterIP\n  port: 80\n</code></pre> <ol> <li>How many copies of the app to run.</li> <li>The specific version of your code built by the CI pipeline.</li> </ol> <p>Edit the File, Then Commit</p> <p>Change <code>values.yaml</code> directly and commit it to version control before running <code>helm install</code>. Every deployment must be reproducible from your repo \u2014 never rely on flags or files that exist only on your machine.</p>"},{"location":"day_one/helm/first_deploy/#install-the-release","title":"Install the Release","text":"<p>Navigate to the directory containing your chart and run:</p> <p>\u26a0\ufe0f Caution (Modifies Resources):</p> Install from local directory<pre><code># Usage: helm install &lt;release-name&gt; &lt;path-to-chart&gt;\nhelm install my-app ./my-chart\n# NAME: my-app\n# LAST DEPLOYED: Mon Feb 10 10:00:00 2026\n# NAMESPACE: default\n# STATUS: deployed\n# REVISION: 1\n</code></pre>"},{"location":"day_one/helm/first_deploy/#find-the-available-settings","title":"Find the Available Settings","text":"<p>Every vendor chart has different options. Start by exporting the defaults so you can see what's configurable:</p> <p>\u2705 Safe (Read-Only):</p> Export default values<pre><code>helm show values bitnami/nginx &gt; nginx-values.yaml\n</code></pre>"},{"location":"day_one/helm/first_deploy/#customize-commit-and-deploy","title":"Customize, Commit, and Deploy","text":"<p>Open <code>nginx-values.yaml</code>, change the settings you need, then commit the file to version control. This file is now your source of truth for this deployment.</p> <p>Never Use --set</p> <p>The <code>--set</code> flag writes configuration into thin air \u2014 it exists nowhere but the command that ran it. If the release needs to be rebuilt, rolled back by a teammate, or audited, there is no record. Always make changes in a committed <code>values.yaml</code> file.</p> <p>\u26a0\ufe0f Caution (Modifies Resources):</p> Install using your committed values file<pre><code>helm install my-web-server bitnami/nginx -f nginx-values.yaml\n# NAME: my-web-server\n# LAST DEPLOYED: Mon Feb 10 10:05:00 2026\n# NAMESPACE: default\n# STATUS: deployed\n# REVISION: 1\n</code></pre> <p>Every setting is documented in your committed file. Anyone can reproduce or debug this deployment.</p>"},{"location":"day_one/helm/first_deploy/#verifying-the-deployment","title":"Verifying the Deployment","text":"<p>Regardless of how you installed it, you need to make sure it worked.</p>"},{"location":"day_one/helm/first_deploy/#check-helm-status","title":"Check Helm Status","text":"<p><code>Helm</code> will tell you if the Release was successful.</p> Check Helm release<pre><code>helm list\n# NAME       NAMESPACE REVISION STATUS   CHART         APP VERSION\n# my-app     default   1        deployed my-app-0.1.0  1.0.1\n</code></pre>"},{"location":"day_one/helm/first_deploy/#use-kubectl-to-investigate","title":"Use kubectl to Investigate","text":"<p><code>Helm</code> creates Kubernetes resources. Use your <code>kubectl</code> skills to see them. If you're new to <code>kubectl</code>, Essential kubectl Commands covers everything you'll need for day-to-day investigation.</p> <p>\u2705 Safe (Read-Only):</p> See resources created by Helm<pre><code>kubectl get all\n# NAME                          READY   STATUS    RESTARTS   AGE\n# pod/my-app-7c5ddbdf54-x8f9p   1/1     Running   0          2m\n#\n# NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n# service/my-app-svc   ClusterIP   10.96.0.1    &lt;none&gt;        80/TCP    2m\n#\n# NAME                     READY   UP-TO-DATE   AVAILABLE   AGE\n# deployment.apps/my-app   1/1     1            1           2m\n</code></pre>"},{"location":"day_one/helm/first_deploy/#updating-and-rolling-back","title":"Updating and Rolling Back","text":"<p>One of <code>Helm</code>'s superpowers is the ability to change things safely.</p>"},{"location":"day_one/helm/first_deploy/#upgrading","title":"Upgrading","text":"<p>Edit your <code>values.yaml</code>, commit the change, then run:</p> <p>\u26a0\ufe0f Caution (Modifies Resources):</p> Upgrade a release<pre><code># Scenario 1 (local chart - values.yaml is inside the chart directory)\nhelm upgrade my-app ./my-chart\n\n# Scenario 2 (vendor chart - values file committed separately)\nhelm upgrade my-web-server bitnami/nginx -f nginx-values.yaml\n# STATUS: deployed\n# REVISION: 2\n</code></pre>"},{"location":"day_one/helm/first_deploy/#rolling-back","title":"Rolling Back","text":"<p>If the upgrade breaks something, you can go back to the previous version instantly. First, check your release history to identify the revision you want:</p> <p>\u2705 Safe (Read-Only):</p> View release history<pre><code>helm history my-app\n# REVISION    UPDATED                     STATUS      CHART           APP VERSION    DESCRIPTION\n# 1           Mon Feb 10 10:00:00 2026    superseded  my-app-0.1.0    1.0.1          Install complete\n# 2           Mon Feb 10 11:00:00 2026    deployed    my-app-0.1.0    1.0.2          Upgrade complete\n</code></pre> <p>Then roll back to the revision you want:</p> <p>\u26a0\ufe0f Caution (Modifies Resources):</p> Rollback to revision 1<pre><code>helm rollback my-app 1\n# Rollback was a success! Happy Helming.\n</code></pre>"},{"location":"day_one/helm/first_deploy/#cleaning-up","title":"Cleaning Up","text":"<p>When you're done with your test, you can remove everything <code>Helm</code> created with one command.</p> <p>\ud83d\udea8 DANGER (Destructive):</p> Uninstall a release<pre><code>helm uninstall my-app\n# release \"my-app\" uninstalled\n</code></pre>"},{"location":"day_one/helm/first_deploy/#common-pitfalls","title":"Common Pitfalls","text":"Release Already Exists <p>Running <code>helm install</code> twice with the same release name fails:</p> <pre><code>Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n</code></pre> <p>Fix: Use <code>helm upgrade</code> instead, or <code>helm uninstall</code> first if you want a clean slate.</p> Deployed But Pods Aren't Running <p><code>helm list</code> shows <code>STATUS: deployed</code>, but your application isn't responding. <code>Helm</code> only tracks whether the chart was applied\u2014it doesn't wait for Pods to become healthy.</p> <p>Fix: Always follow <code>helm list</code> with <code>kubectl get pods</code>. If Pods show <code>CrashLoopBackOff</code> or <code>Error</code>, use <code>kubectl logs &lt;pod-name&gt;</code> to investigate.</p> Values Change Not Taking Effect <p>You edited <code>values.yaml</code> and ran <code>helm upgrade</code>, but nothing changed. Most likely cause: you forgot to save the file, or <code>Helm</code> is reading a cached version.</p> <p>Fix: Verify what <code>Helm</code> actually applied with <code>helm get values &lt;release-name&gt;</code>.</p> Release Not in helm list <p><code>helm list</code> shows nothing, but you know the release exists. <code>Helm</code>'s <code>list</code> command only shows releases in your current namespace.</p> <p>Fix: Search other namespaces with <code>helm list -n &lt;namespace&gt;</code> or <code>helm list -A</code> to list all namespaces.</p>"},{"location":"day_one/helm/first_deploy/#practice-exercise","title":"Practice Exercise","text":"Exercise: Deploy and Scale Nginx <ol> <li>Add the Bitnami repo (if you haven't already: <code>helm repo add bitnami https://charts.bitnami.com/bitnami</code>).</li> <li>Export the default <code>nginx</code> values to a file: <code>helm show values bitnami/nginx &gt; practice-values.yaml</code>.</li> <li>Open <code>practice-values.yaml</code> and set <code>replicaCount</code> to <code>3</code>. Save the file.</li> <li>Install a release named <code>practice-web</code> using your values file.</li> <li>Verify 3 pods are running with <code>kubectl get pods</code>.</li> <li>Uninstall the release.</li> </ol> Solution <pre><code># Export and edit values (set replicaCount: 3 in the file)\nhelm show values bitnami/nginx &gt; practice-values.yaml\n\n# Install using the committed values file\nhelm install practice-web bitnami/nginx -f practice-values.yaml\n\n# Verify\nkubectl get pods\n# NAME                            READY   STATUS    RESTARTS   AGE\n# practice-web-7c5ddbdf54-2xkqn   1/1     Running   0          20s\n# practice-web-7c5ddbdf54-8mz4p   1/1     Running   0          20s\n# practice-web-7c5ddbdf54-kx9qw   1/1     Running   0          20s\n\n# Cleanup\nhelm uninstall practice-web\n</code></pre>"},{"location":"day_one/helm/first_deploy/#quick-recap","title":"Quick Recap","text":"Action Command Why It Matters Deploy <code>helm install &lt;name&gt; &lt;chart&gt;</code> Creates a new Release Customize Edit and commit <code>values.yaml</code> Declarative, reproducible config Verify <code>helm list</code> Confirms the release status Update <code>helm upgrade &lt;name&gt; &lt;chart&gt;</code> Applies changes to a release Rollback <code>helm rollback &lt;name&gt; &lt;rev&gt;</code> Reverts to a known good state Delete <code>helm uninstall &lt;name&gt;</code> Removes all created resources"},{"location":"day_one/helm/first_deploy/#further-reading","title":"Further Reading","text":""},{"location":"day_one/helm/first_deploy/#official-documentation","title":"Official Documentation","text":"<ul> <li>helm install Reference - Full options for the <code>helm install</code> command and chart sources</li> <li>helm upgrade Reference - Complete options for upgrading and customizing releases</li> <li>Using Helm - Core <code>Helm</code> workflows: searching, installing, and managing releases</li> </ul>"},{"location":"day_one/helm/first_deploy/#deep-dives","title":"Deep Dives","text":"<ul> <li>Helm Chart Structure - Understanding the directory layout and files in a chart</li> <li>The <code>values.yaml</code> Hierarchy - How values are merged and overridden in <code>Helm</code></li> </ul>"},{"location":"day_one/helm/first_deploy/#related-articles","title":"Related Articles","text":"<ul> <li>Getting Helm Access - Prerequisites for this article: install, connect, and add repositories</li> </ul>"},{"location":"day_one/helm/first_deploy/#whats-next","title":"What's Next?","text":"<p>You've deployed your first application using <code>Helm</code>! You've seen how <code>values.yaml</code> controls the deployment and how <code>Helm</code> manages the lifecycle.</p> <p>Next: Essential Helm Commands - Master the lifecycle management commands for investigating, updating, and fixing your releases (coming soon).</p> <p>Also worth exploring: If you haven't worked through the <code>kubectl</code> path, Essential kubectl Commands is useful reading\u2014<code>Helm</code> creates the same Kubernetes resources, and you'll use <code>kubectl</code> daily for debugging regardless of how you deploy.</p>"},{"location":"day_one/kubectl/access/","title":"Getting kubectl Access","text":"<p>Part of Day One: Getting Started</p> <p>This is the second article in Day One: Getting Started. Make sure you've read What Is Kubernetes? first.</p> <p>Before starting, your platform team should have provided:</p> <p>Kubernetes Onboarding Checklist</p> <p>Before you can connect, your platform team needs to give you:</p> <p>Required:</p> <ul> <li>Cluster name - Which Kubernetes cluster to connect to (e.g., <code>dev-cluster</code>, <code>eks-production</code>)</li> <li>Your namespace - Where you can deploy (e.g., <code>dev-yourteam</code>, <code>yourname-dev</code>)</li> <li>Authentication method - How to login (AWS CLI, Azure CLI, OIDC, etc.)</li> <li>Connection instructions - Commands to run or documentation link</li> </ul> <p>Optional (depends on your company's setup):</p> <ul> <li>Cluster region/location - For cloud providers (e.g., <code>us-west-2</code>, <code>europe-west1</code>)</li> <li>VPN requirements - Do you need to be on VPN to access the cluster?</li> <li>Registry credentials - For pulling images from private container registries</li> <li>Kubeconfig file - Some companies provide a ready-to-use config file</li> <li>Resource limits - What your namespace is allowed to use (CPU/memory quotas)</li> <li>Support channel - Slack channel or contact for help</li> </ul> <p>Missing something? Ask your platform team:</p> <ul> <li>\"What cluster should I connect to?\"</li> <li>\"What namespace am I assigned?\"</li> <li>\"How do I authenticate to the cluster?\"</li> <li>\"Do I need VPN to access the cluster?\"</li> </ul> <p>If your platform team sent you a Slack message like \"install <code>kubectl</code> and run this command,\" you're ready to start!</p> <p>Good news: Modern Kubernetes authentication generates YOUR individual credentials automatically. No shared passwords, no precious files to protect. Your identity = your credentials = audit trail shows what YOU did.</p> <p>Let's get you connected.</p> <p>What You'll Learn</p> <p>By the end of this article, you'll be able to:</p> <ul> <li>Install <code>kubectl</code> on your operating system (macOS, Linux, or Windows)</li> <li>Understand which authentication method your company uses (cloud provider, OIDC, or certificates)</li> <li>Connect to your company's Kubernetes cluster with YOUR individual credentials</li> <li>Verify your access and check which namespace you're assigned</li> <li>Switch between multiple Kubernetes contexts (dev, staging, prod)</li> <li>Troubleshoot common connection problems</li> </ul>"},{"location":"day_one/kubectl/access/#what-youre-connecting-to","title":"What You're Connecting To","text":"<p>Your company has a Kubernetes cluster\u2014a group of servers running Kubernetes. You're not managing the cluster; you're just deploying your applications to it.</p> <p>Think of it like:</p> <ul> <li>The cluster is a shared server</li> <li>Your namespace is your home directory</li> <li><code>kubectl</code> is your SSH client</li> </ul> <p>You have access to YOUR namespace, not the whole cluster.</p> <p>Namespace isolation means you only see resources in YOUR namespace by design. Other teams have their own namespaces isolated from yours. If <code>kubectl get pods</code> returns \"No resources found,\" that's normal\u2014you haven't deployed anything yet. Other teams' applications are running in their namespaces, invisible to you unless you explicitly switch namespaces with <code>-n other-namespace</code>. This isolation is a security and organizational feature, not a limitation.</p> <pre><code>graph TD\n    You[You&lt;br/&gt;Developer]\n    kubectl[kubectl CLI]\n    auth[Your Identity&lt;br/&gt;AWS IAM / Azure AD / OIDC / Cert]\n    kubeconfig[~/.kube/config&lt;br/&gt;generated config]\n    cluster[Kubernetes Cluster]\n    namespace[Your Namespace&lt;br/&gt;dev-yourteam]\n\n    You --&gt;|runs commands| kubectl\n    kubectl --&gt;|reads| kubeconfig\n    kubeconfig --&gt;|authenticates with| auth\n    auth --&gt;|proves identity to| cluster\n    cluster --&gt;|grants access to| namespace\n\n    style You fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style kubectl fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style auth fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style kubeconfig fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style cluster fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style namespace fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre>"},{"location":"day_one/kubectl/access/#installing-kubectl","title":"Installing kubectl","text":"<p><code>kubectl</code> (pronounced \"kube-control\" or \"kube-cuttle\") is the command-line tool for talking to Kubernetes.</p> macOSLinuxWindows <p>Using Homebrew: Install kubectl with Homebrew<pre><code>brew install kubectl\n</code></pre></p> <p>Verify installation: Verify kubectl installation<pre><code>kubectl version --client\n# Should show: Client Version: v1.28.x or similar\n</code></pre></p> <p>Apple Silicon (M1/M2/M3) Macs</p> <p>Using Homebrew? You're all set\u2014Homebrew automatically installs the correct ARM64 version.</p> <p>Manual download? Use the <code>darwin/arm64</code> binary instead of <code>darwin/amd64</code>:</p> Download kubectl for Apple Silicon<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre> <p>Intel Mac? Use <code>darwin/amd64</code> instead of <code>darwin/arm64</code> in the URL above.</p> <p>Download and install: Download and install kubectl<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre></p> <p>Verify installation: Verify kubectl installation<pre><code>kubectl version --client\n</code></pre></p> Don't have sudo access? Install to your home directory <p>If you don't have sudo privileges or prefer a user-space installation:</p> Install kubectl without sudo<pre><code># Download kubectl\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n\n# Make it executable\nchmod +x kubectl\n\n# Create local bin directory if it doesn't exist\nmkdir -p ~/.local/bin\n\n# Move kubectl there\nmv kubectl ~/.local/bin/\n</code></pre> <p>Add to PATH (if not already):</p> Add ~/.local/bin to PATH<pre><code># Add to your shell config (~/.bashrc, ~/.zshrc, etc.)\necho 'export PATH=\"$HOME/.local/bin:$PATH\"' &gt;&gt; ~/.bashrc\n\n# Reload your shell config\nsource ~/.bashrc\n</code></pre> <p>Verify: Verify kubectl installation<pre><code>kubectl version --client\n</code></pre></p> PowerShell (Recommended)Manual DownloadPackage Managers <p>Open PowerShell as Administrator and run:</p> Download kubectl<pre><code># Download a recent stable release (check https://kubernetes.io/releases/ for latest)\ncurl.exe -LO \"https://dl.k8s.io/release/v1.31.0/bin/windows/amd64/kubectl.exe\"  # (1)!\n\n# Move to a permanent location\nNew-Item -Path \"$env:USERPROFILE\\bin\" -ItemType Directory -Force\nMove-Item -Path \"kubectl.exe\" -Destination \"$env:USERPROFILE\\bin\\kubectl.exe\"\n</code></pre> <ol> <li>v1.31.0 is used as an example - <code>kubectl</code> versions are typically compatible across multiple Kubernetes versions. Your cluster's version doesn't need to match exactly. Check Kubernetes releases for the latest stable version.</li> </ol> <p>Add to your PATH (still in PowerShell as Administrator):</p> Add kubectl to PATH<pre><code># Get current PATH\n$currentPath = [Environment]::GetEnvironmentVariable(\"Path\", \"User\")\n\n# Add kubectl directory to PATH\n[Environment]::SetEnvironmentVariable(\n    \"Path\",\n    \"$currentPath;$env:USERPROFILE\\bin\",\n    \"User\"\n)\n</code></pre> <p>Close and reopen PowerShell (needed for PATH change to take effect).</p> <p>If you prefer clicking through:</p> <ol> <li> <p>Download a recent release:</p> <ul> <li>For latest version: Check Kubernetes releases first, then download</li> <li>Or use v1.31.0 (example): Download <code>kubectl.exe</code></li> <li>Save the file to your Downloads folder</li> </ul> </li> <li> <p>Create a directory for <code>kubectl</code>:</p> <ul> <li>Open File Explorer</li> <li>Navigate to <code>C:\\Users\\YourUsername\\</code></li> <li>Create a new folder called <code>bin</code></li> <li>Move <code>kubectl.exe</code> from Downloads to <code>C:\\Users\\YourUsername\\bin\\</code></li> </ul> </li> <li> <p>Add to PATH:</p> <ul> <li>Press <code>Win + X</code>, select \"System\"</li> <li>Click \"Advanced system settings\" (right side)</li> <li>Click \"Environment Variables\" button</li> <li>Under \"User variables\", find and select \"Path\", click \"Edit\"</li> <li>Click \"New\" and add: <code>C:\\Users\\YourUsername\\bin</code></li> <li>Click OK on all windows</li> <li>Restart any open PowerShell or Command Prompt windows</li> </ul> </li> </ol> <p>If you use Chocolatey: Install with Chocolatey<pre><code>choco install kubernetes-cli\n</code></pre></p> <p>If you use winget (Windows 10+): Install with winget<pre><code>winget install -e --id Kubernetes.kubectl\n</code></pre></p> <p>Verify Installation:</p> <p>Open a new PowerShell window (not as Administrator needed) and run:</p> Verify kubectl works<pre><code>kubectl version --client\n# Should show: Client Version: v1.31.x\n</code></pre> <p>Troubleshooting</p> <p>If you get \"<code>kubectl</code> is not recognized\":</p> <ol> <li>Make sure you opened a new PowerShell window after changing PATH</li> <li>Verify <code>kubectl.exe</code> exists: <code>Test-Path \"$env:USERPROFILE\\bin\\kubectl.exe\"</code></li> <li>Check your PATH includes the bin folder: <code>$env:PATH -split ';' | Select-String 'bin'</code></li> </ol>"},{"location":"day_one/kubectl/access/#what-if-i-cant-install-kubectl","title":"What If I Can't Install kubectl?","text":"<p>Restricted Corporate Environment?</p> <p>If you can't install kubectl (restricted machine, lack of permissions, security policies), you have options:</p> <p>Common Scenarios:</p> <ul> <li> <p> No Admin/Sudo Access</p> <p>Solution: Install to your home directory (no permissions needed)</p> <ul> <li>Linux/Mac: Use the \"Don't have sudo access?\" section above</li> <li>Windows: Install to <code>$env:USERPROFILE\\bin</code> (PowerShell method works without admin)</li> </ul> <p>All installation methods shown above work without admin privileges.</p> </li> <li> <p> Corporate Security Blocks kubectl</p> <p>Solution: Contact your platform or IT team</p> <p>Your company may provide:</p> <ul> <li>Pre-approved kubectl installer package</li> <li>Company-managed software portal</li> <li>Already installed on developer VMs</li> <li>Web-based kubectl access (Kubernetes Dashboard, Lens, k9s)</li> </ul> <p>Ask: \"How do I get kubectl installed on my machine?\" or \"Is there an approved kubectl installer?\"</p> </li> <li> <p> Using a Managed Development Environment</p> <p>Solution: kubectl might already be there</p> <p>If your company provides:</p> <ul> <li>Cloud-based IDEs (GitHub Codespaces, GitPod, AWS Cloud9)</li> <li>Jump boxes or bastion hosts</li> <li>Developer VMs</li> </ul> <p>Check first: Run <code>kubectl version --client</code> in your terminal\u2014it might already be installed.</p> </li> <li> <p> Can't Install Anything</p> <p>Solution: Web-based alternatives exist</p> <p>Ask your platform team if they provide:</p> <ul> <li>Kubernetes Dashboard - Web UI for the cluster</li> <li>Rancher - Multi-cluster management with web UI</li> <li>Lens - Desktop Kubernetes IDE (might be pre-approved)</li> <li>OpenShift Console - Built-in web UI (if using OpenShift)</li> </ul> <p>These aren't as powerful as <code>kubectl</code>, but they let you explore and deploy.</p> </li> </ul> <p>Bottom line: If you're blocked, your platform team can help. They want you to deploy\u2014they'll have a solution.</p>"},{"location":"day_one/kubectl/access/#using-openshift-use-oc-instead","title":"Using OpenShift? Use <code>oc</code> Instead","text":"<p>OpenShift Users: Read This First</p> <p>If your company uses Red Hat OpenShift (not vanilla Kubernetes), you'll use the <code>oc</code> command instead of <code>kubectl</code>.</p> <p>Good news: The <code>oc</code> command is fully compatible with <code>kubectl</code>\u2014all the commands you'll learn in this series work identically.</p> <p>What's OpenShift?</p> <p>OpenShift is Red Hat's Kubernetes distribution (like Ubuntu is a Linux distribution). It adds enterprise features on top of Kubernetes, but the core is still Kubernetes.</p> <p>Do I use <code>oc</code> or <code>kubectl</code>?</p> <p>Ask your platform team, or check their instructions:</p> <ul> <li>If they mention \"OpenShift\" or \"oc login\" \u2192 use <code>oc</code></li> <li>If they mention \"EKS\" (AWS), \"GKE\" (Google), or \"AKS\" (Azure) \u2192 use <code>kubectl</code></li> <li>Not sure? Try <code>oc version</code> in your terminal\u2014if it's installed, you're probably using OpenShift</li> </ul> <p>Installing <code>oc</code>:</p> macOSLinuxWindows Install oc with Homebrew<pre><code>brew install openshift-cli\n</code></pre> <p>Download from Red Hat OpenShift downloads:</p> Install oc on Linux<pre><code>wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/openshift-client-linux.tar.gz\ntar -xzf openshift-client-linux.tar.gz\nsudo mv oc /usr/local/bin/\nsudo mv kubectl /usr/local/bin/  # OpenShift includes kubectl too\n</code></pre> <p>Download from Red Hat OpenShift downloads:</p> <ol> <li>Download and extract the ZIP</li> <li>Move <code>oc.exe</code> to <code>C:\\Users\\YourUsername\\bin\\</code></li> <li>Add to PATH (same steps as kubectl installation above)</li> </ol> <p>Using <code>oc</code> with this guide:</p> <p>Throughout this series, whenever you see <code>kubectl</code>, just use <code>oc</code> instead:</p> <ul> <li><code>kubectl get pods</code> \u2192 <code>oc get pods</code> (works identically)</li> <li><code>kubectl apply -f file.yaml</code> \u2192 <code>oc apply -f file.yaml</code> (works identically)</li> <li><code>kubectl describe pod my-pod</code> \u2192 <code>oc describe pod my-pod</code> (works identically)</li> </ul> <p>Bonus: <code>oc</code> has some OpenShift-specific commands (<code>oc new-app</code>, <code>oc new-project</code>) that <code>kubectl</code> doesn't have, but you don't need those for Day One.</p> <p>For the Rest of This Article</p> <p>We'll use <code>kubectl</code> in all examples, but if you're using OpenShift, mentally substitute <code>oc</code> every time you see it.</p>"},{"location":"day_one/kubectl/access/#getting-your-credentials","title":"Getting Your Credentials","text":"<p><code>kubectl</code> needs to know:</p> <ol> <li>Where the cluster is (API server address)</li> <li>Who you are (YOUR credentials, not shared)</li> <li>Which namespace to use</li> </ol> <p>This information gets stored in a kubeconfig file:</p> <ul> <li>Linux/Mac: <code>~/.kube/config</code></li> <li>Windows: <code>%USERPROFILE%\\.kube\\config</code> (or <code>$env:USERPROFILE\\.kube\\config</code> in PowerShell)</li> </ul> <p>But you don't create this file manually\u2014your platform team's instructions will generate it with YOUR individual credentials.</p>"},{"location":"day_one/kubectl/access/#which-method-does-your-company-use","title":"Which Method Does Your Company Use?","text":"<p>Check what your platform team told you:</p> <pre><code>graph TD\n    Start[Platform team gave you&lt;br/&gt;instructions]\n    CheckInstructions{What did they&lt;br/&gt;tell you to install?}\n\n    AWS[AWS CLI&lt;br/&gt;aws]\n    GCP[gcloud CLI&lt;br/&gt;gcloud]\n    Azure[Azure CLI&lt;br/&gt;az]\n    Browser[Login via&lt;br/&gt;browser/portal]\n    CertFile[Certificate files&lt;br/&gt;.crt and .key]\n    ConfigFile[kubeconfig file&lt;br/&gt;to copy]\n\n    Start --&gt; CheckInstructions\n\n    CheckInstructions --&gt;|aws CLI| AWS\n    CheckInstructions --&gt;|gcloud CLI| GCP\n    CheckInstructions --&gt;|az CLI| Azure\n    CheckInstructions --&gt;|Browser login&lt;br/&gt;URL/portal| Browser\n    CheckInstructions --&gt;|Certificate files| CertFile\n    CheckInstructions --&gt;|Just a config file| ConfigFile\n\n    AWS --&gt; CloudProvider[\u2601\ufe0f Cloud Provider&lt;br/&gt;AWS EKS tab]\n    GCP --&gt; CloudProvider2[\u2601\ufe0f Cloud Provider&lt;br/&gt;GKE tab]\n    Azure --&gt; CloudProvider3[\u2601\ufe0f Cloud Provider&lt;br/&gt;AKS tab]\n    Browser --&gt; OIDC[\ud83d\udd10 OIDC/SSO Login tab]\n    CertFile --&gt; Certificate[\ud83d\udd11 Certificate-Based Auth tab]\n    ConfigFile --&gt; Shared[\u26a0\ufe0f Shared Config File tab&lt;br/&gt;Anti-pattern]\n\n    style Start fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style CheckInstructions fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style CloudProvider fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style CloudProvider2 fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style CloudProvider3 fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style OIDC fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Certificate fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Shared fill:#c53030,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style AWS fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style GCP fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Azure fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Browser fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style CertFile fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style ConfigFile fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre> <ul> <li> <p> AWS EKS</p> <p>You know it's AWS if:</p> <ul> <li>Instructions mention \"EKS\" or \"Elastic Kubernetes Service\"</li> <li>You were told to install AWS CLI (<code>aws</code>)</li> <li>You have AWS IAM access or AWS SSO login</li> </ul> <p>\u2192 Use the Cloud Provider (AWS EKS) tab below</p> </li> <li> <p> Google Cloud GKE</p> <p>You know it's GKE if:</p> <ul> <li>Instructions mention \"GKE\" or \"Google Kubernetes Engine\"</li> <li>You were told to install <code>gcloud</code></li> <li>You have a Google Cloud project assigned</li> </ul> <p>\u2192 Use the Cloud Provider (GKE) tab below</p> </li> <li> <p> Azure AKS</p> <p>You know it's AKS if:</p> <ul> <li>Instructions mention \"AKS\" or \"Azure Kubernetes Service\"</li> <li>You were told to install Azure CLI (<code>az</code>)</li> <li>You have Azure Active Directory access</li> </ul> <p>\u2192 Use the Cloud Provider (AKS) tab below</p> </li> <li> <p> Company SSO (Okta, Azure AD, Google)</p> <p>You know it's SSO if:</p> <ul> <li>Instructions mention logging in through a browser</li> <li>You were given a URL or portal to visit</li> <li>You log in with your company email/password + 2FA</li> </ul> <p>\u2192 Use the OIDC/SSO Login tab below</p> </li> </ul> <p>Still not sure? Ask your platform team: \"What authentication method does our Kubernetes cluster use?\"</p>"},{"location":"day_one/kubectl/access/#how-your-company-authenticates","title":"How Your Company Authenticates","text":"<p>Most enterprises use one of these methods:</p> Cloud Provider (Most Common)OIDC/SSO LoginCertificate-Based Auth\u26a0\ufe0f Shared Config File (Anti-Pattern) <p>Why it matters: Your company's cloud identity (AWS IAM, Google Cloud, Azure AD) becomes your Kubernetes identity. No separate passwords, automatic credential refresh, and IT manages everything in one place. This is the most common approach for companies using managed Kubernetes services (EKS, GKE, AKS).</p> <p>If your cluster runs on AWS, Google Cloud, or Azure, your cloud identity generates Kubernetes credentials automatically.</p> <p>AWS EKS:</p> <p>Your platform team should tell you the cluster name and region. If they didn't, here's how to find it:</p> How do I find my cluster name and region? <p>Ask your platform team first - they should tell you which cluster to use.</p> <p>If you have AWS CLI access and need to discover clusters:</p> List all EKS clusters you have access to<pre><code># List clusters in a specific region\naws eks list-clusters --region us-west-2\n\n# Check multiple common regions\nfor region in us-east-1 us-west-2 eu-west-1; do\n    echo \"Checking $region...\"\n    aws eks list-clusters --region $region\ndone\n</code></pre> <p>Most companies have a naming convention: - <code>dev-cluster</code>, <code>staging-cluster</code>, <code>prod-cluster</code> - <code>teamname-dev</code>, <code>teamname-prod</code> - <code>companyname-k8s-dev</code></p> <p>Check your onboarding docs, Confluence, or internal wiki for cluster details.</p> Connect to your EKS cluster<pre><code># Replace with your actual cluster name and region\naws eks update-kubeconfig --name my-cluster --region us-west-2\n\n# This generates ~/.kube/config with YOUR AWS IAM identity\n# Credentials auto-refresh using your `aws` CLI session\n</code></pre> <p>What just happened?</p> <ul> <li><code>aws eks update-kubeconfig</code> called the EKS API to get cluster connection details</li> <li>It added/updated <code>~/.kube/config</code> with the cluster's API server address</li> <li>It configured authentication to use YOUR AWS IAM identity (via <code>aws</code> CLI)</li> <li>Every <code>kubectl</code> command now authenticates as YOU using your AWS credentials</li> </ul> <p>Google Cloud GKE:</p> <p>Your platform team should tell you the cluster name and zone/region. If they didn't:</p> How do I find my cluster name and zone? <p>Ask your platform team first - they should specify which cluster and project to use.</p> <p>If you have gcloud CLI access:</p> List all GKE clusters you have access to<pre><code># List clusters in current project\ngcloud container clusters list\n\n# List clusters in a specific project\ngcloud container clusters list --project=my-project-id\n\n# See which project you're currently using\ngcloud config get-value project\n</code></pre> <p>The output shows: cluster name, location (zone or region), and status.</p> <p>Check your onboarding documentation for the correct project and cluster names.</p> Connect to your GKE cluster<pre><code># Replace with your actual cluster name and zone\ngcloud container clusters get-credentials my-cluster --zone us-central1-a\n\n# Or if using a regional cluster (multi-zone):\ngcloud container clusters get-credentials my-cluster --region us-central1\n\n# This generates ~/.kube/config with YOUR Google identity\n# Credentials auto-refresh using your `gcloud` session\n</code></pre> <p>What just happened?</p> <ul> <li><code>gcloud container clusters get-credentials</code> fetched cluster details from GKE API</li> <li>It updated <code>~/.kube/config</code> with the cluster's API server address</li> <li>It configured authentication to use YOUR Google Cloud identity (via <code>gcloud</code> CLI)</li> <li>Every <code>kubectl</code> command authenticates as YOU using your Google credentials</li> </ul> <p>Azure AKS:</p> <p>Your platform team should tell you the cluster name and resource group. If they didn't:</p> How do I find my cluster name and resource group? <p>Ask your platform team first - they should specify which cluster and subscription to use.</p> <p>If you have Azure CLI access:</p> List all AKS clusters you have access to<pre><code># List clusters in current subscription\naz aks list --output table\n\n# List clusters in a specific resource group\naz aks list --resource-group my-resource-group --output table\n\n# See which subscription you're currently using\naz account show\n\n# List all your subscriptions\naz account list --output table\n</code></pre> <p>The output shows: cluster name, resource group, location, and status.</p> <p>Resource groups typically named: - <code>rg-dev</code>, <code>rg-staging</code>, <code>rg-prod</code> - <code>teamname-resources</code>, <code>projectname-k8s</code></p> <p>Check your onboarding documentation for cluster and resource group names.</p> Connect to your AKS cluster<pre><code># Replace with your actual cluster name and resource group\naz aks get-credentials --name my-cluster --resource-group my-rg\n\n# This generates ~/.kube/config with YOUR Azure AD identity\n# Credentials auto-refresh using your `az` CLI session\n</code></pre> <p>What just happened?</p> <ul> <li><code>az aks get-credentials</code> fetched cluster details from AKS API</li> <li>It updated <code>~/.kube/config</code> with the cluster's API server address</li> <li>It configured authentication to use YOUR Azure AD identity (via <code>az</code> CLI)</li> <li>Every <code>kubectl</code> command authenticates as YOU using your Azure credentials</li> </ul> <p>Why this is better</p> <ul> <li>Your identity = Audit logs show what YOU did, not \"shared-user\"</li> <li>Auto-refresh = No expired credentials</li> <li>Revocation = You leave the company? Your cloud access is revoked = <code>kubectl</code> access revoked</li> <li>No shared secrets = Nothing precious to protect</li> </ul> <p>Why it matters: Single sign-on means one password for everything\u2014Kubernetes, email, internal tools. Your IT team adds/removes you from a group in Okta or Azure AD, and your Kubernetes access updates instantly. Browser-based login with 2FA is more secure than long-lived credentials.</p> <p>If your company uses Okta, Azure AD, or Google Workspace, you authenticate through your SSO provider using a browser login.</p> <p>Common tools your company might use:</p> kubelogin (Azure AD)TeleportGeneric OIDC (Okta, Google, etc.) <p>If your company uses Azure Active Directory for Kubernetes authentication:</p> <p>Install kubelogin: Install kubelogin for Azure AD authentication<pre><code># macOS\nbrew install Azure/kubelogin/kubelogin\n\n# Linux\ncurl -LO https://github.com/Azure/kubelogin/releases/latest/download/kubelogin-linux-amd64.zip\nunzip kubelogin-linux-amd64.zip &amp;&amp; sudo mv bin/linux_amd64/kubelogin /usr/local/bin/\n\n# Windows\nchoco install kubelogin\n</code></pre></p> <p>Your platform team will provide: - The kubeconfig file or command to generate it - Instructions to convert it to use Azure AD auth</p> <p>Typical flow: Configure Azure AD authentication for kubectl<pre><code># Get kubeconfig (platform team provides cluster name)\naz aks get-credentials --name my-cluster --resource-group my-rg\n\n# Convert to Azure AD login\nkubelogin convert-kubeconfig -l azurecli\n\n# Now when you run kubectl, it will open your browser for login\nkubectl get pods\n</code></pre></p> <p>What happens: First <code>kubectl</code> command opens browser \u2192 Azure AD login \u2192 token stored \u2192 future commands use the token until it expires (typically 1 hour).</p> <p>If your company uses Teleport for access management:</p> <p>Install Teleport client: Install Teleport<pre><code># macOS\nbrew install teleport\n\n# Linux - download from https://goteleport.com/download/\n# Windows - download installer from https://goteleport.com/download/\n</code></pre></p> <p>Your platform team will provide: - Teleport proxy URL (e.g., <code>teleport.company.com</code>) - Cluster name to connect to</p> <p>Login flow: Login to Kubernetes via Teleport<pre><code># Login to Teleport (opens browser for SSO)\ntsh login --proxy=teleport.company.com --user=your.email@company.com\n\n# List available Kubernetes clusters\ntsh kube ls\n\n# Login to specific cluster\ntsh kube login my-cluster\n\n# Now kubectl commands work\nkubectl get pods\n</code></pre></p> <p>What happens: <code>tsh</code> generates short-lived certificates (typically 12 hours) and updates your kubeconfig automatically.</p> <p>If your company uses Okta, Google Workspace, or another OIDC provider:</p> <p>Your platform team should provide:</p> <ul> <li>Specific <code>kubectl</code> plugin or tool to install</li> <li>The issuer URL (identity provider)</li> <li>Client ID for the Kubernetes cluster</li> </ul> <p>Common pattern with <code>kubectl oidc-login</code> plugin: Setup generic OIDC authentication<pre><code># Install plugin (example for macOS)\nbrew install int128/kubelogin/kubelogin\n\n# Platform team provides kubeconfig or setup command\nkubectl oidc-login setup \\\n  --oidc-issuer-url=https://accounts.company.com \\\n  --oidc-client-id=kubernetes-cluster-id\n\n# First kubectl command opens browser\nkubectl get pods\n</code></pre></p> <p>Company-specific tools:</p> <p>Some companies build internal tools. Your instructions might look like: Company-specific authentication tool<pre><code># Company-specific login command (example)\ncompany-k8s-login dev\n\n# Or a web portal where you download a kubeconfig\n# Visit https://k8s.company.com \u2192 Login \u2192 Download Config\n</code></pre></p> <p>General OIDC Flow:</p> <ol> <li>Run the command or first <code>kubectl</code> command</li> <li>Browser opens automatically</li> <li>Log in with company email + password + 2FA</li> <li>Browser shows \"Success! You can close this window\"</li> <li>Token stored in <code>~/.kube/cache/</code> or similar</li> <li>Token valid for hours/days (depends on company policy)</li> <li>When expired, you'll be prompted to login again</li> </ol> <p>What just happened?</p> <ul> <li>Your identity provider (Okta, Azure AD, Google) verified who you are</li> <li>It issued a time-limited token proving your identity</li> <li>Kubernetes cluster trusts tokens from your company's identity provider</li> <li>Every <code>kubectl</code> command includes this token - audit logs show YOUR username</li> </ul> <p>Why this is better</p> <ul> <li>Your identity = Audit trail shows your actual user account</li> <li>Centralized access control = IT manages permissions in one place (add/remove from Okta group \u2192 instant K8s access change)</li> <li>Token expiration = Automatic security boundary (lost laptop = tokens expire)</li> <li>Single sign-on = Same login for Kubernetes, AWS, internal tools</li> </ul> <p>Why it matters: Certificates work offline (no external identity provider needed) and are cryptographically secure. Common in self-hosted clusters, air-gapped environments, and industries with strict compliance requirements. You own the private key\u2014maximum control, maximum responsibility.</p> <p>Less common in 2026, but some companies issue individual X.509 client certificates for Kubernetes authentication.</p> <p>When you'd see this:</p> <ul> <li>Self-hosted Kubernetes clusters (not managed cloud services)</li> <li>Companies with strict security requirements (defense, finance, healthcare)</li> <li>Environments where OIDC integration isn't available</li> <li>OpenShift clusters (uses certificates internally)</li> </ul> <p>How certificates work:</p> <ul> <li>Your company's Kubernetes cluster has a Certificate Authority (CA)</li> <li>Platform team generates a certificate + private key signed by that CA</li> <li>Certificate contains YOUR username (in the Common Name field)</li> <li>Kubernetes cluster trusts certificates signed by its CA</li> <li>Certificates expire (typically 30-90 days) for security</li> </ul> <p>Common workflows:</p> Automated Tool (Easiest)Self-Service PortalCSR Workflow (Advanced) <p>If your platform team provides a script or command-line tool:</p> Generate certificate with automated tool<pre><code># Platform team provides a script or tool\ncompany-k8s-cert-generator --user yourname@company.com --cluster dev\n\n# This generates:\n# - yourname.crt (certificate - can be shared)\n# - yourname.key (private key - NEVER share)\n# - Updates ~/.kube/config automatically\n</code></pre> <p>This is the easiest option - everything is automated for you.</p> <ol> <li>Visit internal portal (e.g., <code>https://k8s-certs.company.com</code>)</li> <li>Log in with company credentials</li> <li>Request certificate for cluster \"dev\" or \"prod\"</li> <li>Download <code>yourname.crt</code> and <code>yourname.key</code></li> <li>Configure <code>kubectl</code>:</li> </ol> Configure kubectl to use your certificate<pre><code># Add your certificate to kubectl config\nkubectl config set-credentials yourname \\\n  --client-certificate=yourname.crt \\\n  --client-key=yourname.key \\\n  --embed-certs=true  # (1)!\n\n# Set the cluster and context (platform team provides these values)\nkubectl config set-cluster my-cluster \\\n  --server=https://k8s-api.company.com:6443 \\\n  --certificate-authority=ca.crt\n\nkubectl config set-context my-cluster \\\n  --cluster=my-cluster \\\n  --user=yourname \\\n  --namespace=default\n\nkubectl config use-context my-cluster\n</code></pre> <ol> <li><code>--embed-certs=true</code> encodes the certificate into the kubeconfig file instead of referencing the file path. Useful if you move the kubeconfig to another machine.</li> </ol> <p>Certificate Signing Request (CSR) - Advanced setup where YOU generate the private key:</p> Generate certificate with CSR (advanced)<pre><code># Generate your private key (keep this SECRET)\nopenssl genrsa -out yourname.key 2048\n\n# Generate a Certificate Signing Request\nopenssl req -new -key yourname.key -out yourname.csr \\\n  -subj \"/CN=yourname@company.com/O=dev-team\"\n\n# Submit CSR to platform team (via portal or email)\n# They sign it and return yourname.crt\n\n# Configure kubectl (same as Option 2)\n</code></pre> <p>What just happened? (applies to all options)</p> <ul> <li>You now have a cryptographic identity tied to YOUR username</li> <li>Every <code>kubectl</code> command uses your certificate to prove who you are</li> <li>Kubernetes audit logs show your certificate's Common Name (your username)</li> <li>When certificate expires, you'll get \"Unauthorized\" errors and need to request a new one</li> </ul> <p>Certificate renewal:</p> <p>Certificates expire by design. When yours expires:</p> <ol> <li>You'll see errors like: <code>Unable to connect to the server: x509: certificate has expired</code></li> <li>Request a new certificate using the same process</li> <li>Update your kubeconfig with the new certificate</li> <li>Set a calendar reminder 1 week before expiration</li> </ol> <p>Protect Your Private Key</p> <ul> <li><code>yourname.key</code> is like a password - NEVER share it</li> <li>Don't commit it to git</li> <li>Don't email it or post it in Slack</li> <li>Store it securely with permissions: <code>chmod 600 yourname.key</code></li> <li>If compromised, report to your platform team immediately</li> </ul> <p>Why this matters (and why it's problematic): Shared credentials mean everyone appears as the same user in audit logs\u2014no accountability. If one person leaves or is compromised, everyone's credentials must be regenerated. This was common in 2015, but modern alternatives (cloud IAM, OIDC) are vastly superior.</p> <p>If your platform team sent you a kubeconfig file to copy, this is an older, less secure approach. It works, but understand the risks:</p> Copy shared kubeconfig file (anti-pattern)<pre><code># Create kubectl config directory\nmkdir -p ~/.kube\n\n# Copy the file you received\ncp /path/to/received-config ~/.kube/config\n\n# Secure it (important!)\nchmod 600 ~/.kube/config\n</code></pre> <p>Why this is problematic</p> <ul> <li>Shared identity = Everyone appears as the same user in audit logs</li> <li>No accountability = Can't tell who did what</li> <li>Revocation nightmare = One person leaves? Regenerate and redistribute to everyone</li> <li>Credential sprawl = File gets copied to laptops, home directories, accidentally committed to git</li> <li>No expiration = Static credentials don't rotate automatically</li> </ul> <p>If your company uses this method, consider asking your platform team about migrating to cloud IAM or OIDC. It's worth the effort for security and operability.</p> <p>If you must use this method:</p> <ul> <li>Treat the file like a password</li> <li>Never commit it to git (add <code>~/.kube/config</code> to <code>.gitignore</code>)</li> <li>Never share it in Slack or public forums</li> <li>Request access to the cluster with YOUR identity instead</li> </ul>"},{"location":"day_one/kubectl/access/#credential-lifecycle-what-happens-over-time","title":"Credential Lifecycle: What Happens Over Time?","text":"<p>Understanding when your credentials expire and what to do about it:</p> <p>Quick Summary</p> <ul> <li>Cloud providers (AWS/GCP/Azure): Auto-refresh every 15-60 minutes - you rarely notice</li> <li>OIDC/SSO: Tokens last hours/days - you'll need to re-login occasionally</li> <li>Certificates: Last weeks/months - set a calendar reminder to renew</li> <li>Shared configs: Often permanent (but shouldn't be used!)</li> </ul> Auth Method Typical Lifespan What Happens When Expired How to Refresh AWS EKS 15 minutes (token auto-refreshes) Transparent - <code>kubectl</code> automatically calls <code>aws</code> CLI to get a fresh token Usually automatic. If you get errors, re-login to AWS: <code>aws sso login</code> or <code>aws configure</code> GCP GKE 1 hour (token auto-refreshes) Transparent - <code>kubectl</code> automatically calls <code>gcloud</code> to get a fresh token Usually automatic. If you get errors, re-login: <code>gcloud auth login</code> Azure AKS 1 hour (token auto-refreshes) Transparent - <code>kubectl</code> automatically calls <code>az</code> CLI to get a fresh token Usually automatic. If you get errors, re-login: <code>az login</code> OIDC/SSO 1 hour - 24 hours (varies by company policy) <code>kubectl</code> commands fail with \"Unauthorized\" error Re-run the login command (e.g., <code>tsh kube login</code>, browser login again) Certificates 30-90 days (no auto-refresh) <code>kubectl</code> commands fail with \"certificate has expired\" error Request new certificate from platform team or self-service portal Shared config Often permanent (security risk!) Usually doesn't expire unless manually rotated Platform team distributes new config file What does 'Auto-Refresh' mean? <p>For cloud provider auth (AWS, GCP, Azure):</p> <ul> <li>Your kubeconfig contains a command like <code>aws eks get-token</code></li> <li>Every <code>kubectl</code> command runs this command to get a fresh token</li> <li>As long as your cloud CLI is logged in, <code>kubectl</code> works</li> <li>You rarely think about expiration - it just works</li> </ul> <p>For OIDC/SSO:</p> <ul> <li>Token stored in <code>~/.kube/cache/</code> or similar</li> <li><code>kubectl</code> uses cached token until it expires</li> <li>When expired, you're prompted to login again (browser opens)</li> <li>You'll know when it expires - you'll have to login again</li> </ul> <p>For certificates:</p> <ul> <li>Certificate file has an expiration date baked in</li> <li>No automatic refresh mechanism</li> <li>Set a calendar reminder to request a new cert before expiration</li> <li>Some companies send email reminders</li> </ul>"},{"location":"day_one/kubectl/access/#common-scenarios","title":"Common Scenarios","text":"I closed my laptop for the weekend - will <code>kubectl</code> still work? <p>Cloud provider auth (AWS/GCP/Azure): Probably yes, as long as your cloud CLI session is still valid. Cloud sessions typically last days/weeks.</p> <p>OIDC/SSO: Depends on your company's token lifetime. If tokens last 1 hour, no. If they last 12 hours, maybe. Just re-login when needed.</p> <p>Certificates: Yes, until the certificate expiration date (weeks/months away).</p> I got 'Unauthorized' or 'certificate has expired' errors <p>First, check which auth method you're using: Check your authentication method<pre><code>kubectl config view --minify | grep -E 'user:|exec:|client-certificate'\n</code></pre></p> <p>If you see <code>exec:</code> (cloud provider): - Check your cloud CLI login: <code>aws sts get-caller-identity</code>, <code>gcloud auth list</code>, or <code>az account show</code> - Re-login if needed: <code>aws sso login</code>, <code>gcloud auth login</code>, or <code>az login</code></p> <p>If you see OIDC-related fields: - Re-run your company's login command or let <code>kubectl</code> prompt you to login</p> <p>If you see <code>client-certificate:</code>: - Your certificate expired - request a new one from your platform team</p> Do I need to re-run the setup command every day? <p>No! Setup commands like <code>aws eks update-kubeconfig</code> are one-time (or very infrequent). They generate the kubeconfig file. After that:</p> <ul> <li>Cloud provider: Just keep your cloud CLI logged in</li> <li>OIDC: Re-login when tokens expire (hours/days)</li> <li>Certificates: Only when certs expire (weeks/months)</li> </ul> <p>You DON'T re-run <code>aws eks update-kubeconfig</code> daily - that's only needed if the cluster configuration changes or you're setting up a new machine.</p>"},{"location":"day_one/kubectl/access/#understanding-contexts","title":"Understanding Contexts","text":"<p>A context is a saved combination of:</p> <ul> <li>Cluster - which Kubernetes cluster to talk to</li> <li>User - which credentials to use</li> <li>Namespace - your default workspace in that cluster</li> </ul> <p>Think of contexts as \"saved profiles\" - like browser profiles or AWS CLI profiles.</p>"},{"location":"day_one/kubectl/access/#why-multiple-contexts","title":"Why Multiple Contexts?","text":"<p>You'll accumulate contexts over time:</p> <ul> <li>Multiple environments: dev, staging, prod clusters</li> <li>Multiple teams: frontend-dev, backend-dev, data-platform</li> <li>Multiple projects: project-a-dev, project-b-dev</li> <li>Multiple companies: if you're a consultant working with several clients</li> </ul> <p>Real-world example: Example: Multiple contexts for different environments<pre><code>kubectl config get-contexts\n# CURRENT   NAME                    CLUSTER         AUTHINFO       NAMESPACE\n# *         company-dev             eks-us-west-2   aws-iam        my-team-dev\n#           company-staging         eks-us-west-2   aws-iam        my-team-staging\n#           company-prod            eks-us-east-1   aws-iam        my-team-prod\n#           side-project            gke-cluster     gcp-iam        default\n</code></pre></p> <p>The <code>*</code> shows your current context - where <code>kubectl</code> commands will run.</p>"},{"location":"day_one/kubectl/access/#switching-contexts","title":"Switching Contexts","text":"<p>View available contexts: List all available contexts<pre><code>kubectl config get-contexts\n</code></pre></p> <p>Switch to a different context: Switch to a different context<pre><code>kubectl config use-context company-staging\n# Switched to context \"company-staging\"\n\n# Verify you're in the right place\nkubectl config current-context\n# company-staging\n</code></pre></p> <p>Pro tip: Use <code>kubectx</code> tool for faster switching: Install and use kubectx for easier context switching<pre><code># Install kubectx\nbrew install kubectx  # macOS\n# or download from https://github.com/ahmetb/kubectx\n\n# List contexts\nkubectx\n\n# Switch with shorter command\nkubectx company-staging\n\n# Switch back to previous context\nkubectx -\n</code></pre></p>"},{"location":"day_one/kubectl/access/#contexts-vs-authentication","title":"Contexts vs Authentication","text":"<p>Context just stores the \"what\" and \"who\": Context configuration in kubeconfig<pre><code>contexts:\n- name: company-dev\n  context:\n    cluster: eks-us-west-2    # What cluster\n    user: aws-iam              # Which auth method/user\n    namespace: my-team-dev     # Default namespace\n</code></pre></p> <p>The \"user\" references your auth method: User authentication configuration in kubeconfig<pre><code>users:\n- name: aws-iam\n  user:\n    exec:                      # Cloud provider auth\n      command: aws\n      args:\n        - eks\n        - get-token\n        - --cluster-name\n        - my-cluster\n</code></pre></p> <p>When you switch contexts, you're changing:</p> <ol> <li>Which cluster API server to connect to</li> <li>Which auth credentials to use (may be same user, different cluster)</li> <li>Which namespace is your default</li> </ol> <p>You DON'T need to re-authenticate when switching contexts (unless auth has expired).</p>"},{"location":"day_one/kubectl/access/#context-safety","title":"Context Safety","text":"Be VERY careful with prod contexts <p>Accidents happen when you think you're in dev but you're actually in prod.</p> <p>Safety tips:</p> <ol> <li> <p>Check before destructive commands: Always verify your context before deleting<pre><code>kubectl config current-context  # Always check first!\nkubectl delete deployment my-app  # Then run the command\n</code></pre></p> </li> <li> <p>Name your contexts clearly:</p> </li> <li>Good: <code>company-dev</code>, <code>company-PROD</code></li> <li> <p>Bad: <code>context1</code>, <code>k8s</code></p> </li> <li> <p>Use namespace in context name:</p> </li> <li>Good: <code>eks-dev-team-frontend</code>, <code>eks-prod-team-frontend</code></li> <li> <p>Makes it obvious at a glance</p> </li> <li> <p>Set up shell prompt to show current context:    Show Kubernetes context in shell prompt<pre><code># Add to ~/.bashrc or ~/.zshrc\n# Shows context in your terminal prompt\n# Many tutorials available for \"kubernetes context in prompt\"\n</code></pre></p> </li> <li> <p>Consider separate kubeconfig files for prod: Use separate kubeconfig files for prod safety (Linux/Mac)<pre><code>export KUBECONFIG=~/.kube/config-dev  # Dev and staging\nexport KUBECONFIG=~/.kube/config-prod # Prod only (separate terminal)\n</code></pre></p> </li> </ol> Use separate kubeconfig files for prod safety (Windows PowerShell)<pre><code>$env:KUBECONFIG=\"$env:USERPROFILE\\.kube\\config-dev\"  # Dev and staging\n$env:KUBECONFIG=\"$env:USERPROFILE\\.kube\\config-prod\" # Prod only (separate window)\n</code></pre>"},{"location":"day_one/kubectl/access/#verifying-your-connection","title":"Verifying Your Connection","text":""},{"location":"day_one/kubectl/access/#step-1-check-cluster-connection","title":"Step 1: Check cluster connection","text":"Verify cluster connection<pre><code>kubectl cluster-info\n# Should show: Kubernetes control plane is running at https://...\n</code></pre>"},{"location":"day_one/kubectl/access/#step-2-check-your-namespace","title":"Step 2: Check your namespace","text":"Check your assigned namespace<pre><code>kubectl config view --minify | grep namespace  # (1)!\n# Should show your assigned namespace\n</code></pre> <ol> <li><code>grep namespace</code> filters the output to show only lines containing \"namespace\" - makes it easier to spot your assigned namespace in the config</li> </ol> <p>What's <code>--minify</code>?</p> <p>The <code>--minify</code> flag shows only your current context's configuration (cluster, user, namespace). Without it, <code>kubectl config view</code> dumps your entire kubeconfig with all clusters and contexts, which can be overwhelming. Use <code>--minify</code> when troubleshooting to see just what's active right now.</p> <p>If empty, your platform team should tell you what namespace to use.</p> <p>Set your namespace:</p> Set your default namespace<pre><code>kubectl config set-context --current --namespace=your-namespace-here\n</code></pre>"},{"location":"day_one/kubectl/access/#step-3-try-listing-pods","title":"Step 3: Try listing pods","text":"Test namespace access<pre><code>kubectl get pods\n# Might show \"No resources found\" - that's OK! It means you're connected.\n</code></pre> <p>If you see an error like \"Unauthorized\" or \"Forbidden\", contact your platform team.</p>"},{"location":"day_one/kubectl/access/#common-connection-problems","title":"Common Connection Problems","text":"Error: The connection to the server was refused <p>Problem: <code>kubectl</code> can't reach the cluster.</p> <p>Possible causes: - VPN required but not connected - Wrong cluster address in kubeconfig - Firewall blocking connection</p> <p>Try:</p> <ol> <li>Connect to VPN if your company requires it</li> <li>Ask platform team if cluster is accessible from your network</li> <li>Verify cluster address: <code>kubectl config view</code></li> </ol> Error: Forbidden / Unauthorized <p>Problem: Authentication failed.</p> <p>Possible causes: - Cloud CLI session expired (AWS, GCP, Azure) - OIDC token expired (need to re-login) - Access not granted yet to cluster or namespace - Certificate expired</p> <p>Try:</p> <ol> <li>If using cloud provider auth:<ul> <li>AWS: <code>aws sts get-caller-identity</code> (verify you're logged in)</li> <li>GCP: <code>gcloud auth list</code> (verify active account)</li> <li>Azure: <code>az account show</code> (verify subscription)</li> </ul> </li> <li>If using OIDC: Re-run the login command (token likely expired)</li> <li>Ask platform team if your access is active and which namespace you're assigned</li> <li>Check config: <code>kubectl config view --minify</code> to see current user</li> </ol> Error: No resources found in namespace <p>Problem: Actually not a problem! This means you're connected but haven't deployed anything yet.</p> <p>This is success. Move to the next article.</p>"},{"location":"day_one/kubectl/access/#understanding-your-access","title":"Understanding Your Access","text":""},{"location":"day_one/kubectl/access/#what-you-can-do","title":"What You Can Do","text":"<p>In YOUR namespace, you can typically:</p> <ul> <li>\u2705 Create pods, deployments, services</li> <li>\u2705 View logs</li> <li>\u2705 Delete resources you created</li> <li>\u2705 Scale deployments</li> </ul>"},{"location":"day_one/kubectl/access/#what-you-cant-do","title":"What You Can't Do","text":"<ul> <li>\u274c Access other teams' namespaces</li> <li>\u274c Modify cluster-wide resources</li> <li>\u274c Access production (unless explicitly granted)</li> <li>\u274c Create namespaces</li> </ul> <p>This is good! It means you can experiment safely without breaking anything outside your namespace.</p>"},{"location":"day_one/kubectl/access/#the-kubectl-cheat-sheet","title":"The kubectl Cheat Sheet","text":"<p>Essential commands you'll use constantly:</p> <p>\u2705 Safe (Read-Only) - Use these freely to explore:</p> View resources (safe to run)<pre><code># See what's running\nkubectl get pods\nkubectl get deployments\nkubectl get services\n\n# Get detailed info about a resource\nkubectl describe pod &lt;pod-name&gt;\n\n# View application logs\nkubectl logs &lt;pod-name&gt;\n\n# Check which context you're in\nkubectl config current-context\n</code></pre> <p>\u26a0\ufe0f Caution (Modifies Resources) - Double-check before running:</p> Modify resources (check your context first!)<pre><code># Apply configuration from a file\nkubectl apply -f my-app.yaml\n\n# Scale a deployment\nkubectl scale deployment my-app --replicas=3\n\n# Execute commands inside a pod\nkubectl exec -it &lt;pod-name&gt; -- /bin/bash\n</code></pre> <p>\ud83d\udea8 DANGER (Destructive) - Always verify context and namespace:</p> Delete resources (DANGEROUS - cannot be undone)<pre><code># Delete a specific resource\nkubectl delete pod &lt;pod-name&gt;\n\n# Delete everything matching a label\nkubectl delete deployment my-app\n\n# NEVER run these without explicit instruction:\n# kubectl delete --all\n# kubectl delete namespace &lt;name&gt;\n</code></pre> <p>We'll cover all of these in detail in the next article about essential <code>kubectl</code> commands.</p>"},{"location":"day_one/kubectl/access/#practice-exercise","title":"Practice Exercise","text":"Exercise: Verify Your Setup <p>Complete these steps to verify you're ready:</p> <ol> <li>Install <code>kubectl</code> for your operating system (see tabs above)</li> <li>Follow the authentication method your company uses (Cloud Provider, OIDC, or Certificate tabs above)</li> <li>Run <code>kubectl cluster-info</code> successfully</li> <li>Run <code>kubectl get pods</code> (even if it says \"No resources found\")</li> <li>Check your current namespace</li> </ol> Solution Verify your kubectl setup<pre><code># 1. Check kubectl is installed\nkubectl version --client\n\n# 2. Verify kubeconfig exists\nls ~/.kube/config  # Linux/Mac\n# Windows (PowerShell): Test-Path \"$env:USERPROFILE\\.kube\\config\"\n\n# 3. Test cluster connection\nkubectl cluster-info\n\n# 4. Test namespace access\nkubectl get pods\n\n# 5. See current context and namespace\nkubectl config get-contexts\nkubectl config view --minify | grep namespace\n</code></pre> <p>Success looks like: - <code>kubectl cluster-info</code> shows cluster address - <code>kubectl get pods</code> returns (even if empty) - No \"Unauthorized\" or \"Forbidden\" errors</p>"},{"location":"day_one/kubectl/access/#quick-recap","title":"Quick Recap","text":"Task Command Install kubectl <code>brew install kubectl</code> (macOS) Verify connection <code>kubectl cluster-info</code> List pods <code>kubectl get pods</code> Switch context <code>kubectl config use-context &lt;name&gt;</code> Set namespace <code>kubectl config set-context --current --namespace=&lt;ns&gt;</code>"},{"location":"day_one/kubectl/access/#further-reading","title":"Further Reading","text":""},{"location":"day_one/kubectl/access/#official-documentation","title":"Official Documentation","text":"<ul> <li>Install kubectl - Installation for all platforms</li> <li>Configure kubectl - kubeconfig setup</li> <li>kubectl Cheat Sheet - Command reference</li> </ul>"},{"location":"day_one/kubectl/access/#authentication-methods","title":"Authentication Methods","text":"<ul> <li>Kubernetes Authentication - Official authentication overview</li> <li>AWS EKS Authentication - IAM integration with EKS</li> <li>GKE <code>kubectl</code> Access - Configuring <code>kubectl</code> for GKE clusters</li> <li>AKS Authentication - Azure AD integration</li> </ul>"},{"location":"day_one/kubectl/access/#deep-dives","title":"Deep Dives","text":"<ul> <li>Understanding kubeconfig - Configuration file format</li> <li>kubectl Context and Configuration - Managing contexts</li> </ul>"},{"location":"day_one/kubectl/access/#whats-next","title":"What's Next?","text":"<p>You're connected! <code>kubectl</code> is working, and you can access your namespace. Ready to deploy something?</p> <p>Next: Your First Deployment - Deploy a simple web application and see it run in Kubernetes.</p> <p>Don't worry if <code>kubectl</code> feels foreign. After deploying a few applications, these commands will become second nature.</p>"},{"location":"day_one/kubectl/commands/","title":"Essential kubectl Commands","text":"<p>Part of Day One: Getting Started</p> <p>This is the fourth article in Day One: Getting Started. If you haven't deployed anything yet, do Your First Deployment first.</p> <p>You just deployed your first application to Kubernetes. It worked! But you also noticed something: you ran <code>kubectl apply</code>, then <code>kubectl get pods</code>, then <code>kubectl describe</code>, then <code>kubectl logs</code>... and you're starting to wonder: how many of these commands do I actually need to know?</p> <p>Good news: despite hundreds of possible <code>kubectl</code> commands and flags, you'll use the same 10-15 commands about 90% of the time. Master these, and you'll be productive with Kubernetes.</p> <p>This guide organizes the essential commands by what you're trying to do, shows you exactly what output to expect, and gives you the confidence to explore safely.</p> <p>What You'll Learn</p> <p>By the end of this article, you'll know:</p> <ul> <li>The daily commands you'll run dozens of times (read-only and safe)</li> <li>The deployment commands that modify resources (use with awareness)</li> <li>The troubleshooting commands that save you when things break</li> <li>How to combine commands with useful flags</li> <li>Which commands are safe vs. destructive</li> </ul>"},{"location":"day_one/kubectl/commands/#the-daily-commands","title":"The Daily Commands","text":"<p>Start with read-only commands. They're safe, informative, and help you understand what's actually running in your cluster before you make any changes.</p> <pre><code>graph TD\n    Start[Read-Only Commands&lt;br/&gt;\u2705 Safe to explore] --&gt; Get[kubectl get&lt;br/&gt;List resources]\n    Start --&gt; Describe[kubectl describe&lt;br/&gt;Detailed info + events]\n    Start --&gt; Logs[kubectl logs&lt;br/&gt;Application output]\n    Start --&gt; Explain[kubectl explain&lt;br/&gt;Documentation]\n\n    style Start fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Get fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Describe fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Logs fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Explain fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre> <p>These are the commands you'll run dozens of times per day. They're all equally important\u2014use whichever one fits your current need:</p> kubectl getkubectl describekubectl logskubectl explain Pro Tips - Click to Expand <p>Create shell aliases for commands you run constantly:</p> bash/zsh (Linux/Mac)PowerShell (Windows) Add to ~/.bashrc or ~/.zshrc<pre><code>alias k=kubectl\nalias kgp='kubectl get pods'\nalias kgd='kubectl get deployments'\nalias kgs='kubectl get svc'\nalias kga='kubectl get all'\nalias kd='kubectl describe'\nalias kl='kubectl logs'\nalias kaf='kubectl apply -f'\nalias kdel='kubectl delete'\n</code></pre> <p>After adding these, run <code>source ~/.bashrc</code> (or <code>source ~/.zshrc</code>), then you can use <code>k get pods</code> instead of <code>kubectl get pods</code>.</p> Add to your PowerShell profile<pre><code># Find your profile location:\n# $PROFILE\n\n# Edit your profile (creates if missing):\n# notepad $PROFILE\n\n# Add these functions:\nfunction k { kubectl $args }\nfunction kgp { kubectl get pods $args }\nfunction kgd { kubectl get deployments $args }\nfunction kgs { kubectl get svc $args }\nfunction kga { kubectl get all $args }\nfunction kd { kubectl describe $args }\nfunction kl { kubectl logs $args }\nfunction kaf { kubectl apply -f $args }\nfunction kdel { kubectl delete $args }\n</code></pre> <p>After adding these, restart PowerShell, then you can use <code>k get pods</code> instead of <code>kubectl get pods</code>.</p> <p>Reload Without Restarting</p> <p>After editing your profile, reload it: <pre><code>. $PROFILE\n</code></pre></p> <p>Enable kubectl autocomplete for tab completion of commands and resource names:</p> bashzshPowerShell Add to ~/.bashrc<pre><code># Enable kubectl autocompletion\nsource &lt;(kubectl completion bash)\n\n# If using alias 'k', add completion for it:\ncomplete -o default -F __start_kubectl k\n</code></pre> <p>After adding, run <code>source ~/.bashrc</code> or restart your terminal.</p> Add to ~/.zshrc<pre><code># Enable kubectl autocompletion\nsource &lt;(kubectl completion zsh)\n\n# If using alias 'k', add completion for it:\ncompdef __start_kubectl k\n</code></pre> <p>After adding, run <code>source ~/.zshrc</code> or restart your terminal.</p> Add to your PowerShell profile<pre><code># Enable kubectl autocompletion\nkubectl completion powershell | Out-String | Invoke-Expression\n\n# Optional: Set alias 'k' to kubectl\nSet-Alias -Name k -Value kubectl\n</code></pre> <p>After adding, restart PowerShell or run <code>. $PROFILE</code>.</p> <p>With autocomplete enabled: Press <code>Tab</code> while typing commands:</p> <ul> <li><code>kubectl get po</code> + <code>Tab</code> \u2192 <code>kubectl get pods</code></li> <li><code>kubectl describe pod my-app-</code> + <code>Tab</code> \u2192 shows matching pod names</li> <li><code>kubectl -n</code> + <code>Tab</code> \u2192 shows available namespaces</li> </ul> <p>Set your default namespace so you don't need <code>-n namespace</code> every time:</p> Set Default Namespace (Works in all shells)<pre><code>kubectl config set-context --current --namespace=your-namespace\n# Now all commands use this namespace by default\n</code></pre> <p>Need to switch between multiple contexts?</p> <p>If you work with multiple clusters (dev, staging, prod), you'll use <code>kubectl config use-context</code> to switch between them. See Getting kubectl Access: Switching Contexts for the full workflow.</p>"},{"location":"day_one/kubectl/commands/#get-viewing-resources","title":"Get (Viewing Resources)","text":"<p>The <code>kubectl get</code> command lists resources. This is usually your first step when checking on deployments, debugging issues, or just seeing what's running.</p> List Pods<pre><code>kubectl get pods\n# NAME                        READY   STATUS    RESTARTS   AGE\n# my-app-7c5ddbdf54-2xkqn     1/1     Running   0          5m\n# my-app-7c5ddbdf54-8mz4p     1/1     Running   0          5m\n</code></pre> List Deployments<pre><code>kubectl get deployments\nkubectl get deploy  # Short form\n# NAME      READY   UP-TO-DATE   AVAILABLE   AGE\n# my-app    3/3     3            3           10m\n</code></pre> List Services<pre><code>kubectl get services\nkubectl get svc  # Short form\n# NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\n# my-app-svc   ClusterIP   10.96.45.123    &lt;none&gt;        80/TCP    10m\n</code></pre> List Everything in Namespace<pre><code>kubectl get all\n# Shows pods, services, deployments, replicasets\n</code></pre> Get More Details<pre><code>kubectl get pods -o wide\n# NAME                        READY   STATUS    RESTARTS   AGE   IP           NODE\n# my-app-7c5ddbdf54-2xkqn     1/1     Running   0          5m    10.244.1.5   worker-1\n</code></pre> <p>\u2705 Safe: Read-only, can't break anything</p>"},{"location":"day_one/kubectl/commands/#describe-detailed-info","title":"Describe (Detailed Info)","text":"<p>When <code>kubectl get</code> shows something's wrong, <code>kubectl describe</code> tells you why. It shows events, status details, and error messages\u2014your primary debugging tool.</p> Describe a Pod<pre><code>kubectl describe pod my-app-7c5ddbdf54-2xkqn\n# Name:             my-app-7c5ddbdf54-2xkqn\n# Namespace:        default\n# Node:             worker-1/192.168.1.10\n# Status:           Running\n# IP:               10.244.1.5\n# Containers:\n#   my-app:\n#     Image:          nginx:1.21\n#     Port:           80/TCP\n#     State:          Running\n# Events:\n#   Type    Reason     Age   Message\n#   ----    ------     ----  -------\n#   Normal  Scheduled  5m    Successfully assigned pod to worker-1\n#   Normal  Pulled     5m    Container image pulled\n#   Normal  Created    5m    Created container\n#   Normal  Started    5m    Started container\n</code></pre> <p>Pro tip: Always check the Events section at the bottom\u2014that's where error messages appear.</p> <p>\u2705 Safe: Read-only</p>"},{"location":"day_one/kubectl/commands/#logs-application-output","title":"Logs (Application Output)","text":"<p>The <code>kubectl logs</code> command shows your application's stdout/stderr\u2014exactly what you'd see if you ran the container locally with <code>docker logs</code>.</p> View Pod Logs<pre><code>kubectl logs my-app-7c5ddbdf54-2xkqn\n# 192.168.1.100 - - [12/Feb/2026:14:23:11 +0000] \"GET / HTTP/1.1\" 200\n# 192.168.1.101 - - [12/Feb/2026:14:23:15 +0000] \"GET /health HTTP/1.1\" 200\n</code></pre> Follow Logs in Real-Time<pre><code>kubectl logs -f my-app-7c5ddbdf54-2xkqn\n# Like tail -f - keeps running and shows new log lines as they appear\n# Press Ctrl+C to stop\n</code></pre> Last N Lines<pre><code>kubectl logs --tail=50 my-app-7c5ddbdf54-2xkqn\n# Shows only the last 50 lines\n</code></pre> Multi-Container Pod<pre><code>kubectl logs my-app-7c5ddbdf54-2xkqn -c nginx\n# When a pod has multiple containers, specify which one\n</code></pre> <p>\u2705 Safe: Read-only</p>"},{"location":"day_one/kubectl/commands/#explain-documentation","title":"Explain (Documentation)","text":"<p>The <code>kubectl explain</code> command is built-in documentation for every Kubernetes resource. No need to leave your terminal to look up YAML fields.</p> Explain a Resource Type<pre><code>kubectl explain pods\n# KIND:     Pod\n# VERSION:  v1\n# DESCRIPTION:\n#      Pod is a collection of containers that can run on a host...\n</code></pre> Drill Down into Fields<pre><code>kubectl explain pods.spec\n# Shows all fields under spec\n\nkubectl explain pods.spec.containers\n# Shows container-specific fields\n\nkubectl explain pods.spec.containers.resources\n# Shows resource request/limit fields\n</code></pre> Explain Other Resources<pre><code>kubectl explain deployments\nkubectl explain services\nkubectl explain configmaps\n# Works for ANY Kubernetes resource\n</code></pre> <p>Pro tip: When writing YAML and you forget a field name, use <code>kubectl explain</code> instead of googling.</p> <p>\u2705 Safe: Just shows documentation\u2014doesn't interact with the cluster</p>"},{"location":"day_one/kubectl/commands/#deployment-commands","title":"Deployment Commands","text":"<p>Now we move from read-only to commands that modify your cluster. These are still safe in your dev namespace, but use them with awareness.</p> <pre><code>graph TD\n    Start[Need to change&lt;br/&gt;resources?] --&gt; Apply[kubectl apply&lt;br/&gt;Create/Update]\n    Apply --&gt; Check{Is it working?}\n\n    Check --&gt;|Yes| Scale[kubectl scale&lt;br/&gt;Adjust replicas]\n    Check --&gt;|No| Delete[kubectl delete&lt;br/&gt;Remove resources]\n\n    Scale --&gt; Done[Changes applied&lt;br/&gt;\u26a0\ufe0f Namespace modified]\n    Delete --&gt; Done\n\n    style Start fill:#1a202c,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Apply fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Check fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Scale fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Delete fill:#c53030,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Done fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre> kubectl applykubectl deletekubectl scale"},{"location":"day_one/kubectl/commands/#apply-createupdate-resources","title":"Apply (Create/Update Resources)","text":"<p>The <code>kubectl apply</code> command is how you deploy applications. It reads YAML files and creates or updates resources to match what's in the file.</p> Apply a Single File<pre><code>kubectl apply -f deployment.yaml\n# deployment.apps/my-app created\n</code></pre> <p>Example YAML that <code>kubectl apply</code> reads:</p> deployment.yaml<pre><code>apiVersion: apps/v1  # (1)!\nkind: Deployment  # (2)!\nmetadata:\n  name: my-app  # (3)!\nspec:\n  replicas: 3  # (4)!\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app  # (5)!\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21  # (6)!\n        ports:\n        - containerPort: 80\n</code></pre> <ol> <li>API version tells Kubernetes which resource schema to use</li> <li>Deployment manages ReplicaSets, which manage Pods</li> <li>Name must be unique within the namespace</li> <li>Kubernetes maintains exactly 3 running pods</li> <li>Labels connect Deployments \u2192 Pods \u2192 Services</li> <li>Always pin versions - avoid <code>:latest</code> in production</li> </ol> Apply All Files in Directory<pre><code>kubectl apply -f ./configs/\n# deployment.apps/my-app created\n# service/my-app-svc created\n# configmap/my-app-config created\n</code></pre> Apply from URL<pre><code>kubectl apply -f https://k8s.io/examples/application/deployment.yaml\n# Useful for examples and quick tests\n</code></pre> <p>How it works: <code>kubectl apply</code> is declarative\u2014it figures out what needs to be created, updated, or left alone. Run it multiple times safely; it only makes necessary changes.</p> <p>\u26a0\ufe0f Caution: Creates or modifies resources in your namespace - can trigger Pod restarts if changing images or resource limits</p>"},{"location":"day_one/kubectl/commands/#delete-remove-resources","title":"Delete (Remove Resources)","text":"<p>The <code>kubectl delete</code> command removes resources. Use carefully\u2014there's no \"Are you sure?\" prompt.</p> Delete by File<pre><code>kubectl delete -f deployment.yaml\n# deployment.apps \"my-app\" deleted\n# Removes everything defined in that YAML file\n</code></pre> Delete by Resource Name<pre><code>kubectl delete pod my-app-7c5ddbdf54-2xkqn\n# pod \"my-app-7c5ddbdf54-2xkqn\" deleted\n\nkubectl delete deployment my-app\n# deployment.apps \"my-app\" deleted\n# Also deletes the ReplicaSet and all pods it manages\n</code></pre> Delete by Label<pre><code>kubectl delete all -l app=myapp\n# Deletes all resources with label app=myapp\n# CAREFUL: 'all' means pods, services, deployments, etc.\n</code></pre> <p>Namespace Awareness</p> <p><code>kubectl delete</code> only affects your current namespace. You can't accidentally delete production resources if you're in the dev namespace\u2014but always check with <code>kubectl config view --minify | grep namespace</code> first.</p> <p>\ud83d\udea8 DANGER: Permanently deletes resources\u2014no confirmation prompt</p>"},{"location":"day_one/kubectl/commands/#scale-adjust-replicas","title":"Scale (Adjust Replicas)","text":"<p>\u26a0\ufe0f Caution: The <code>kubectl scale</code> command changes how many copies (replicas) of your application are running immediately. Great for testing load handling or temporarily scaling down, but affects live traffic.</p> Scale Up<pre><code>kubectl scale deployment my-app --replicas=5\n# deployment.apps/my-app scaled\n# Kubernetes creates 2 more pods (you had 3, now you'll have 5)\n</code></pre> Scale Down<pre><code>kubectl scale deployment my-app --replicas=1\n# deployment.apps/my-app scaled\n# Kubernetes terminates 2 pods (keeping 1 running)\n# \u26a0\ufe0f Active connections to terminated pods may be dropped\n</code></pre> Scale to Zero<pre><code>kubectl scale deployment my-app --replicas=0\n# Stops all pods but keeps the deployment\n# \u26a0\ufe0f Application becomes completely unavailable\n</code></pre> <p>Note: Scaling changes take a few seconds. Use <code>kubectl get pods -w</code> to watch pods starting or terminating.</p> <p>Production consideration: Scaling down can drop active connections. In production, prefer gradual scaling or use <code>kubectl rollout restart</code> for graceful pod replacement.</p>"},{"location":"day_one/kubectl/commands/#troubleshooting-commands","title":"Troubleshooting Commands","text":"<p>When things break (and they will), these commands help you investigate and fix problems.</p> <pre><code>graph TD\n    Start[Something's broken?] --&gt; Port[kubectl port-forward&lt;br/&gt;Test locally]\n    Port --&gt; Works{Does it work&lt;br/&gt;locally?}\n\n    Works --&gt;|No| Exec[kubectl exec&lt;br/&gt;Shell into pod]\n    Works --&gt;|Yes| Rollout[kubectl rollout&lt;br/&gt;Check deployment]\n\n    Exec --&gt; Debug[Debug inside&lt;br/&gt;container]\n    Rollout --&gt; Fix{Need to&lt;br/&gt;rollback?}\n\n    Fix --&gt;|Yes| Undo[kubectl rollout undo&lt;br/&gt;Revert to previous]\n    Fix --&gt;|No| Restart[kubectl rollout restart&lt;br/&gt;Recreate pods]\n\n    Debug --&gt; Solved[Problem identified&lt;br/&gt;\u26a0\ufe0f Fix and redeploy]\n    Undo --&gt; Solved\n    Restart --&gt; Solved\n\n    style Start fill:#1a202c,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Port fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Works fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Exec fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Rollout fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Debug fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Fix fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Undo fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Restart fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Solved fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre> kubectl port-forwardkubectl execkubectl rollout"},{"location":"day_one/kubectl/commands/#port-forward-local-access","title":"Port Forward (Local Access)","text":"<p>The <code>kubectl port-forward</code> command creates a tunnel from your laptop to a pod or service in the cluster. Perfect for testing before exposing services publicly.</p> Forward Pod Port to localhost<pre><code>kubectl port-forward pod/my-app-7c5ddbdf54-2xkqn 8080:80\n# Forwarding from 127.0.0.1:8080 -&gt; 80\n# Now open http://localhost:8080 in your browser\n# Press Ctrl+C to stop forwarding\n</code></pre> Forward Service Port<pre><code>kubectl port-forward service/my-app-svc 8080:80\n# Forwards to one of the service's pods automatically\n# Better than pod forwarding (works even if pods restart)\n</code></pre> <p>When to use this:</p> <ul> <li>Testing an application before creating an Ingress</li> <li>Accessing a database that's not publicly exposed</li> <li>Debugging connectivity issues</li> </ul> <p>\u2705 Safe: Only affects your local machine\u2014doesn't change anything in the cluster</p>"},{"location":"day_one/kubectl/commands/#exec-shell-into-pod","title":"Exec (Shell Into Pod)","text":"<p>The <code>kubectl exec</code> command runs commands inside a running container\u2014like SSH but for containers. Incredibly useful for debugging when logs don't tell you enough.</p> Major Security Risk - Click to Expand <p>Many enterprise organizations disable <code>kubectl exec</code> entirely. Here's why:</p> <p>You're inside the cluster network: Once you <code>exec</code> into a pod, you have network access to:</p> <ul> <li>Internal services (databases, APIs, message queues)</li> <li>Other pods in the same namespace</li> <li>Potentially other namespaces (depending on network policies)</li> <li>Secrets mounted as environment variables or files</li> </ul> <p>An attacker with <code>exec</code> access can:</p> <ul> <li>Exfiltrate sensitive data from databases</li> <li>Access internal APIs that should be isolated</li> <li>Pivot to other systems within the cluster</li> <li>Read mounted secrets and credentials</li> </ul> <p>Enterprise reality: Many organizations restrict <code>exec</code> via RBAC policies or disable it completely in production. If you get \"Forbidden\" errors, this is intentional security policy\u2014not a bug.</p> <p>Alternatives to consider:</p> <ul> <li>Use <code>kubectl logs</code> for application debugging (safer)</li> <li>Build better observability into your app (metrics, traces)</li> <li>Use ephemeral debug containers (Kubernetes 1.23+) with time limits</li> <li>Request temporary elevated access through formal security processes</li> </ul> Run a Single Command<pre><code>kubectl exec my-app-7c5ddbdf54-2xkqn -- ls -la /app\n# total 12\n# drwxr-xr-x    2 root     root          4096 Feb 12 14:20 .\n# drwxr-xr-x    1 root     root          4096 Feb 12 14:20 ..\n# -rw-r--r--    1 root     root           612 Feb 12 14:20 index.html\n</code></pre> Interactive Shell<pre><code>kubectl exec -it my-app-7c5ddbdf54-2xkqn -- /bin/bash\n# Opens interactive shell inside the container\n# You're now \"inside\" the pod - explore, debug, test\n\n# If bash isn't available, try sh:\nkubectl exec -it my-app-7c5ddbdf54-2xkqn -- sh\n</code></pre> Multi-Container Pod<pre><code>kubectl exec -it my-app-7c5ddbdf54-2xkqn -c nginx -- sh\n# Specify container name when pod has multiple containers\n</code></pre> If Exec is Blocked<pre><code>kubectl exec my-app-7c5ddbdf54-2xkqn -- sh\n# Error from server (Forbidden): pods \"my-app-7c5ddbdf54-2xkqn\" is forbidden:\n# User \"your-username\" cannot create resource \"pods/exec\" in namespace \"dev\"\n\n# This is expected in many enterprise environments\n# Use kubectl logs instead, or request access through proper channels\n</code></pre> <pre><code>**Common debugging tasks (if you have `exec` access):**\n\n- Check if config files are mounted correctly\n- Test network connectivity (`curl`, `ping`)\n- Verify environment variables\n- Inspect file permissions\n\n\ud83d\udea8 **DANGER:**\n\n- You can modify the container's filesystem (changes lost when pod/container restarts)\n- You have network access to internal cluster services\n- Your actions are audited\u2014use responsibly and only in dev/test environments\n- Never use `exec` in production unless explicitly authorized\n</code></pre>"},{"location":"day_one/kubectl/commands/#rollout-manage-deployments","title":"Rollout (Manage Deployments)","text":"<p>The <code>kubectl rollout</code> commands manage deployment updates, history, and rollbacks. Essential when deployments go wrong.</p> Check Rollout Status<pre><code>kubectl rollout status deployment/my-app\n# Waiting for deployment \"my-app\" rollout to finish: 1 out of 3 new replicas updated...\n# deployment \"my-app\" successfully rolled out\n</code></pre> View Rollout History<pre><code>kubectl rollout history deployment/my-app\n# REVISION  CHANGE-CAUSE\n# 1         &lt;none&gt;\n# 2         kubectl apply --filename=deployment.yaml\n# 3         kubectl apply --filename=deployment.yaml\n</code></pre> Rollback to Previous Version<pre><code>kubectl rollout undo deployment/my-app\n# deployment.apps/my-app rolled back\n# Reverts to the previous version immediately\n</code></pre> Rollback to Specific Revision<pre><code>kubectl rollout undo deployment/my-app --to-revision=2\n# deployment.apps/my-app rolled back\n</code></pre> Restart Deployment<pre><code>kubectl rollout restart deployment/my-app\n# deployment.apps/my-app restarted\n# Recreates all pods (useful when config changed but image didn't)\n</code></pre> <p>\u26a0\ufe0f Caution: These commands affect your running application\u2014use <code>rollout status</code> first to understand what's happening</p>"},{"location":"day_one/kubectl/commands/#useful-flags","title":"Useful Flags","text":"<p>These flags work with most <code>kubectl</code> commands and dramatically increase their power.</p> NamespaceOutput FormatsLabelsWatch <p>Combine flags: You can use multiple flags together, like <code>kubectl get pods -n dev -l app=nginx -o wide --watch</code></p>"},{"location":"day_one/kubectl/commands/#namespace","title":"Namespace","text":"<p>By default, <code>kubectl</code> commands operate on your current namespace. Use these flags to work with other namespaces:</p> Specific Namespace<pre><code>kubectl get pods -n other-namespace\n# Shows pods in 'other-namespace' instead of current namespace\n</code></pre> All Namespaces<pre><code>kubectl get pods --all-namespaces\nkubectl get pods -A  # Short form\n# Shows pods from all namespaces (if you have permission)\n</code></pre>"},{"location":"day_one/kubectl/commands/#output-formats","title":"Output Formats","text":"<p>Get resources in different formats for parsing, saving, or detailed inspection:</p> YAML Output<pre><code>kubectl get pod my-app-7c5ddbdf54-2xkqn -o yaml\n# Shows complete pod definition in YAML\n# Great for seeing every field and current values\n</code></pre> JSON Output<pre><code>kubectl get pod my-app-7c5ddbdf54-2xkqn -o json\n# Same as YAML but in JSON format\n# Useful for scripting with jq\n</code></pre> Custom Columns<pre><code>kubectl get pods -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,IP:.status.podIP\n# NAME                        STATUS    IP\n# my-app-7c5ddbdf54-2xkqn     Running   10.244.1.5\n</code></pre>"},{"location":"day_one/kubectl/commands/#labels","title":"Labels","text":"<p>Labels are key/value pairs attached to resources\u2014the fundamental way Kubernetes connects resources together.</p> Filter by Label<pre><code>kubectl get pods -l app=nginx\n# Shows only pods with label app=nginx\n\nkubectl get pods -l app=nginx,environment=prod\n# Multiple labels (AND condition)\n</code></pre> Show Labels<pre><code>kubectl get pods --show-labels\n# NAME                        READY   STATUS    LABELS\n# my-app-7c5ddbdf54-2xkqn     1/1     Running   app=nginx,pod-template-hash=7c5ddbdf54\n</code></pre> Trace Label Matching (Troubleshooting)<pre><code># 1. Check what label your Service is looking for\nkubectl get service my-app-svc -o jsonpath='{.spec.selector}'\n# {\"app\":\"nginx\"}\n\n# 2. Check if your Pods have that label\nkubectl get pods -l app=nginx\n# If empty, labels don't match!\n\n# 3. Check Service endpoints\nkubectl get endpoints my-app-svc\n# Should show Pod IPs - if empty, labels don't match\n</code></pre> <p>Understanding Labels</p> <p>Labels are how Services find Pods, how Deployments manage Pods, and how you filter resources. Without matching labels, Services can't route traffic\u2014this is the #1 cause of \"my service returns 503\" problems.</p> <p>Visual explanation: See the label matching diagram in Your First Deployment showing how Deployment, Pods, and Service connect via labels.</p> <p>Deep dive: For the complete explanation, see Understanding What Happened: Labels - The Glue</p>"},{"location":"day_one/kubectl/commands/#watch","title":"Watch","text":"<p>Watch resources in real-time as they change:</p> Watch for Changes<pre><code>kubectl get pods --watch\nkubectl get pods -w  # Short form\n# Shows initial list, then updates as pods start/stop/change\n# Press Ctrl+C to stop watching\n</code></pre>"},{"location":"day_one/kubectl/commands/#command-cheat-sheet","title":"Command Cheat Sheet","text":"<p>Quick reference for the 10 essential commands:</p> Task Command Safety List pods <code>kubectl get pods</code> \u2705 Safe Pod details <code>kubectl describe pod &lt;name&gt;</code> \u2705 Safe Pod logs <code>kubectl logs &lt;name&gt;</code> \u2705 Safe Shell in pod <code>kubectl exec -it &lt;name&gt; -- sh</code> \u26a0\ufe0f Caution Create resources <code>kubectl apply -f file.yaml</code> \u26a0\ufe0f Caution Delete resources <code>kubectl delete -f file.yaml</code> \ud83d\udea8 Danger Scale deployment <code>kubectl scale deployment &lt;name&gt; --replicas=N</code> \u26a0\ufe0f Caution Port forward <code>kubectl port-forward pod/&lt;name&gt; 8080:80</code> \u2705 Safe Rollback <code>kubectl rollout undo deployment/&lt;name&gt;</code> \u26a0\ufe0f Caution Get docs <code>kubectl explain &lt;resource&gt;</code> \u2705 Safe"},{"location":"day_one/kubectl/commands/#real-world-workflows","title":"Real-World Workflows","text":"<p>Real-world scenarios showing how to combine commands effectively.</p> Debugging a Broken PodDeploying an UpdateChecking Application Health"},{"location":"day_one/kubectl/commands/#debugging-a-broken-pod","title":"Debugging a Broken Pod","text":"<p>When you see a pod with status <code>CrashLoopBackOff</code>, <code>Error</code>, or <code>ImagePullBackOff</code>:</p> 1. Identify the Problem Pod<pre><code>kubectl get pods\n# NAME                        READY   STATUS             RESTARTS   AGE\n# my-app-7c5ddbdf54-2xkqn     0/1     CrashLoopBackOff   5          3m\n</code></pre> 2. Check Events<pre><code>kubectl describe pod my-app-7c5ddbdf54-2xkqn\n# Look at the Events section at the bottom\n# Common errors:\n# - \"ImagePullBackOff\" \u2192 wrong image name\n# - \"CrashLoopBackOff\" \u2192 container exits immediately\n# - \"Pending\" with events \u2192 resource constraints\n</code></pre> 3. Check Application Logs<pre><code>kubectl logs my-app-7c5ddbdf54-2xkqn\n# See what the application printed before crashing\n\n# If it restarted multiple times, check previous logs:\nkubectl logs my-app-7c5ddbdf54-2xkqn --previous\n</code></pre> 4. Shell Into Pod (if it's running)<pre><code>kubectl exec -it my-app-7c5ddbdf54-2xkqn -- sh\n# Investigate filesystem, test connections, check config\n</code></pre>"},{"location":"day_one/kubectl/commands/#deploying-an-update","title":"Deploying an Update","text":"<p>The safe way to deploy a new version:</p> 1. Edit Your YAML<pre><code># Update the image version in deployment.yaml\n# image: nginx:1.21 \u2192 image: nginx:1.22\n</code></pre> 2. Apply the Change<pre><code>kubectl apply -f deployment.yaml\n# deployment.apps/my-app configured\n</code></pre> 3. Watch the Rollout<pre><code>kubectl rollout status deployment/my-app\n# Waiting for deployment \"my-app\" rollout to finish...\n# deployment \"my-app\" successfully rolled out\n</code></pre> 4. Verify Pods are Running<pre><code>kubectl get pods\n# All pods should show STATUS: Running and READY: 1/1\n</code></pre> 5. If Something Broke, Rollback<pre><code>kubectl rollout undo deployment/my-app\n# deployment.apps/my-app rolled back\n# Instantly reverts to the previous working version\n</code></pre>"},{"location":"day_one/kubectl/commands/#checking-application-health","title":"Checking Application Health","text":"<p>Quick health check workflow:</p> 1. Check All Resources<pre><code>kubectl get all\n# See pods, services, deployments, replicasets at once\n</code></pre> 2. Verify Pods are Running<pre><code>kubectl get pods\n# All should be Running with correct READY count\n</code></pre> 3. Check Service Exists<pre><code>kubectl get svc\n# Verify your service has a ClusterIP\n</code></pre> 4. Test Connectivity<pre><code>kubectl port-forward service/my-app-svc 8080:80\n# Forwarding from 127.0.0.1:8080 -&gt; 80\n\n# Open http://localhost:8080 in your browser\n# If it works, your app is healthy\n</code></pre>"},{"location":"day_one/kubectl/commands/#real-world-pitfalls","title":"Real-World Pitfalls","text":"<p>Common pitfalls that trip up even experienced users:</p> Wrong Namespace--all Flag DangerLabel Selectorskubectl apply Risks--previous Flag"},{"location":"day_one/kubectl/commands/#the-wrong-namespace-trap","title":"The Wrong Namespace Trap","text":"<p>The most common mistake: running commands in the wrong namespace.</p> Always Check Your Current Namespace<pre><code>kubectl config view --minify | grep namespace\n# namespace: dev\n\n# Or check explicitly on each command:\nkubectl get pods -n production  # \u26a0\ufe0f Be VERY careful with production\n</code></pre> <p>You think you're in <code>dev</code>, but you're actually in <code>production</code>. This leads to:</p> <ul> <li>Deleting production pods by accident</li> <li>Deploying test code to production</li> <li>Scaling down production deployments</li> </ul> <p>Protection: Set your namespace explicitly and verify before destructive operations.</p>"},{"location":"day_one/kubectl/commands/#the-all-flag-danger","title":"The <code>--all</code> Flag Danger","text":"<p>Commands with <code>--all</code> or <code>-A</code> are powerful but dangerous:</p> Dangerous: Affects ALL Namespaces<pre><code>kubectl delete pods --all -A\n# \ud83d\udea8 DELETES EVERY POD IN EVERY NAMESPACE YOU HAVE ACCESS TO\n\nkubectl get pods -A\n# \u2705 Safe - just lists pods across all namespaces\n</code></pre> <p>Rule: <code>--all</code> with read-only commands (<code>get</code>, <code>describe</code>) is safe. <code>--all</code> with destructive commands (<code>delete</code>) is extremely dangerous.</p>"},{"location":"day_one/kubectl/commands/#the-l-label-selector-trap","title":"The <code>-l</code> Label Selector Trap","text":"<p>Label selectors are powerful but can match more than you expect:</p> What Will This Delete?<pre><code>kubectl delete all -l app=myapp\n# Deletes: pods, services, deployments, replicasets with label app=myapp\n# Does NOT delete: configmaps, secrets, persistent volumes\n# \"all\" doesn't mean \"everything\"\n</code></pre> <p>Always test with <code>get</code> first:</p> Safe Testing Pattern<pre><code># 1. See what matches\nkubectl get all -l app=myapp\n\n# 2. If it looks right, then delete\nkubectl delete all -l app=myapp\n</code></pre>"},{"location":"day_one/kubectl/commands/#the-kubectl-apply-isnt-always-safe","title":"The <code>kubectl apply</code> Isn't Always Safe","text":"<p>While <code>kubectl apply</code> is declarative and generally safe, it can still cause disruptions:</p> <ul> <li>Changing resource limits \u2192 pods restart</li> <li>Updating image tags \u2192 rolling restart</li> <li>Modifying service selectors \u2192 traffic disruption</li> </ul> <p>Best practice: Always review changes with <code>kubectl diff</code> first (if available) or understand what will change before applying.</p>"},{"location":"day_one/kubectl/commands/#the-forgotten-previous-flag","title":"The Forgotten <code>--previous</code> Flag","text":"<p>When debugging crashed pods:</p> Wrong - Shows Current Logs (Empty)<pre><code>kubectl logs crashing-pod-abc123\n# (empty output - pod just restarted)\n\nkubectl logs crashing-pod-abc123 --previous\n# Shows logs from BEFORE the crash - what you actually need\n</code></pre> <p>If a pod is crash-looping, always use <code>--previous</code> to see what happened before the restart.</p>"},{"location":"day_one/kubectl/commands/#practice-exercises","title":"Practice Exercises","text":"Exercise 1: Explore Your Deployed Application <p>You deployed an application in the previous article. Use the commands you learned to investigate what's actually running.</p> <p>Tasks:</p> <ol> <li>List all pods in your namespace</li> <li>Get detailed information about one pod</li> <li>View the logs from that pod</li> <li>List all services</li> </ol> <p>Goal: Get comfortable with read-only commands.</p> Solution 1. List All Pods<pre><code>kubectl get pods\n# NAME                        READY   STATUS    RESTARTS   AGE\n# nginx-7c5ddbdf54-x8f9p     1/1     Running   0          10m\n</code></pre> 2. Get Detailed Pod Info<pre><code>kubectl describe pod nginx-7c5ddbdf54-x8f9p\n# Check the Events section at the bottom\n# Look at Status, IP address, Node assignment\n</code></pre> 3. View Pod Logs<pre><code>kubectl logs nginx-7c5ddbdf54-x8f9p\n# Should see nginx access logs (or your app's logs)\n</code></pre> 4. List Services<pre><code>kubectl get svc\n# NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)\n# nginx-svc    ClusterIP   10.96.45.123    &lt;none&gt;        80/TCP\n</code></pre> <p>What you learned: These four commands (<code>get pods</code>, <code>describe pod</code>, <code>logs</code>, <code>get svc</code>) are your daily toolkit. They're all read-only and safe to run anytime.</p> Exercise 2: Scale Your Application <p>Practice scaling your deployment up and down.</p> <p>Tasks:</p> <ol> <li>Check current number of replicas</li> <li>Scale to 5 replicas</li> <li>Watch pods being created</li> <li>Scale back to 2 replicas</li> <li>Verify the change</li> </ol> <p>Goal: Get comfortable modifying resources safely.</p> Solution 1. Check Current Replicas<pre><code>kubectl get deployment\n# NAME     READY   UP-TO-DATE   AVAILABLE   AGE\n# nginx    3/3     3            3           15m\n# Currently has 3 replicas\n</code></pre> 2. Scale to 5 Replicas<pre><code>kubectl scale deployment nginx --replicas=5\n# deployment.apps/nginx scaled\n</code></pre> 3. Watch Pods Being Created<pre><code>kubectl get pods -w\n# Watch as new pods go from Pending \u2192 ContainerCreating \u2192 Running\n# Press Ctrl+C to stop watching\n</code></pre> 4. Scale Back to 2<pre><code>kubectl scale deployment nginx --replicas=2\n# deployment.apps/nginx scaled\n</code></pre> 5. Verify the Change<pre><code>kubectl get pods\n# Should now see only 2 pods\n# Some pods will show \"Terminating\" status briefly\n\nkubectl get deployment\n# NAME     READY   UP-TO-DATE   AVAILABLE   AGE\n# nginx    2/2     2            2           20m\n</code></pre> <p>What you learned: Scaling is instant and safe. Kubernetes handles starting/stopping pods automatically. In production, you'd set replica counts in your YAML, but <code>kubectl scale</code> is useful for testing.</p> Exercise 3: Test Connectivity with Port Forwarding <p>Access your application locally without exposing it publicly.</p> <p>Tasks:</p> <ol> <li>Forward your service's port 80 to localhost:8080</li> <li>Test it in your browser</li> <li>Try forwarding directly to a pod</li> </ol> <p>Goal: Learn how to test applications before exposing them.</p> Solution 1. Forward Service Port<pre><code>kubectl port-forward service/nginx-svc 8080:80\n# Forwarding from 127.0.0.1:8080 -&gt; 80\n# Forwarding from [::1]:8080 -&gt; 80\n# (Keeps running - don't close the terminal yet)\n</code></pre> 2. Test in Browser<pre><code># Open a new terminal (don't stop port-forward)\n# Visit http://localhost:8080\n\n# Or use curl:\ncurl http://localhost:8080\n# Should see HTML response from nginx\n</code></pre> 3. Forward to a Specific Pod<pre><code># Stop the previous port-forward (Ctrl+C)\n\n# Get a pod name:\nkubectl get pods\n# NAME                     READY   STATUS    RESTARTS   AGE\n# nginx-7c5ddbdf54-x8f9p   1/1     Running   0          25m\n\n# Forward to that specific pod:\nkubectl port-forward pod/nginx-7c5ddbdf54-x8f9p 8080:80\n# Works the same way\n</code></pre> <p>What you learned: Port forwarding is perfect for testing. Service forwarding is usually better than pod forwarding because pods can restart (changing their name), but the service stays consistent.</p> Exercise 4: Investigate a Resource Type <p>Use <code>kubectl explain</code> to learn about Kubernetes resources without leaving your terminal.</p> <p>Tasks:</p> <ol> <li>Get documentation for pods</li> <li>Drill down into pod.spec</li> <li>Find out what fields are available under containers</li> </ol> <p>Goal: Get comfortable with built-in documentation.</p> Solution 1. Explain Pods<pre><code>kubectl explain pods\n# KIND:     Pod\n# VERSION:  v1\n# DESCRIPTION:\n#      Pod is a collection of containers...\n</code></pre> 2. Drill Into pod.spec<pre><code>kubectl explain pod.spec\n# RESOURCE: spec &lt;Object&gt;\n# DESCRIPTION:\n#      Specification of the desired behavior of the pod...\n# FIELDS:\n#    containers  &lt;[]Object&gt; -required-\n#    volumes     &lt;[]Object&gt;\n#    ... (many more fields)\n</code></pre> 3. Explore Containers<pre><code>kubectl explain pod.spec.containers\n# RESOURCE: containers &lt;[]Object&gt;\n# FIELDS:\n#    image  &lt;string&gt; -required-\n#    name   &lt;string&gt; -required-\n#    ports  &lt;[]Object&gt;\n#    env    &lt;[]Object&gt;\n#    resources  &lt;Object&gt;\n#    ... (many more fields)\n</code></pre> Bonus: Check Resources Field<pre><code>kubectl explain pod.spec.containers.resources\n# Shows how to set CPU/memory requests and limits\n</code></pre> <p>What you learned: <code>kubectl explain</code> is like having Kubernetes documentation built into your terminal. Use it when writing YAML to find field names and understand what's required.</p> Challenge: Deploy, Break, and Fix <p>Advanced exercise: Deploy an application, intentionally break it, then debug and fix it.</p> <p>Scenario: You're deploying a new image version, but you typo the image name. Debug and fix it using only <code>kubectl</code> commands.</p> <p>Tasks:</p> <ol> <li>Create a deployment with an intentionally wrong image name (<code>nginx:nonexistent-tag</code>)</li> <li>Observe the failure</li> <li>Diagnose the problem using <code>kubectl describe</code> and <code>kubectl logs</code></li> <li>Fix it by updating the deployment</li> <li>Verify it's now working</li> </ol> Solution 1. Create Broken Deployment<pre><code># Create a file called broken-deployment.yaml:\ncat &gt; broken-deployment.yaml &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: broken-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: broken-app\n  template:\n    metadata:\n      labels:\n        app: broken-app\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:nonexistent-tag\n        ports:\n        - containerPort: 80\nEOF\n\nkubectl apply -f broken-deployment.yaml\n# deployment.apps/broken-app created\n</code></pre> 2. Observe the Failure<pre><code>kubectl get pods\n# NAME                          READY   STATUS             RESTARTS   AGE\n# broken-app-7c5ddbdf54-2xkqn   0/1     ImagePullBackOff   0          30s\n# STATUS shows ImagePullBackOff - image can't be pulled\n</code></pre> 3. Diagnose with describe<pre><code>kubectl describe pod broken-app-7c5ddbdf54-2xkqn\n# Events:\n#   Type     Reason     Age   Message\n#   ----     ------     ----  -------\n#   Warning  Failed     10s   Failed to pull image \"nginx:nonexistent-tag\"\n#   Warning  Failed     10s   Error: ErrImagePull\n# Clear error message: image doesn't exist\n</code></pre> 4. Fix the Deployment<pre><code># Edit broken-deployment.yaml and change:\n#   image: nginx:nonexistent-tag\n# to:\n#   image: nginx:1.21\n# Save the file, then reapply:\nkubectl apply -f broken-deployment.yaml\n# deployment.apps/broken-app configured\n</code></pre> 5. Verify It's Fixed<pre><code>kubectl get pods\n# NAME                          READY   STATUS    RESTARTS   AGE\n# broken-app-7c5ddbdf54-abc123  1/1     Running   0          20s\n# broken-app-7c5ddbdf54-def456  1/1     Running   0          18s\n\nkubectl describe deployment broken-app\n# Should show 2/2 replicas available\n</code></pre> <p>What you learned: This is real-world debugging. <code>ImagePullBackOff</code> is a common error. The Events section in <code>kubectl describe</code> always tells you what went wrong. Fixing is as simple as correcting the YAML and running <code>kubectl apply</code> again.</p>"},{"location":"day_one/kubectl/commands/#quick-recap","title":"Quick Recap","text":"<p>Top 10 commands you'll use daily:</p> Category Command Purpose Read-Only <code>kubectl get pods</code> List all pods Read-Only <code>kubectl describe pod &lt;name&gt;</code> Detailed pod info and events Read-Only <code>kubectl logs &lt;name&gt;</code> Application output Read-Only <code>kubectl explain &lt;resource&gt;</code> Built-in documentation Deployment <code>kubectl apply -f &lt;file&gt;</code> Create or update resources Deployment <code>kubectl scale deployment &lt;name&gt; --replicas=N</code> Adjust replica count Deployment <code>kubectl delete &lt;resource&gt; &lt;name&gt;</code> Remove resources Troubleshooting <code>kubectl exec -it &lt;name&gt; -- sh</code> Shell into container Troubleshooting <code>kubectl port-forward service/&lt;name&gt; 8080:80</code> Test locally Troubleshooting <code>kubectl rollout undo deployment/&lt;name&gt;</code> Rollback deployment <p>Remember: Start with read-only commands (<code>get</code>, <code>describe</code>, <code>logs</code>) to understand the state, then use deployment commands to make changes.</p>"},{"location":"day_one/kubectl/commands/#further-reading","title":"Further Reading","text":""},{"location":"day_one/kubectl/commands/#official-documentation","title":"Official Documentation","text":"<ul> <li>kubectl Cheat Sheet - Official quick reference</li> <li>kubectl Command Reference - Complete command documentation</li> <li>kubectl Usage Conventions - Best practices and patterns</li> </ul>"},{"location":"day_one/kubectl/commands/#interactive-learning","title":"Interactive Learning","text":"<ul> <li>kubectl Book - Comprehensive guide to kubectl usage</li> <li>Kube by Example - Hands-on learning paths from Red Hat</li> <li>Kubernetes Tutorials - Official interactive tutorials</li> <li>Kubernetes Examples on GitHub - Real-world YAML examples</li> </ul>"},{"location":"day_one/kubectl/commands/#related-articles","title":"Related Articles","text":"<ul> <li>Your First Deployment - How you got here</li> <li>Understanding What Happened - What's next: understand the architecture</li> </ul>"},{"location":"day_one/kubectl/commands/#whats-next","title":"What's Next?","text":"<p>You've mastered the essential commands. Now understand what actually happens when you deploy:</p> <p>Understanding What Happened - Learn about the Kubernetes architecture, how controllers work, and what all those resources (<code>Deployment</code>, <code>ReplicaSet</code>, <code>Pod</code>, <code>Service</code>) actually do behind the scenes.</p>"},{"location":"day_one/kubectl/first_deploy/","title":"Your First Deployment","text":"<p>Part of Day One: Getting Started</p> <p>This is the third article in Day One: Getting Started. Make sure you've completed Getting kubectl Access first.</p> <p>You're connected to your cluster. You have <code>kubectl</code> working. Now comes the moment of truth: deploying something.</p> <p>If you're nervous about breaking things or typing the wrong command\u2014that's completely normal. We'll start with a simple web application and walk through every step. By the end, you'll see your application running in Kubernetes.</p> <p>This is the moment it becomes real.</p> <p>What You'll Learn</p> <p>By the end of this article, you'll know how to:</p> <ul> <li> <p>Write a Deployment YAML - Define what you want running</p> </li> <li> <p>Apply it to your cluster - Use <code>kubectl apply</code> to deploy</p> </li> <li>Expose it with a Service - Make your app accessible</li> <li>Test and verify - Confirm everything works</li> <li>Scale your application - Increase/decrease replicas</li> <li>Clean up resources - Remove what you've created</li> </ul>"},{"location":"day_one/kubectl/first_deploy/#the-deployment-journey","title":"The Deployment Journey","text":"<pre><code>graph TD\n    A[You: kubectl apply] --&gt; B[Deployment Created]\n    B --&gt; C[ReplicaSet Created]\n    C --&gt; D1[Pod 1]\n    C --&gt; D2[Pod 2]\n    D1 --&gt; E1[nginx container]\n    D2 --&gt; E2[nginx container]\n\n    style A fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style B fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style C fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style D1 fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style D2 fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style E1 fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style E2 fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre>"},{"location":"day_one/kubectl/first_deploy/#your-first-deployment_1","title":"Your First Deployment","text":""},{"location":"day_one/kubectl/first_deploy/#understanding-the-yaml","title":"Understanding the YAML","text":"<p>We're deploying a simple nginx web server. Why nginx?</p> <ul> <li>Small image (downloads fast)</li> <li>Well-known and stable</li> <li>Easy to test (just open a browser)</li> <li>Same principles apply to YOUR application later</li> </ul> <p>The Deployment YAML:</p> nginx-deployment.yaml<pre><code>apiVersion: apps/v1  # (1)!\nkind: Deployment  # (2)!\nmetadata:\n  name: my-first-app  # (3)!\n  labels:\n    app: nginx\nspec:\n  replicas: 2  # (4)!\n  selector:\n    matchLabels:\n      app: nginx  # (5)!\n  template:\n    metadata:\n      labels:\n        app: nginx  # (6)!\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21  # (7)!\n        ports:\n        - containerPort: 80  # (8)!\n</code></pre> <ol> <li>API version for Deployments</li> <li>We're creating a Deployment\u2014it manages Pods for us</li> <li>Name must be unique within your namespace</li> <li>Kubernetes will maintain exactly 2 running copies</li> <li>Deployment finds its Pods using this label</li> <li>Pods get this label\u2014must match selector above</li> <li>Container image to run\u2014always pin versions (not <code>:latest</code>)</li> <li>Port the container listens on inside the Pod</li> </ol> <p>Labels Are the Glue</p> <p>Notice the <code>app: nginx</code> label appears three times in that YAML:</p> <ol> <li>Deployment metadata - Identifying the Deployment itself</li> <li>Selector matchLabels - The Deployment finds \"its\" Pods using this</li> <li>Pod template labels - The Pods get this label when created</li> </ol> <p>This label matching is how Kubernetes connects resources. Your Service will also use <code>app: nginx</code> to find these Pods. If the labels don't match, nothing works. We'll explore this in depth in Understanding What Happened.</p> <p>How label matching works:</p> <pre><code>graph TD\n    Deployment[\"&lt;b&gt;Deployment&lt;/b&gt;&lt;br/&gt;selector:&lt;br/&gt;matchLabels:&lt;br/&gt;app: nginx\"]\n\n    Deployment --&gt;|\"creates &amp; manages\"| Pod1[\"&lt;b&gt;Pod 1&lt;/b&gt;&lt;br/&gt;labels:&lt;br/&gt;app: nginx\"]\n    Deployment --&gt;|\"creates &amp; manages\"| Pod2[\"&lt;b&gt;Pod 2&lt;/b&gt;&lt;br/&gt;labels:&lt;br/&gt;app: nginx\"]\n\n    Pod1 &lt;--&gt;|\"label match:&lt;br/&gt;app: nginx\"| Service[\"&lt;b&gt;Service&lt;/b&gt;&lt;br/&gt;selector:&lt;br/&gt;app: nginx\"]\n    Pod2 &lt;--&gt;|\"label match:&lt;br/&gt;app: nginx\"| Service\n\n    Service --&gt;|\"external traffic\"| Traffic[\"Traffic\"]\n\n    style Deployment fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Pod1 fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Pod2 fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Service fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Traffic fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre> <p>The critical rule: All three must have <code>app: nginx</code> for this to work:</p> <ul> <li>Deployment's <code>selector.matchLabels</code> \u2192 finds Pods to manage</li> <li>Pod's <code>labels</code> \u2192 identifies which Deployment owns it</li> <li>Service's <code>selector</code> \u2192 finds Pods to route traffic to</li> </ul> <p>If labels don't match, nothing connects. This is the #1 beginner mistake.</p> <p>Don't memorize this. Just understand the key parts:</p> <ul> <li><code>kind: Deployment</code> - We're creating a Deployment (manages Pods for us)</li> <li><code>replicas: 2</code> - We want 2 copies running</li> <li><code>image: nginx:1.21</code> - What container to run</li> <li><code>containerPort: 80</code> - What port the app listens on</li> </ul>"},{"location":"day_one/kubectl/first_deploy/#deploying-it","title":"Deploying It","text":"<ul> <li> <p> Step 1: Create the File</p> <p>Create <code>nginx-deployment.yaml</code> with the content above. You can use any text editor (<code>nano</code>, <code>vim</code>, or your IDE).</p> <p>Why this matters: YAML files let you declare what you want. Kubernetes reads it and makes it happen.</p> </li> <li> <p> Step 2: Apply It</p> <p>\u26a0\ufe0f Caution (Modifies Resources):</p> Apply the Deployment<pre><code>kubectl apply -f nginx-deployment.yaml\n# deployment.apps/my-first-app created\n</code></pre> <p>Namespace Context</p> <p>These commands operate in your current namespace. Check with: <pre><code>kubectl config view --minify | grep namespace\n</code></pre> To specify a namespace: <code>kubectl get pods -n dev</code></p> <p>That's it. Kubernetes is now creating your Pods.</p> <p>Safe to Run in Dev</p> <p><code>kubectl apply</code> is safe in dev environments. It creates or updates resources declaratively. You won't break anything\u2014worst case, you'll need to delete and try again.</p> </li> <li> <p> Step 3: Watch It Come Up</p> <p>\u2705 Safe (Read-Only):</p> Check Pod Status<pre><code>kubectl get pods\n# NAME                            READY   STATUS              RESTARTS   AGE\n# my-first-app-7c5ddbdf54-2xkqn   0/1     ContainerCreating   0          5s\n# my-first-app-7c5ddbdf54-8mz4p   0/1     ContainerCreating   0          5s\n</code></pre> <p>Wait a few seconds and check again:</p> Verify Pods are Running<pre><code>kubectl get pods\n# NAME                            READY   STATUS    RESTARTS   AGE\n# my-first-app-7c5ddbdf54-2xkqn   1/1     Running   0          20s\n# my-first-app-7c5ddbdf54-8mz4p   1/1     Running   0          20s\n</code></pre> <p>STATUS: Running - SUCCESS!</p> </li> </ul>"},{"location":"day_one/kubectl/first_deploy/#making-it-accessible","title":"Making It Accessible","text":"<p>Your Pods are running, but you can't access them yet. They're isolated inside the cluster. We need a Service.</p> nginx-service.yaml<pre><code>apiVersion: v1  # (1)!\nkind: Service  # (2)!\nmetadata:\n  name: my-first-app-svc  # (3)!\nspec:\n  type: NodePort  # (4)!\n  selector:\n    app: nginx  # (5)!\n  ports:\n  - port: 80  # (6)!\n    targetPort: 80  # (7)!\n    nodePort: 30080  # (8)!\n</code></pre> <ol> <li>API version for Services</li> <li>We're creating a Service\u2014routes traffic to Pods</li> <li>Name of the Service (used for DNS)</li> <li>NodePort exposes the service externally for testing</li> <li>CRITICAL: This selector must match the Pod labels from your Deployment\u2014this is how Services find Pods</li> <li>Port the Service listens on</li> <li>Port to forward to on the Pod (container's port 80)</li> <li>External port on the node (range 30000-32767)</li> </ol> <p>Apply the Service:</p> <p>\u26a0\ufe0f Caution (Modifies Resources):</p> Create the Service<pre><code>kubectl apply -f nginx-service.yaml\n# service/my-first-app-svc created\n</code></pre> <p>\u2705 Safe (Read-Only):</p> Verify Service Created<pre><code>kubectl get service my-first-app-svc\n# NAME                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\n# my-first-app-svc    NodePort   10.96.123.45    &lt;none&gt;        80:30080/TCP   10s\n</code></pre>"},{"location":"day_one/kubectl/first_deploy/#testing-it","title":"Testing It","text":"Option 1: Port Forward (Easiest)Option 2: From Inside the Cluster <p>Goal: Access your service from your local machine without exposing it externally.</p> <p>\u2705 Safe (Creates Temporary Tunnel):</p> Forward Local Port to Service<pre><code>kubectl port-forward service/my-first-app-svc 8080:80\n# Forwarding from 127.0.0.1:8080 -&gt; 80\n</code></pre> <p>Open your browser and navigate to: <code>http://localhost:8080</code></p> <p>You should see the \"Welcome to nginx!\" page. Press <code>Ctrl+C</code> in your terminal to stop port forwarding when you're done.</p> <p>Goal: Test how other applications in your cluster will \"see\" and access your service using DNS.</p> <p>\u26a0\ufe0f Caution (Creates Temporary Pod):</p> Create Test Pod<pre><code>kubectl run test --image=busybox -it --rm -- sh\n</code></pre> <p>\u2705 Safe (Read-Only from Inside Pod):</p> Test Service from Inside Cluster<pre><code># Once inside the pod prompt:\nwget -O- http://my-first-app-svc\n# Shows nginx HTML source code\nexit\n</code></pre> <p>The <code>--rm</code> flag automatically deletes the test pod when you type <code>exit</code>.</p>"},{"location":"day_one/kubectl/first_deploy/#understanding-what-happened","title":"Understanding What Happened","text":"<p>Think of a Deployment like a Factory Manager. You hand the Manager a blueprint (your YAML) and say \"I want 2 copies running.\" The Manager hires a Supervisor (ReplicaSet) to watch the factory floor and keep exactly 2 machines (Pods) running at all times.</p> <p>Your Service acts like a reception desk\u2014it routes visitors to whichever machines are currently operational.</p> <p>The key: You declared what you wanted. Kubernetes figured out how to make it happen\u2014and will keep it that way automatically.</p> <p>Want the Full Technical Breakdown?</p> <p>This is enough to understand your deployment worked. For the complete step-by-step flow (what actually happened in the cluster, which components did what, and how it all connects), see Understanding What Happened\u2014the final Day One article that explains the architecture.</p>"},{"location":"day_one/kubectl/first_deploy/#common-pitfalls","title":"Common Pitfalls","text":"<p>Things don't always go smoothly. Here are the issues you're most likely to encounter.</p> <p>Don't try to memorize these\u2014if something goes wrong, come back to this section or jump to Essential kubectl Commands for detailed troubleshooting workflows. This is a reference for when you need it, not a study guide.</p> Pods Stuck in 'ImagePullBackOff' <p>Problem: Kubernetes can't download your container image.</p> <p>Check it: See detailed error<pre><code>kubectl describe pod my-first-app-7c5ddbdf54-2xkqn\n# Look for \"Events\" section at bottom\n</code></pre></p> <p>Common causes:</p> <ul> <li>Typo in image name - <code>nginx:1.21</code> not <code>nginx:121</code></li> <li>Image doesn't exist - Check the registry</li> <li>Private registry without credentials - Need imagePullSecrets</li> <li>Network/registry access issue - Cluster can't reach Docker Hub</li> </ul> <p>Fix: Correct the image name in your YAML and reapply.</p> Private Registry Authentication (Common in Companies) <p>If your company uses a private container registry (AWS ECR, Google Artifact Registry, Azure Container Registry, or self-hosted), Kubernetes needs authentication credentials to pull images.</p> <p>How to tell if this is your issue: Check the error message<pre><code>kubectl describe pod my-app-7c5ddbdf54-2xkqn\n# Events:\n#   Warning  Failed   10s  Failed to pull image \"mycompany.azurecr.io/my-app:latest\"\n#   Warning  Failed   10s  Error: ErrImagePull\n# Look for \"unauthorized\" or \"authentication required\"\n</code></pre></p> <p>The fix: imagePullSecrets</p> <p>Your platform team should provide:</p> <ol> <li>The registry credentials (username/password or token)</li> <li>Instructions to create the secret</li> <li>The secret name to reference</li> </ol> <p>Typical workflow:</p> 1. Create imagePullSecret (platform team provides credentials)<pre><code>kubectl create secret docker-registry my-registry-secret \\\n  --docker-server=mycompany.azurecr.io \\\n  --docker-username=&lt;username&gt; \\\n  --docker-password=&lt;password&gt; \\\n  --docker-email=&lt;your-email&gt;\n# secret/my-registry-secret created\n</code></pre> 2. Reference the secret in your Deployment<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      imagePullSecrets:  # Add this\n      - name: my-registry-secret  # Reference the secret\n      containers:\n      - name: my-app\n        image: mycompany.azurecr.io/my-app:1.0  # Private image\n        ports:\n        - containerPort: 80\n</code></pre> 3. Reapply the Deployment<pre><code>kubectl apply -f deployment.yaml\n# Now Kubernetes can authenticate and pull the private image\n</code></pre> <p>Important notes:</p> <ul> <li>Don't hardcode credentials in YAML\u2014always use Secrets</li> <li>Platform teams often automate this - Check if your namespace has default imagePullSecrets already configured</li> <li>Cloud provider managed registries (ECR, GCR, ACR) often use cluster IAM roles instead of imagePullSecrets</li> </ul> <p>Ask your platform team:</p> <ul> <li>\"Do I need imagePullSecrets for our registry?\"</li> <li>\"Is there a default imagePullSecret already configured in my namespace?\"</li> <li>\"How do I authenticate to pull images from our private registry?\"</li> </ul> <p>They may have already configured automatic authentication at the cluster or namespace level.</p> Deployment Shows '0/2 Ready' <p>Problem: Deployment created, but no Pods are running.</p> <p>Check it: See what's wrong<pre><code>kubectl get pods\nkubectl describe deployment my-first-app\n</code></pre></p> <p>Common causes:</p> <ul> <li>Resource limits - Not enough CPU/memory in cluster</li> <li>Image pull failing - See ImagePullBackOff above</li> <li>Container crashing - App has a startup error</li> </ul> <p>Fix: Use <code>kubectl describe pod &lt;pod-name&gt;</code> to see the specific error.</p> Service Can't Find Pods (503 errors) <p>Problem: Service is created but traffic doesn't reach Pods.</p> <p>Check it: Check Service endpoints<pre><code>kubectl get endpoints my-first-app-svc\n# Should show Pod IPs - if empty, labels don't match\n</code></pre></p> <p>Common cause: Label mismatch between Service selector and Pod labels.</p> <p>Your Service says: <pre><code>selector:\n  app: nginx  # Looking for pods with this label\n</code></pre></p> <p>Your Pods must have: <pre><code>metadata:\n  labels:\n    app: nginx  # Must match!\n</code></pre></p> <p>Fix: Ensure Service selector matches Deployment's Pod template labels exactly.</p> Changes to YAML Don't Apply <p>Problem: You edited the YAML but nothing changed.</p> <p>Common causes:</p> <ul> <li>Forgot to save the file - Check your editor</li> <li>Applied wrong file - <code>kubectl apply -f old-file.yaml</code></li> <li>Immutable field change - Some fields can't be updated (rare)</li> </ul> <p>Fix: Save, verify filename, reapply. For immutable fields, delete and recreate.</p> <p>Pro Debugging Workflow</p> <p>When something goes wrong, follow this order:</p> <ol> <li><code>kubectl get pods</code> - What's the status?</li> <li><code>kubectl describe pod &lt;name&gt;</code> - What are the events?</li> <li><code>kubectl logs &lt;pod-name&gt;</code> - What does the app say?</li> <li><code>kubectl get events --sort-by=.metadata.creationTimestamp</code> - Recent cluster events</li> </ol>"},{"location":"day_one/kubectl/first_deploy/#working-with-your-deployment","title":"Working With Your Deployment","text":""},{"location":"day_one/kubectl/first_deploy/#exploring-your-deployment","title":"Exploring Your Deployment","text":"<p>\u2705 Safe (Read-Only):</p> See All Resources<pre><code>kubectl get all\n# Shows: deployments, replicasets, pods, services\n</code></pre> Detailed Deployment Info<pre><code>kubectl describe deployment my-first-app\n# Shows: replicas, strategy, events, conditions\n</code></pre> View Pod Logs<pre><code>kubectl logs my-first-app-7c5ddbdf54-2xkqn\n# Shows: container logs (nginx access/error logs)\n</code></pre> Execute Command in Pod<pre><code>kubectl exec my-first-app-7c5ddbdf54-2xkqn -- nginx -v\n# nginx version: nginx/1.21.6\n</code></pre> <p>Pro Tip: Follow Logs</p> <p>Use <code>-f</code> to follow logs in real-time: <pre><code>kubectl logs -f my-first-app-7c5ddbdf54-2xkqn\n</code></pre> Press <code>Ctrl+C</code> to stop.</p>"},{"location":"day_one/kubectl/first_deploy/#scaling-up","title":"Scaling Up","text":"<p>Want more copies? Change replicas:</p> <p>\u26a0\ufe0f Caution (Modifies Resources):</p> Scale Deployment to 5 Replicas<pre><code>kubectl scale deployment my-first-app --replicas=5\n# deployment.apps/my-first-app scaled\n</code></pre> <p>\u2705 Safe (Read-Only):</p> Verify Scaling<pre><code>kubectl get pods\n# NAME                            READY   STATUS    RESTARTS   AGE\n# my-first-app-7c5ddbdf54-2xkqn   1/1     Running   0          5m\n# my-first-app-7c5ddbdf54-8mz4p   1/1     Running   0          5m\n# my-first-app-7c5ddbdf54-kx9qw   1/1     Running   0          10s\n# my-first-app-7c5ddbdf54-p2mzn   1/1     Running   0          10s\n# my-first-app-7c5ddbdf54-w4r7t   1/1     Running   0          10s\n</code></pre> <p>Now you have 5 pods! Kubernetes automatically created 3 more to match your desired state.</p>"},{"location":"day_one/kubectl/first_deploy/#cleaning-up","title":"Cleaning Up","text":"<p>When done testing:</p> <p>\ud83d\udea8 DANGER (Destructive):</p> Delete Resources by Name<pre><code>kubectl delete deployment my-first-app\n# deployment.apps \"my-first-app\" deleted\n\nkubectl delete service my-first-app-svc\n# service \"my-first-app-svc\" deleted\n</code></pre> <p>Or delete from files:</p> Delete Resources from Files<pre><code>kubectl delete -f nginx-deployment.yaml\nkubectl delete -f nginx-service.yaml\n</code></pre> <p>Deletion is Immediate</p> <p><code>kubectl delete</code> removes resources immediately. In dev, this is fine. In production, always double-check what you're deleting and consider using staging environments first.</p>"},{"location":"day_one/kubectl/first_deploy/#practice-exercise","title":"Practice Exercise","text":"Exercise: Deploy Your Own App <p>Goal: Deploy nginx with 3 replicas, expose it with a Service, and access it.</p> <p>Steps:</p> <ol> <li>Create deployment YAML with 3 replicas</li> <li>Apply it</li> <li>Create service YAML</li> <li>Apply it</li> <li>Access it with port-forward</li> <li>Scale to 5 replicas</li> <li>Clean up</li> </ol> Solution <p>Step 1: Create Deployment YAML</p> <p>Create <code>my-deployment.yaml</code>:</p> my-deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: practice-app\n  labels:\n    app: practice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: practice\n  template:\n    metadata:\n      labels:\n        app: practice\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        ports:\n        - containerPort: 80\n</code></pre> <p>Step 2: Apply Deployment</p> <pre><code>kubectl apply -f my-deployment.yaml\n# deployment.apps/practice-app created\n\nkubectl get pods\n# Should show 3 pods\n</code></pre> <p>Step 3: Create Service YAML</p> <p>Create <code>my-service.yaml</code>:</p> my-service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: practice-svc\nspec:\n  type: NodePort\n  selector:\n    app: practice\n  ports:\n  - port: 80\n    targetPort: 80\n    nodePort: 30090\n</code></pre> <p>Step 4: Apply Service</p> <pre><code>kubectl apply -f my-service.yaml\n# service/practice-svc created\n\nkubectl get service practice-svc\n# Verify service created\n</code></pre> <p>Step 5: Access with Port Forward</p> <pre><code>kubectl port-forward service/practice-svc 8080:80\n# Forwarding from 127.0.0.1:8080 -&gt; 80\n</code></pre> <p>Open browser: <code>http://localhost:8080</code></p> <p>You should see the nginx welcome page.</p> <p>Press <code>Ctrl+C</code> to stop port forwarding.</p> <p>Step 6: Scale to 5 Replicas</p> <pre><code>kubectl scale deployment practice-app --replicas=5\n# deployment.apps/practice-app scaled\n\nkubectl get pods\n# Should show 5 pods now\n</code></pre> <p>Step 7: Clean Up</p> <pre><code>kubectl delete -f my-deployment.yaml\nkubectl delete -f my-service.yaml\n# Or:\n# kubectl delete deployment practice-app\n# kubectl delete service practice-svc\n</code></pre> <p>What you learned:</p> <ul> <li>Creating deployment YAML from scratch</li> <li>Setting replicas</li> <li>Creating services</li> <li>Port forwarding for local access</li> <li>Scaling deployments</li> <li>Cleaning up resources</li> </ul>"},{"location":"day_one/kubectl/first_deploy/#quick-recap","title":"Quick Recap","text":"What You Did Command Created Deployment <code>kubectl apply -f deployment.yaml</code> Checked Pods <code>kubectl get pods</code> Created Service <code>kubectl apply -f service.yaml</code> Accessed App <code>kubectl port-forward</code> Scaled <code>kubectl scale deployment</code> Cleaned Up <code>kubectl delete</code>"},{"location":"day_one/kubectl/first_deploy/#further-reading","title":"Further Reading","text":""},{"location":"day_one/kubectl/first_deploy/#official-documentation","title":"Official Documentation","text":"<ul> <li>Kubernetes Docs: Deployments - Complete Deployment reference with examples</li> <li>Kubernetes Docs: Services - Service types and networking concepts</li> <li>kubectl Reference: apply - Full <code>kubectl apply</code> command reference</li> <li>kubectl Reference: get - Using <code>kubectl get</code> to view resources</li> </ul>"},{"location":"day_one/kubectl/first_deploy/#deep-dives","title":"Deep Dives","text":"<ul> <li>Understanding Kubernetes Objects - How Kubernetes represents resources</li> <li>Kubernetes API Concepts - Understanding <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code></li> </ul>"},{"location":"day_one/kubectl/first_deploy/#related-articles","title":"Related Articles","text":"<ul> <li>Day One Overview - See all Day One articles</li> <li>Getting kubectl Access - Review how to connect to your cluster</li> <li>What Is Kubernetes? - Understanding Kubernetes fundamentals</li> </ul>"},{"location":"day_one/kubectl/first_deploy/#whats-next","title":"What's Next?","text":"<p>You've deployed your first application! Next up in the Day One series:</p> <ul> <li>Essential kubectl Commands - Master the 10 commands you'll use every day</li> <li>Understanding What Happened - Deep dive into Deployments, ReplicaSets, and Pods</li> </ul> <p>Check the Day One Overview for the complete learning path.</p> <p>Congratulations! You're officially running applications on Kubernetes.</p>"},{"location":"day_one/kubectl/understanding/","title":"Understanding What Happened","text":"<p>Part of Day One: Getting Started</p> <p>This is the final article in Day One: Getting Started. You should have completed Your First Deployment and learned the Essential kubectl Commands before reading this.</p> <p>You ran <code>kubectl apply -f deployment.yaml</code> and your app deployed. You checked with <code>kubectl get pods</code> and saw your application running. It feels like magic.</p> <p>But what actually happened? When you told Kubernetes \"I want this app running,\" it created multiple resources working together behind the scenes. Understanding these resources transforms you from someone who copies commands to someone who truly understands Kubernetes.</p> <p>This article reveals what Kubernetes created when you deployed your application\u2014and why each piece matters.</p> <p>What You'll Learn</p> <p>By the end of this article, you'll understand:</p> <ul> <li>What Kubernetes created when you ran <code>kubectl apply</code> - the full resource hierarchy</li> <li>Deployments, ReplicaSets, and Pods - what each resource does and why all three exist</li> <li>How they work together - the flow from your YAML file to running containers</li> <li>Labels and selectors - how Services find Pods and Deployments manage Pods</li> <li>Self-healing behavior - how Kubernetes automatically replaces crashed Pods</li> <li>The cluster architecture - control plane, worker nodes, and how they interact</li> </ul>"},{"location":"day_one/kubectl/understanding/#the-resource-hierarchy","title":"The Resource Hierarchy","text":"<p>When you deployed your application in Your First Deployment, Kubernetes didn't just start containers. It created multiple resources working together in a specific hierarchy. Here's what got created and how they relate:</p> <pre><code>graph TD\n    Y1[You: kubectl apply&lt;br/&gt;deployment.yaml]\n\n    Y1 --&gt; Dep[Deployment&lt;br/&gt;manages desired state]\n    Dep --&gt; RS[ReplicaSet&lt;br/&gt;maintains pod count]\n    RS --&gt; P1[Pod 1&lt;br/&gt;app: nginx]\n    RS --&gt; P2[Pod 2&lt;br/&gt;app: nginx]\n    RS --&gt; P3[Pod 3&lt;br/&gt;app: nginx]\n\n    P1 &lt;-.-&gt;|Routes traffic| Svc[Service&lt;br/&gt;selector: app=nginx&lt;br/&gt;separate YAML]\n    P2 &lt;-.-&gt;|Routes traffic| Svc\n    P3 &lt;-.-&gt;|Routes traffic| Svc\n\n    Svc &lt;--&gt;|Requests &amp; Responses| External[Other Apps&lt;br/&gt;Other Services&lt;br/&gt;External Traffic]\n\n    Svc -.- Y2[You: kubectl apply&lt;br/&gt;service.yaml]\n\n    style Y1 fill:#1a202c,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Y2 fill:#1a202c,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Dep fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style RS fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style P1 fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style P2 fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style P3 fill:#2f855a,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style Svc fill:#2d3748,stroke:#cbd5e0,stroke-width:2px,color:#fff\n    style External fill:#4a5568,stroke:#cbd5e0,stroke-width:2px,color:#fff</code></pre> <p>Pods are the center - two flows connect to them:</p> <ul> <li>From ABOVE (deployment.yaml): You create a Deployment \u2192 It creates a ReplicaSet \u2192 ReplicaSet creates Pods</li> <li>From BELOW (service.yaml): You create a Service separately \u2192 It receives external traffic \u2192 Routes traffic UP to Pods by matching labels</li> </ul> <p>How the Service works:</p> <ul> <li>Stable gateway to Pods - Provides a permanent entry point while Pods come and go</li> <li>Receives all traffic - Other applications, services, or external sources send requests to the Service</li> <li>Routes to ephemeral Pods - Finds and distributes traffic to Pods by matching labels (<code>selector: app=nginx</code>)</li> <li>Independent of Deployments - Can route to ANY Pods with matching labels, even from different Deployments</li> <li>Survives Pod restarts - Pods die and get recreated constantly; the Service IP and DNS name stay stable</li> </ul> <p>Key insight: Pods don't know about the Service. The Service finds Pods using labels and routes traffic to them. The Deployment creates Pods from above, the Service routes to them from below.</p>"},{"location":"day_one/kubectl/understanding/#the-kubernetes-resources-explained","title":"The Kubernetes Resources Explained","text":"<p>Building on the basics: Remember the Factory Manager analogy from Your First Deployment? Now we're diving deep into how that factory actually operates\u2014each resource's role and technical details.</p> <ul> <li> <p> Deployment: The Manager</p> <p>Why it matters: The Deployment is what you interact with. It maintains your desired state\u2014if you want 3 replicas, the Deployment ensures 3 replicas exist.</p> See Your Deployment<pre><code>kubectl get deployments\n# NAME               READY   UP-TO-DATE   AVAILABLE   AGE\n# nginx-deployment   3/3     3            3           5m\n</code></pre> <p>What to notice:</p> <ul> <li><code>READY</code>: How many Pods are running vs. desired (3/3 means all good)</li> <li><code>UP-TO-DATE</code>: Pods running the latest configuration</li> <li><code>AVAILABLE</code>: Pods ready to serve traffic</li> </ul> <p>The Deployment handles updates, rollbacks, and scaling. When you run <code>kubectl scale</code> or <code>kubectl rollout</code>, you're talking to the Deployment.</p> </li> <li> <p> ReplicaSet: The Supervisor</p> <p>Why it matters: The ReplicaSet is the Deployment's worker. Its job: ensure exactly the right number of Pods exist. If a Pod crashes, the ReplicaSet creates a replacement.</p> See Your ReplicaSet<pre><code>kubectl get replicasets\n# NAME                         DESIRED   CURRENT   READY   AGE\n# nginx-deployment-7c5ddbdf54  3         3         3       5m\n</code></pre> <p>What to notice:</p> <ul> <li>The name includes a hash (<code>7c5ddbdf54</code>) - this is the Pod template version</li> <li><code>DESIRED</code>: What the Deployment wants (3)</li> <li><code>CURRENT</code>: How many Pods exist right now (3)</li> <li><code>READY</code>: How many Pods passed health checks (3)</li> </ul> <p>You rarely interact with ReplicaSets directly. The Deployment manages them. But understanding they exist helps you troubleshoot\u2014if you see old ReplicaSets with 0 Pods, those are from previous deployments (kept for rollback).</p> </li> <li> <p> Pods: The Workers</p> <p>Why it matters: Pods are where your containers actually run. Everything else exists to manage these. When you check logs, exec into a shell, or debug networking\u2014you're interacting with Pods.</p> See Your Pods<pre><code>kubectl get pods\n# NAME                               READY   STATUS    RESTARTS   AGE\n# nginx-deployment-7c5ddbdf54-8kp2m  1/1     Running   0          5m\n# nginx-deployment-7c5ddbdf54-j9xvq  1/1     Running   0          5m\n# nginx-deployment-7c5ddbdf54-wnr4k  1/1     Running   0          5m\n</code></pre> <p>What to notice:</p> <ul> <li>Each Pod has the ReplicaSet name + a unique suffix</li> <li><code>READY</code>: How many containers are running in the Pod (1/1 means 1 of 1 is ready)</li> <li><code>STATUS</code>: <code>Running</code> is good; <code>Pending</code>, <code>CrashLoopBackOff</code>, or <code>Error</code> need investigation</li> <li><code>RESTARTS</code>: How many times the container has restarted (0 is ideal)</li> </ul> <p>Key insight: Pods are ephemeral. They can be deleted and recreated at any time. Never rely on a specific Pod existing\u2014that's why you use Deployments and Services.</p> </li> <li> <p> Service: The Router (Separate YAML!)</p> <p>Why it matters: Pods have changing IP addresses. Services provide a stable endpoint that routes traffic to your Pods, no matter which specific Pods are running.</p> <p>Critical: The Service is NOT created by the Deployment. It's a separate YAML file you apply separately. The Service finds Pods using label selectors\u2014if the labels match, the Service routes to them.</p> See Your Service (if created)<pre><code>kubectl get services\n# NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\n# nginx-service   ClusterIP   10.96.145.22    &lt;none&gt;        80/TCP    5m\n</code></pre> <p>What to notice:</p> <ul> <li><code>TYPE</code>: How the Service is exposed (ClusterIP = internal only)</li> <li><code>CLUSTER-IP</code>: The stable internal IP for this Service</li> <li><code>PORT(S)</code>: What port the Service listens on</li> </ul> <p>When you need it: If your app needs to receive network traffic (web server, API, database), you need a Service. If it just runs a job and exits, you might not.</p> </li> </ul>"},{"location":"day_one/kubectl/understanding/#investigating-with-kubectl","title":"Investigating with kubectl","text":"<p>Let's use <code>kubectl describe</code> to see the full story. This command shows you everything about a resource, including events (what happened).</p> Describe Your DeploymentDescribe a PodDescribe a Service <p>\u2705 Safe (Read-Only):</p> Get Detailed Deployment Info<pre><code>kubectl describe deployment nginx-deployment\n</code></pre> <p>What you'll see:</p> <ul> <li>Selector: How the Deployment finds its Pods (labels)</li> <li>Replicas: Desired vs. actual count</li> <li>Pod Template: The blueprint for Pods (container image, ports, env vars)</li> <li>Events: Actions Kubernetes took (scaled, created ReplicaSet, etc.)</li> </ul> <p>Why it matters: The events section shows you the deployment timeline. If something failed, you'll see error messages here.</p> <p>\u2705 Safe (Read-Only):</p> Get Detailed Pod Info<pre><code>kubectl describe pod nginx-deployment-7c5ddbdf54-8kp2m\n</code></pre> <p>What you'll see:</p> <ul> <li>Node: Which cluster node is running this Pod</li> <li>Status: Current state and why</li> <li>IP: The Pod's internal IP address</li> <li>Containers: What's running inside (image, ports, resource limits)</li> <li>Events: Pod lifecycle (scheduled, pulled image, started container)</li> </ul> <p>Why it matters: If a Pod won't start, the events section tells you why\u2014image pull failure, insufficient resources, crash loops, etc.</p> <p>\u2705 Safe (Read-Only):</p> Get Detailed Service Info<pre><code>kubectl describe service nginx-service\n</code></pre> <p>What you'll see:</p> <ul> <li>Selector: How the Service finds Pods to route to (labels)</li> <li>Type: ClusterIP, NodePort, or LoadBalancer</li> <li>Endpoints: The actual Pod IPs the Service routes to</li> <li>Events: Any Service configuration changes</li> </ul> <p>Why it matters: The endpoints list shows which Pods are receiving traffic. If it's empty, your Service selector doesn't match your Pod labels.</p>"},{"location":"day_one/kubectl/understanding/#how-they-work-together","title":"How They Work Together","text":"<p>Here's what happens when you deploy an application:</p> Step 1: You create a Deployment deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3  # (1)!\n  selector:\n    matchLabels:\n      app: nginx  # (2)!\n  template:\n    metadata:\n      labels:\n        app: nginx  # (3)!\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        ports:\n        - containerPort: 80\n</code></pre> <ol> <li>\"I want 3 copies running\"</li> <li>\"Find Pods with this label\"</li> <li>\"Give created Pods this label\"</li> </ol> <p>What you're telling Kubernetes: \"I want 3 replicas of nginx running. Label them with <code>app: nginx</code> so other resources can find them.\"</p> Step 2: The Deployment creates a ReplicaSet <p>The Deployment says: \"Create a ReplicaSet with this Pod template and maintain 3 replicas.\"</p> <p>The ReplicaSet gets a name based on the Deployment name + a hash of the Pod template. If you update the Deployment (change the image, for example), a new ReplicaSet gets created.</p> <p>Why a hash? The hash represents the Pod template version. Different templates = different hashes = different ReplicaSets. This is how Kubernetes manages rolling updates and rollbacks.</p> Step 3: The ReplicaSet creates Pods <p>The ReplicaSet says: \"I need 3 Pods matching this template. Let me create them.\"</p> <p>It creates 3 Pods, each with a unique name (ReplicaSet name + random suffix). The Pods get the labels defined in the template.</p> <p>What you'll see: <code>nginx-deployment-7c5ddbdf54-abc12</code> (Deployment name + ReplicaSet hash + unique Pod suffix)</p> Step 4: Kubernetes schedules the Pods <p>The Kubernetes scheduler picks which nodes (servers) should run each Pod. It considers resources available, node selectors, affinity rules, etc.</p> <p>Behind the scenes: The scheduler looks at CPU/memory requests, node capacity, and placement rules to decide \"Pod 1 goes to node-a, Pod 2 goes to node-b, Pod 3 goes to node-c.\"</p> Step 5: Container runtime starts the containers <p>On each node, the container runtime (Docker, containerd, CRI-O) pulls the image and starts the container(s).</p> <p>The actual work: This is where <code>docker pull nginx:1.21</code> happens and your application container actually starts running.</p> Step 6: The Service routes traffic (separate YAML you created) <p>Remember when you created the Service with <code>kubectl apply -f nginx-service.yaml</code>? That Service watches for Pods with matching labels and adds their IPs to its endpoint list. Now traffic to the Service gets distributed across your Pods.</p> <p>Key insight: The Service is NOT created by the Deployment. You created it separately\u2014two YAMLs, two <code>kubectl apply</code> commands. The Service and Deployment are independent resources that connect only through labels (<code>app: nginx</code>).</p>"},{"location":"day_one/kubectl/understanding/#what-happens-when","title":"What Happens When...","text":"<ul> <li> <p> You scale the Deployment</p> <p>Why it matters: You can increase or decrease capacity instantly.</p> <p>\u26a0\ufe0f Caution (Modifies Resources):</p> Scale to 5 Replicas<pre><code>kubectl scale deployment nginx-deployment --replicas=5\n</code></pre> <p>The flow:</p> <ol> <li>Deployment updates: <code>replicas: 5</code></li> <li>ReplicaSet sees it needs 5 Pods but only has 3</li> <li>ReplicaSet creates 2 more Pods</li> <li>Scheduler places them on nodes</li> <li>Containers start</li> <li>Service automatically adds new Pod IPs to endpoints</li> </ol> <p>Result: You go from 3 running Pods to 5, seamlessly.</p> </li> <li> <p> A Pod crashes</p> <p>The flow:</p> <ol> <li>Pod container exits unexpectedly (bug, out of memory, etc.)</li> <li>ReplicaSet sees it now has 2 healthy Pods instead of 3</li> <li>ReplicaSet creates a replacement Pod</li> <li>Scheduler places it on a node</li> <li>Container starts</li> <li>Service removes the crashed Pod's IP and adds the new one</li> </ol> <p>Result: Your desired 3 replicas are maintained automatically. This is Kubernetes' self-healing.</p> </li> <li> <p> You update the image</p> <p>Why it matters: This is how you deploy new versions of your code.</p> <p>\u26a0\ufe0f Caution (Modifies Resources):</p> deployment.yaml \u2014 update image tag, then apply<pre><code># Change the image tag in your YAML file:\n#   image: nginx:1.21  \u2192  image: nginx:1.22\n# Then apply the change:\n# kubectl apply -f deployment.yaml\n</code></pre> <p>The flow:</p> <ol> <li>Deployment updates the Pod template (new image)</li> <li>Deployment creates a NEW ReplicaSet (with updated template)</li> <li>Deployment scales new ReplicaSet up, old ReplicaSet down (rolling update)</li> <li>Old Pods terminate, new Pods start with new image</li> <li>Service seamlessly routes to both old and new Pods during transition</li> </ol> <p>Result: Zero-downtime update. The old ReplicaSet remains (scaled to 0) for easy rollback.</p> </li> <li> <p> You delete the Deployment</p> <p>Why it matters: Clean up resources when they're no longer needed.</p> <p>\ud83d\udea8 DANGER (Destructive):</p> Delete Everything<pre><code>kubectl delete deployment nginx-deployment\n</code></pre> <p>The flow:</p> <ol> <li>Deployment gets deleted</li> <li>Deployment tells its ReplicaSet(s) to scale to 0</li> <li>ReplicaSet deletes all Pods</li> <li>ReplicaSet gets deleted</li> <li>Service remains (it's independent) but has no endpoints</li> </ol> <p>Result: Everything created by the Deployment is cleaned up. Services are separate\u2014you need to delete them explicitly.</p> </li> </ul>"},{"location":"day_one/kubectl/understanding/#labels-the-glue","title":"Labels: The Glue","text":"<p>Everything connects through labels. Labels are key-value pairs you attach to resources.</p> How Labels Connect Resources<pre><code># Deployment creates Pods with this label:\nspec:\n  template:\n    metadata:\n      labels:\n        app: nginx  # (1)!\n\n# Service finds Pods with this label:\nspec:\n  selector:\n    app: nginx  # (2)!\n\n# ReplicaSet manages Pods with this label:\nspec:\n  selector:\n    matchLabels:\n      app: nginx  # (3)!\n</code></pre> <ol> <li>Every Pod created by this Deployment gets <code>app: nginx</code></li> <li>The Service routes to any Pod with <code>app: nginx</code></li> <li>The ReplicaSet maintains the count of Pods with <code>app: nginx</code></li> </ol> <p>Without matching labels, nothing connects. If your Service selector doesn't match your Pod labels, traffic won't route. If your Deployment selector doesn't match your template labels, the ReplicaSet won't manage the Pods.</p> <p>Label Mismatches Are a Common Issue</p> <p>If your Service has no endpoints, check that the Service selector matches your Pod labels exactly. Use <code>kubectl describe service &lt;name&gt;</code> to see the selector and <code>kubectl get pods --show-labels</code> to see Pod labels.</p>"},{"location":"day_one/kubectl/understanding/#practice-exercises","title":"Practice Exercises","text":"Exercise 1: Trace Your Deployment <p>Deploy an application and trace the full resource hierarchy from Deployment to Pods.</p> <p>Goal: Understand the relationship between all resources created.</p> <p>Steps:</p> <ol> <li>Deploy nginx using a Deployment</li> <li>List all Deployments, ReplicaSets, and Pods</li> <li>Identify which ReplicaSet belongs to your Deployment</li> <li>Identify which Pods belong to your ReplicaSet</li> </ol> Solution <p>Create and apply a Deployment:</p> nginx-deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        ports:\n        - containerPort: 80\n</code></pre> Apply the Deployment<pre><code>kubectl apply -f nginx-deployment.yaml\n</code></pre> <p>Now trace the hierarchy:</p> See the Full Chain<pre><code># 1. See your Deployment\nkubectl get deployments\n# NAME               READY   UP-TO-DATE   AVAILABLE   AGE\n# nginx-deployment   3/3     3            3           1m\n\n# 2. See the ReplicaSet it created\nkubectl get replicasets\n# NAME                          DESIRED   CURRENT   READY   AGE\n# nginx-deployment-7c5ddbdf54   3         3         3       1m\n# Notice the name: deployment-name + hash\n\n# 3. See the Pods the ReplicaSet created\nkubectl get pods\n# NAME                                READY   STATUS    RESTARTS   AGE\n# nginx-deployment-7c5ddbdf54-abc12   1/1     Running   0          1m\n# nginx-deployment-7c5ddbdf54-def34   1/1     Running   0          1m\n# nginx-deployment-7c5ddbdf54-ghi56   1/1     Running   0          1m\n# Notice: Each Pod name starts with the ReplicaSet name\n</code></pre> <p>What you learned: The naming convention makes the hierarchy visible. Every resource name includes its parent's name.</p> Exercise 2: Watch Self-Healing in Action <p>Delete a Pod and watch Kubernetes automatically replace it.</p> <p>Goal: See ReplicaSet's self-healing behavior.</p> <p>Hint: Delete a Pod, then immediately run <code>kubectl get pods --watch</code></p> Solution <p>First, get your Pod names:</p> List Pods<pre><code>kubectl get pods\n# NAME                                READY   STATUS    RESTARTS   AGE\n# nginx-deployment-7c5ddbdf54-abc12   1/1     Running   0          5m\n# nginx-deployment-7c5ddbdf54-def34   1/1     Running   0          5m\n# nginx-deployment-7c5ddbdf54-ghi56   1/1     Running   0          5m\n</code></pre> <p>Pick one Pod and delete it:</p> Delete One Pod<pre><code>kubectl delete pod nginx-deployment-7c5ddbdf54-abc12\n</code></pre> <p>Watch what happens:</p> Watch Replacement Creation<pre><code>kubectl get pods --watch\n# NAME                                READY   STATUS    RESTARTS   AGE\n# nginx-deployment-7c5ddbdf54-abc12   1/1     Terminating   0      5m\n# nginx-deployment-7c5ddbdf54-xyz99   0/1     Pending       0      0s\n# nginx-deployment-7c5ddbdf54-xyz99   0/1     ContainerCreating   0   1s\n# nginx-deployment-7c5ddbdf54-xyz99   1/1     Running             0   3s\n</code></pre> <p>What you see:</p> <ol> <li>The Pod you deleted enters <code>Terminating</code> state</li> <li>A new Pod immediately gets created (<code>Pending</code>)</li> <li>The new Pod starts (<code>ContainerCreating</code>)</li> <li>The new Pod becomes <code>Running</code></li> <li>You still have 3 replicas total</li> </ol> <p>What you learned: The ReplicaSet constantly monitors the desired vs. actual state. When a Pod disappears, it immediately creates a replacement.</p> Exercise 3: Understand ReplicaSet Versioning <p>Update your Deployment's image and observe how Kubernetes creates a new ReplicaSet while keeping the old one for rollback.</p> <p>Goal: See how Deployments manage updates using multiple ReplicaSets.</p> <p>Hint: Use <code>kubectl set image</code> to update, then check ReplicaSets.</p> Solution <p>Check current ReplicaSets:</p> Before Update<pre><code>kubectl get replicasets\n# NAME                          DESIRED   CURRENT   READY   AGE\n# nginx-deployment-7c5ddbdf54   3         3         3       10m\n</code></pre> <p>Update the image by editing <code>nginx-deployment.yaml</code> \u2014 change <code>nginx:1.21</code> to <code>nginx:1.22</code>, save, then apply:</p> Apply the Updated Image Tag<pre><code>kubectl apply -f nginx-deployment.yaml\n# deployment.apps/nginx-deployment configured\n</code></pre> <p>Watch the rollout:</p> Watch the Update<pre><code>kubectl rollout status deployment/nginx-deployment\n# Waiting for deployment \"nginx-deployment\" rollout to finish: 1 out of 3 new replicas have been updated...\n# Waiting for deployment \"nginx-deployment\" rollout to finish: 2 out of 3 new replicas have been updated...\n# deployment \"nginx-deployment\" successfully rolled out\n</code></pre> <p>Now check ReplicaSets again:</p> After Update<pre><code>kubectl get replicasets\n# NAME                          DESIRED   CURRENT   READY   AGE\n# nginx-deployment-7c5ddbdf54   0         0         0       15m\n# nginx-deployment-556d429bd4   3         3         3       2m\n</code></pre> <p>What you see:</p> <ul> <li>Old ReplicaSet (<code>7c5ddbdf54</code>): Scaled to 0, but still exists</li> <li>New ReplicaSet (<code>556d429bd4</code>): Running all 3 Pods with new image</li> <li>Different hashes = different Pod templates</li> </ul> <p>Why it matters: The old ReplicaSet enables instant rollback. If the new version has a bug, Kubernetes can scale the old ReplicaSet back up and the new one back down.</p> <p>Try rolling back:</p> Rollback to Previous Version<pre><code>kubectl rollout undo deployment/nginx-deployment\n</code></pre> <p>Check ReplicaSets again\u2014you'll see the old one scaled back up!</p> <p>What you learned: Deployments manage multiple ReplicaSets (one per version). This is how rolling updates and rollbacks work.</p> Exercise 4: Investigate Label Connections <p>Create a Service and verify it finds your Pods using labels.</p> <p>Goal: Understand how labels connect Services to Pods.</p> <p>Steps:</p> <ol> <li>Verify your Deployment's Pods have labels</li> <li>Create a Service with a matching selector</li> <li>Check that the Service found your Pods (endpoints)</li> </ol> Solution <p>First, check your Pod labels (assuming you still have the nginx-deployment from Exercise 1):</p> Show Pod Labels<pre><code>kubectl get pods --show-labels\n# NAME                                READY   STATUS    RESTARTS   AGE   LABELS\n# nginx-deployment-7c5ddbdf54-abc12   1/1     Running   0          10m   app=nginx,pod-template-hash=7c5ddbdf54\n# nginx-deployment-7c5ddbdf54-def34   1/1     Running   0          10m   app=nginx,pod-template-hash=7c5ddbdf54\n# nginx-deployment-7c5ddbdf54-ghi56   1/1     Running   0          10m   app=nginx,pod-template-hash=7c5ddbdf54\n</code></pre> <p>Notice: All Pods have <code>app=nginx</code> label.</p> <p>Create a Service that selects <code>app=nginx</code>:</p> nginx-service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx  # This must match Pod labels!\n  ports:\n  - port: 80\n    targetPort: 80\n  type: ClusterIP\n</code></pre> Apply the Service<pre><code>kubectl apply -f nginx-service.yaml\n</code></pre> <p>Now check if the Service found your Pods:</p> Describe the Service<pre><code>kubectl describe service nginx-service\n# Name:              nginx-service\n# Namespace:         default\n# Labels:            &lt;none&gt;\n# Annotations:       &lt;none&gt;\n# Selector:          app=nginx\n# Type:              ClusterIP\n# IP Family Policy:  SingleStack\n# IP Families:       IPv4\n# IP:                10.96.145.22\n# Port:              &lt;unset&gt;  80/TCP\n# TargetPort:        80/TCP\n# Endpoints:         10.244.1.5:80,10.244.1.6:80,10.244.1.7:80\n# Session Affinity:  None\n# Events:            &lt;none&gt;\n</code></pre> <p>What to notice:</p> <ul> <li>Selector: <code>app=nginx</code> (matches your Pod labels)</li> <li>Endpoints: List of Pod IPs (should show 3 IPs)</li> </ul> <p>If endpoints are empty, your selector doesn't match your Pod labels!</p> <p>What you learned: Services find Pods using label selectors. The Service doesn't \"know about\" the Deployment\u2014it just looks for any Pod with matching labels.</p>"},{"location":"day_one/kubectl/understanding/#quick-recap","title":"Quick Recap","text":"<p>The Resource Hierarchy:</p> Resource Job You Interact With It? Deployment Maintains desired state (replicas, version) \u2705 Yes - for scaling, updates, rollbacks ReplicaSet Ensures exact number of Pods exist \u26a0\ufe0f Rarely - Deployment manages it Pod Runs your container(s) \u2705 Yes - for logs, debugging, troubleshooting Service Routes traffic to Pods \u2705 Yes - for exposing applications <p>Key Insights:</p> <ul> <li>When you deploy, Kubernetes creates a chain: Deployment \u2192 ReplicaSet \u2192 Pods</li> <li>ReplicaSets provide self-healing (replace crashed Pods automatically)</li> <li>Multiple ReplicaSets enable rolling updates and rollbacks</li> <li>Services provide stable networking (Pod IPs change, Service IPs don't)</li> <li>Labels connect everything\u2014Services find Pods, Deployments manage Pods, all through labels</li> <li>Use <code>kubectl describe</code> to investigate what actually happened</li> </ul>"},{"location":"day_one/kubectl/understanding/#youve-completed-day-one","title":"You've Completed Day One","text":"<p>Remember where you started? You had <code>kubectl</code> access to a cluster. Maybe you were intimidated by the command line. Maybe someone said \"we're using Kubernetes now\" and you weren't sure where to begin.</p> <p>Look at what you've accomplished:</p> <p>You deployed your first application. You watched Kubernetes create Pods, maintain them, and scale them automatically. You exposed your application with a Service and verified traffic could reach it. You learned the essential <code>kubectl</code> commands\u2014the ones you'll use every single day. And now you understand what actually happens behind the scenes: how Deployments manage ReplicaSets, how ReplicaSets maintain Pods, how Services route traffic using labels.</p> <p>You're not just running commands anymore\u2014you understand the system.</p> <p>You know that when a Pod crashes, the ReplicaSet notices and creates a replacement. You know that when you update an image, Kubernetes creates a new ReplicaSet and gradually shifts traffic. You know that labels are the glue connecting everything. This isn't magic anymore\u2014it's architecture you can reason about.</p> <p>This foundation is powerful. With what you've learned in Day One, you can:</p> <ul> <li>Deploy applications to your dev cluster with confidence</li> <li>Understand why Pods crash or Services fail to route traffic</li> <li>Scale applications to handle load</li> <li>Roll back when deployments go wrong</li> <li>Communicate with your platform team using the right terminology</li> </ul> <p>The <code>kubectl</code> commands aren't scary anymore. The YAML makes sense. The cluster is a tool you can wield, not a black box you're afraid to touch.</p> <p>Day One is complete. You're ready for production work.</p> <p>Note for Experienced Developers</p> <p>We intentionally skipped one topic: resource requests and limits (how much CPU/memory your Pods need). That's a Level 2 topic for when you're deploying real applications and need to tune performance. Day One focused on getting things running\u2014optimizing resource allocation comes after you understand the basics.</p> <p>The deeper dive into Kubernetes continues with Level 1, where you'll master each primitive in detail\u2014but honestly? With what you know right now, you can already ship code to Kubernetes. Many developers work productively for months with just Day One knowledge.</p> <p>The difference between you when you started Day One and you right now is confidence. You've deployed. You've investigated. You've understood. That's what Day One was for.</p>"},{"location":"day_one/kubectl/understanding/#whats-next","title":"What's Next","text":"<p>Congratulations! You've completed the Day One journey. You've gone from \"What is Kubernetes?\" to understanding the full resource hierarchy of a deployment.</p> <p>When you're ready to move beyond the basics and master the individual building blocks of Kubernetes, continue to:</p> <ul> <li>Level 1: Core Primitives - (coming soon) Deep dive into Pods, Services, ConfigMaps, and Namespaces</li> <li>Essential kubectl Commands - Review the commands for investigating resources</li> <li>Your First Deployment - Revisit the deployment process with your new understanding</li> </ul> <p>Practice makes perfect: Deploy your real application to solidify what you've learned. The best way to master Kubernetes is to use it with actual projects.</p>"},{"location":"day_one/kubectl/understanding/#further-reading","title":"Further Reading","text":""},{"location":"day_one/kubectl/understanding/#official-documentation","title":"Official Documentation","text":"<ul> <li>Kubernetes Concepts: Workloads - Official overview of Pods, Controllers, and Workloads</li> <li>Deployments - Deep dive into Deployment controller</li> <li>ReplicaSets - Understanding ReplicaSets and when you'd use them directly</li> <li>Pods - The atomic unit of Kubernetes</li> <li>Services, Load Balancing, and Networking - How networking works in Kubernetes</li> </ul>"},{"location":"day_one/kubectl/understanding/#deep-dives","title":"Deep Dives","text":"<ul> <li>Understanding Kubernetes Objects - How Kubernetes represents resources (spec, status, metadata)</li> <li>Labels and Selectors - How resources find each other</li> <li>Kubernetes Components - Control plane and node components explained</li> </ul>"},{"location":"day_one/kubectl/understanding/#related-articles","title":"Related Articles","text":"<ul> <li>What Is Kubernetes? - Review the fundamentals and the problem Kubernetes solves</li> <li>Your First Deployment - Revisit the deployment process with your new understanding</li> <li>Essential kubectl Commands - Commands for investigating resource relationships</li> <li>Day One Overview - See your complete Day One journey</li> </ul>"}]}